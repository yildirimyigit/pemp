{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yigit/.local/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "folder_path = '../models/'\n",
    "if folder_path not in sys.path:\n",
    "    sys.path.append(folder_path)\n",
    "\n",
    "folder_path = '../data/'\n",
    "if folder_path not in sys.path:\n",
    "    sys.path.append(folder_path)\n",
    "\n",
    "from cnmp import CNMP\n",
    "from positional_encoders import *\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "def get_free_gpu():\n",
    "    gpu_util = []\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_device(i)  # Switch GPU\n",
    "#        gpu_util.append((i, torch.cuda.memory_stats()['reserved_bytes.all.current'] / (1024 ** 2)))\n",
    "        gpu_util.append((i, torch.cuda.utilization()))\n",
    "    gpu_util.sort(key=lambda x: x[1])\n",
    "    return gpu_util[0][0]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    available_gpu = get_free_gpu()\n",
    "    if available_gpu == 0:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{available_gpu}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(\"Device :\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([18, 400, 1]), y_train shape: torch.Size([18, 400, 7]), g_train shape: torch.Size([18])\n",
      "x_test shape: torch.Size([6, 400, 1]), y_test shape: torch.Size([6, 400, 7]), g_test shape: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "dx, dy, dg, dph, dpe = 1, 7, 1, 0, 27\n",
    "num_demos, num_test = 18, 6\n",
    "num_trajs = num_demos + num_test\n",
    "t_steps = 400\n",
    "n_max, m_max = 20, 20\n",
    "\n",
    "trajectories, freqs = torch.from_numpy(np.load('../data/xarm/processed/norm_400.npy')), torch.from_numpy(np.load('../data/xarm/processed/freqs_400.npy'))\n",
    "\n",
    "perm_ids = torch.randperm(num_trajs)\n",
    "train_ids, test_ids = perm_ids[:num_demos], perm_ids[num_demos:]\n",
    "\n",
    "all_x = torch.linspace(0, 1, t_steps).unsqueeze(-1).unsqueeze(0).repeat(num_trajs,1,1)\n",
    "\n",
    "x_train, x_test = all_x[train_ids], all_x[test_ids]\n",
    "y_train, y_test = trajectories[train_ids], trajectories[test_ids]\n",
    "g_train, g_test = freqs[train_ids], freqs[test_ids]\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}, g_train shape: {g_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}, g_test shape: {g_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = generate_positional_encoding(t_steps, dpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Colorblind-friendly colors (use perceptually distinct colors)\n",
    "colors = [\n",
    "    '#377eb8',  # Blue\n",
    "    '#ff7f00',  # Orange\n",
    "    '#4daf4a',  # Green\n",
    "    '#f781bf',  # Pink\n",
    "    '#a65628',  # Brown\n",
    "    '#984ea3',  # Purple\n",
    "    '#999999',  # Gray\n",
    "    '#e41a1c',  # Red\n",
    "    '#dede00'   # Yellow\n",
    "]\n",
    "dark_gray = '#4d4d4d'\n",
    "linestyles = [(0, (3, 1, 1, 1, 1, 1)), (0, (1, 1)), '--', (0, (5, 10)), ':', '-.', '-', (0, (1, 3))]\n",
    "plt_size_coeff = 4\n",
    "\n",
    "\n",
    "handles = [Line2D([0], [0], color=dark_gray, lw=2, label='Demonstration'),\n",
    "           Line2D([0], [0], color=colors[0], lw=2, label='CNMP Prediction'), \n",
    "           Line2D([0], [0], color=colors[1], lw=2, label='PEMP Prediction')]  # common in all plots\n",
    "\n",
    "min_y, max_y = -np.pi/2, np.pi/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bare:  1063950\n",
      "PE:  1090574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yigit/.local/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "enc_dims = [512,512,512]\n",
    "dec_dims = [512,512,512]\n",
    "\n",
    "m0_ = CNMP(input_dim=dx+dg, output_dim=dy, n_max=n_max, m_max=m_max, encoder_hidden_dims=enc_dims, decoder_hidden_dims=dec_dims, batch_size=batch_size, device=device)\n",
    "opt0 = torch.optim.Adam(lr=3e-4, params=m0_.parameters())\n",
    "\n",
    "m1_ = CNMP(input_dim=dpe+dg, output_dim=dy, n_max=n_max, m_max=m_max, encoder_hidden_dims=enc_dims, decoder_hidden_dims=dec_dims, batch_size=batch_size, device=device)\n",
    "opt1 = torch.optim.Adam(lr=3e-4, params=m1_.parameters())\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in m0_.parameters())\n",
    "print('Bare: ', pytorch_total_params)\n",
    "pytorch_total_params = sum(p.numel() for p in m1_.parameters())\n",
    "print('PE: ', pytorch_total_params)\n",
    "\n",
    "if torch.__version__ >= \"2.0\":\n",
    "    m0, m1 = torch.compile(m0_), torch.compile(m1_)\n",
    "else:\n",
    "    m0, m1 = m0_, m1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs0 = torch.zeros((batch_size, n_max, dx+dg+dy), dtype=torch.float32, device=device)\n",
    "tar_x0 = torch.zeros((batch_size, m_max, dx+dg), dtype=torch.float32, device=device)\n",
    "\n",
    "obs1 = torch.zeros((batch_size, n_max, dpe+dg+dy), dtype=torch.float32, device=device)\n",
    "tar_x1 = torch.zeros((batch_size, m_max, dpe+dg), dtype=torch.float32, device=device)\n",
    "\n",
    "tar_y = torch.zeros((batch_size, m_max, dy), dtype=torch.float32, device=device)\n",
    "obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "tar_mask = torch.zeros((batch_size, m_max), dtype=torch.bool, device=device)\n",
    "\n",
    "def prepare_masked_batch(t: list, traj_ids: list):\n",
    "    global obs0, tar_x0, obs1, tar_x1, tar_y, obs_mask, tar_mask\n",
    "    obs0.fill_(0)\n",
    "    tar_x0.fill_(0)\n",
    "    obs1.fill_(0)\n",
    "    tar_x1.fill_(0)\n",
    "    tar_y.fill_(0)\n",
    "    obs_mask.fill_(False)\n",
    "    tar_mask.fill_(False)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = t[traj_id]\n",
    "\n",
    "        n = torch.randint(1, n_max+1, (1,)).item()\n",
    "        m = torch.randint(1, m_max+1, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = permuted_ids[n:n+m]\n",
    "\n",
    "        obs0[i, :n, :dx] = x_train[traj_id, n_ids]  # t\n",
    "        obs0[i, :n, dx:dx+dg] = g_train[traj_id]  # gamma\n",
    "        obs0[i, :n, dx+dg:] = traj[n_ids]  # SM(t)\n",
    "\n",
    "        obs1[i, :n, :dpe] = pe[n_ids]  # PE(t)\n",
    "        obs1[i, :n, dpe:dpe+dg] = g_train[traj_id]  # gamma\n",
    "        obs1[i, :n, dpe+dg:] = traj[n_ids]  # SM(t)\n",
    "\n",
    "        obs_mask[i, :n] = True\n",
    "        \n",
    "        tar_x0[i, :m, :dx] = x_train[traj_id, m_ids]\n",
    "        tar_x0[i, :m, dx:] = g_train[traj_id]\n",
    "        tar_x1[i, :m, :dpe] = pe[m_ids]\n",
    "        tar_x1[i, :m, dpe:] = g_train[traj_id]        \n",
    "        \n",
    "        tar_y[i, :m] = traj[m_ids]\n",
    "        tar_mask[i, :m] = True\n",
    "\n",
    "\n",
    "test_obs0 = torch.zeros((batch_size, n_max, dx+dg+dy), dtype=torch.float32, device=device)\n",
    "test_tar_x0 = torch.zeros((batch_size, t_steps, dx+dg), dtype=torch.float32, device=device)\n",
    "\n",
    "test_obs1 = torch.zeros((batch_size, n_max, dpe+dg+dy), dtype=torch.float32, device=device)\n",
    "test_tar_x1 = torch.zeros((batch_size, t_steps, dpe+dg), dtype=torch.float32, device=device)\n",
    "\n",
    "test_tar_y = torch.zeros((batch_size, t_steps, dy), dtype=torch.float32, device=device)\n",
    "test_obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "\n",
    "def prepare_masked_test_batch(t: list, traj_ids: list, fixed_ind=None):\n",
    "    global test_obs0, test_tar_x0, test_obs1, test_tar_x1, test_tar_y, test_obs_mask\n",
    "    test_obs0.fill_(0)\n",
    "    test_tar_x0.fill_(0)\n",
    "    test_obs1.fill_(0)\n",
    "    test_tar_x1.fill_(0)\n",
    "    test_tar_y.fill_(0)\n",
    "    test_obs_mask.fill_(False)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = t[traj_id]\n",
    "\n",
    "        # n = num_peaks #torch.randint(5, n_max, (1,)).item()\n",
    "        n = torch.randint(1, n_max+1, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = torch.arange(t_steps)\n",
    "\n",
    "        if fixed_ind != None:\n",
    "            for p in range(n):\n",
    "                n_ids[p] = fixed_ind[i, p]\n",
    "            # n_ids[-1] = fixed_ind[i]\n",
    "\n",
    "        test_obs0[i, :n, :dx] = x_test[traj_id, n_ids]  # t\n",
    "        test_obs0[i, :n, dx:dx+dg] = g_test[traj_id]\n",
    "        test_obs0[i, :n, dx+dg:] = traj[n_ids]  # SM(t)\n",
    "\n",
    "        test_obs1[i, :n, :dpe] = pe[n_ids]  # PE(t)\n",
    "        test_obs1[i, :n, dpe:dpe+dg] = g_test[traj_id]\n",
    "        test_obs1[i, :n, dpe+dg:] = traj[n_ids]\n",
    "\n",
    "        test_obs_mask[i, :n] = True\n",
    "        \n",
    "        test_tar_x0[i, :, :dx] = x_test[traj_id, m_ids]\n",
    "        test_tar_x1[i, :, :dpe] = pe[m_ids]\n",
    "\n",
    "        test_tar_y[i] = traj[m_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1001 14:40:01.904000 159340 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New BARE best: 0.09907682240009308, PE best: 1000000\n",
      "New PE best: 0.08318766951560974, BARE best: 0.09907682240009308\n",
      "Epoch: 0, Losses: BARE: 0.0006635457426309585, PE: 0.0006672039926052094\n",
      "Epoch: 1000, Losses: BARE: -2.994173150954768, PE: -3.1377364461943507\n",
      "Epoch: 2000, Losses: BARE: -3.5175858174562453, PE: -3.9502978510558604\n",
      "Epoch: 3000, Losses: BARE: -3.6715526266098024, PE: -4.30749014512077\n",
      "Epoch: 4000, Losses: BARE: -3.7908132340312006, PE: -4.511866146698594\n",
      "New BARE best: 0.0002618885482661426, PE best: 0.08318766951560974\n",
      "New PE best: 0.00015025038737803698, BARE best: 0.0002618885482661426\n",
      "Epoch: 5000, Losses: BARE: -3.884252186730504, PE: -4.6189114342778925\n",
      "Epoch: 6000, Losses: BARE: -3.976704503521323, PE: -4.829912176251411\n",
      "Epoch: 7000, Losses: BARE: -4.112099011957645, PE: -4.940747477516532\n",
      "Epoch: 8000, Losses: BARE: -4.202797061368823, PE: -4.963381580807269\n",
      "Epoch: 9000, Losses: BARE: -4.336421606242657, PE: -5.083743467226625\n",
      "Epoch: 10000, Losses: BARE: -4.423490005537867, PE: -5.1519461616575715\n",
      "Epoch: 11000, Losses: BARE: -4.515233455196023, PE: -5.186760363027453\n",
      "Epoch: 12000, Losses: BARE: -4.561254187650978, PE: -5.239163713879884\n",
      "Epoch: 13000, Losses: BARE: -4.662561699219048, PE: -5.3041754310801625\n",
      "Epoch: 14000, Losses: BARE: -4.709755034163594, PE: -5.344620947942138\n",
      "Epoch: 15000, Losses: BARE: -4.783478358134627, PE: -5.374958230316639\n",
      "Epoch: 16000, Losses: BARE: -4.835002771437168, PE: -5.431148215025663\n",
      "Epoch: 17000, Losses: BARE: -4.88742445152998, PE: -5.4460263177603485\n",
      "Epoch: 18000, Losses: BARE: -4.934865100741386, PE: -5.480093504518271\n",
      "Epoch: 19000, Losses: BARE: -4.940511447280645, PE: -5.5071788409352305\n",
      "Epoch: 20000, Losses: BARE: -4.9557083473801615, PE: -5.538526004642248\n",
      "Epoch: 21000, Losses: BARE: -5.015105011820793, PE: -5.556026175051928\n",
      "Epoch: 22000, Losses: BARE: -5.07530305403471, PE: -5.58088766270876\n",
      "Epoch: 23000, Losses: BARE: -5.1106251297593115, PE: -5.59441446518898\n",
      "Epoch: 24000, Losses: BARE: -5.140706620898098, PE: -5.633740509152412\n",
      "Epoch: 25000, Losses: BARE: -5.1575890769660475, PE: -5.6501139987260105\n",
      "Epoch: 26000, Losses: BARE: -5.220608082771301, PE: -5.666576465040445\n",
      "Epoch: 27000, Losses: BARE: -5.265199043095111, PE: -5.690679001629353\n",
      "Epoch: 28000, Losses: BARE: -5.298676558077335, PE: -5.70824239730835\n",
      "Epoch: 29000, Losses: BARE: -5.341312632024288, PE: -5.729694001793861\n",
      "Epoch: 30000, Losses: BARE: -5.3473176404833795, PE: -5.719457310259342\n",
      "Epoch: 31000, Losses: BARE: -5.381908679366112, PE: -5.746828014373779\n",
      "Epoch: 32000, Losses: BARE: -5.394920146226883, PE: -5.736662953644991\n",
      "Epoch: 33000, Losses: BARE: -5.435761057794094, PE: -5.735553725197911\n",
      "Epoch: 34000, Losses: BARE: -5.471549011826515, PE: -5.790502243697643\n",
      "Epoch: 35000, Losses: BARE: -5.471796380519867, PE: -5.8019440404772755\n",
      "Epoch: 36000, Losses: BARE: -5.497341970086098, PE: -5.83787774759531\n",
      "Epoch: 37000, Losses: BARE: -5.52177084338665, PE: -5.8448227622509\n",
      "Epoch: 38000, Losses: BARE: -5.52525399428606, PE: -5.873024475634098\n",
      "Epoch: 39000, Losses: BARE: -5.553987992227078, PE: -5.893122668683529\n",
      "Epoch: 40000, Losses: BARE: -5.56058021312952, PE: -5.871087895989418\n",
      "Epoch: 41000, Losses: BARE: -5.57964509510994, PE: -5.8979377264380455\n",
      "Epoch: 42000, Losses: BARE: -5.579185656785965, PE: -5.904608230084181\n",
      "Epoch: 43000, Losses: BARE: -5.5953682857751845, PE: -5.945210755944252\n",
      "Epoch: 44000, Losses: BARE: -5.628917067587375, PE: -5.9464885181188585\n",
      "Epoch: 45000, Losses: BARE: -5.616253321886062, PE: -5.923764565348625\n",
      "Epoch: 46000, Losses: BARE: -5.64393306183815, PE: -5.949542759776115\n",
      "Epoch: 47000, Losses: BARE: -5.666689703941345, PE: -5.952269938111305\n",
      "Epoch: 48000, Losses: BARE: -5.661243489801883, PE: -5.961716292500496\n",
      "Epoch: 49000, Losses: BARE: -5.68239270067215, PE: -5.966669993042946\n",
      "Epoch: 50000, Losses: BARE: -5.695505469024181, PE: -5.972245163798332\n",
      "Epoch: 51000, Losses: BARE: -5.696497352659702, PE: -5.983606184363365\n",
      "Epoch: 52000, Losses: BARE: -5.718842155277729, PE: -5.97514245480299\n",
      "Epoch: 53000, Losses: BARE: -5.722089386761189, PE: -6.006350188791751\n",
      "Epoch: 54000, Losses: BARE: -5.728653591752052, PE: -6.004401921361684\n",
      "Epoch: 55000, Losses: BARE: -5.725494636595249, PE: -5.9981942675113675\n",
      "Epoch: 56000, Losses: BARE: -5.747251367866993, PE: -6.000391998559237\n",
      "Epoch: 57000, Losses: BARE: -5.74140293008089, PE: -6.0240905585289\n",
      "Epoch: 58000, Losses: BARE: -5.767278112709522, PE: -6.028851644515991\n",
      "Epoch: 59000, Losses: BARE: -5.7419535961151125, PE: -6.041057191371918\n",
      "Epoch: 60000, Losses: BARE: -5.764236331999302, PE: -6.051090992927551\n",
      "Epoch: 61000, Losses: BARE: -5.760058044075966, PE: -6.03624217030406\n",
      "Epoch: 62000, Losses: BARE: -5.786637254953384, PE: -6.0535116542577745\n",
      "Epoch: 63000, Losses: BARE: -5.7902594032287595, PE: -6.056274729013443\n",
      "Epoch: 64000, Losses: BARE: -5.807490465223789, PE: -6.0745215523242955\n",
      "Epoch: 65000, Losses: BARE: -5.797762596607209, PE: -6.066648134946823\n",
      "Epoch: 66000, Losses: BARE: -5.788578226208687, PE: -6.07791399794817\n",
      "Epoch: 67000, Losses: BARE: -5.794808889627457, PE: -6.100441056013107\n",
      "Epoch: 68000, Losses: BARE: -5.795269599854946, PE: -6.081297208786011\n",
      "Epoch: 69000, Losses: BARE: -5.809334113836289, PE: -6.100668458819389\n",
      "Epoch: 70000, Losses: BARE: -5.807638298869133, PE: -6.106564052343368\n",
      "Epoch: 71000, Losses: BARE: -5.811302179992199, PE: -6.116447351515293\n",
      "Epoch: 72000, Losses: BARE: -5.8275446117818355, PE: -6.114738690257073\n",
      "Epoch: 73000, Losses: BARE: -5.804579308986664, PE: -6.109307626664639\n",
      "Epoch: 74000, Losses: BARE: -5.824028029799462, PE: -6.106157861888408\n",
      "Epoch: 75000, Losses: BARE: -5.834205818653107, PE: -6.119912665963173\n",
      "Epoch: 76000, Losses: BARE: -5.8299363186955455, PE: -6.118925024986267\n",
      "Epoch: 77000, Losses: BARE: -5.841036321878433, PE: -6.120395375311374\n",
      "Epoch: 78000, Losses: BARE: -5.854705257296562, PE: -6.127046249747276\n",
      "Epoch: 79000, Losses: BARE: -5.846455837965012, PE: -6.133268008708954\n",
      "Epoch: 80000, Losses: BARE: -5.8639837042093275, PE: -6.143950084209442\n",
      "Epoch: 81000, Losses: BARE: -5.845780771970749, PE: -6.132804093122482\n",
      "Epoch: 82000, Losses: BARE: -5.858716706454754, PE: -6.137196412205696\n",
      "Epoch: 83000, Losses: BARE: -5.867111358880996, PE: -6.141730909824371\n",
      "Epoch: 84000, Losses: BARE: -5.859555122613907, PE: -6.144203495144844\n",
      "Epoch: 85000, Losses: BARE: -5.879830136299133, PE: -6.153234867691994\n",
      "Epoch: 86000, Losses: BARE: -5.866632783710957, PE: -6.152042866349221\n",
      "Epoch: 87000, Losses: BARE: -5.863198759913445, PE: -6.144874556183815\n",
      "Epoch: 88000, Losses: BARE: -5.86711284840107, PE: -6.156745234847069\n",
      "Epoch: 89000, Losses: BARE: -5.870127996504307, PE: -6.153807813644409\n",
      "Epoch: 90000, Losses: BARE: -5.870106574773788, PE: -6.160640329599381\n",
      "Epoch: 91000, Losses: BARE: -5.890270411968231, PE: -6.1727510253191\n",
      "Epoch: 92000, Losses: BARE: -5.912838539838791, PE: -6.18035365998745\n",
      "Epoch: 93000, Losses: BARE: -5.8994524867534635, PE: -6.1658890542387965\n",
      "Epoch: 94000, Losses: BARE: -5.89137286645174, PE: -6.182067238330841\n",
      "Epoch: 95000, Losses: BARE: -5.9021886989474295, PE: -6.1790118746757505\n",
      "Epoch: 96000, Losses: BARE: -5.919755257070064, PE: -6.185195508241653\n",
      "Epoch: 97000, Losses: BARE: -5.904448724508286, PE: -6.185035260796547\n",
      "Epoch: 98000, Losses: BARE: -5.904910784959793, PE: -6.18765502256155\n",
      "Epoch: 99000, Losses: BARE: -5.900951722621918, PE: -6.184553156137467\n",
      "Epoch: 100000, Losses: BARE: -5.901848194360733, PE: -6.198235577583313\n",
      "Epoch: 101000, Losses: BARE: -5.927992034435272, PE: -6.202016474604607\n",
      "Epoch: 102000, Losses: BARE: -5.927183489441871, PE: -6.189998131752014\n",
      "Epoch: 103000, Losses: BARE: -5.928126745164394, PE: -6.2061232179403305\n",
      "Epoch: 104000, Losses: BARE: -5.91056179702282, PE: -6.201080459475517\n",
      "Epoch: 105000, Losses: BARE: -5.9298038418293, PE: -6.200968611121177\n",
      "Epoch: 106000, Losses: BARE: -5.946972872793674, PE: -6.215301590204239\n",
      "Epoch: 107000, Losses: BARE: -5.945800243616104, PE: -6.210610523223877\n",
      "Epoch: 108000, Losses: BARE: -5.930495547950268, PE: -6.2163408795595165\n",
      "Epoch: 109000, Losses: BARE: -5.930173723220825, PE: -6.214538487553597\n",
      "Epoch: 110000, Losses: BARE: -5.937587140440941, PE: -6.207070510864257\n",
      "Epoch: 111000, Losses: BARE: -5.942813406944275, PE: -6.219217576622963\n",
      "Epoch: 112000, Losses: BARE: -5.944204813480377, PE: -6.21301073718071\n",
      "Epoch: 113000, Losses: BARE: -5.945231800317765, PE: -6.218793866038323\n",
      "Epoch: 114000, Losses: BARE: -5.944193330168724, PE: -6.221856884241104\n",
      "Epoch: 115000, Losses: BARE: -5.951065966546535, PE: -6.232360300660133\n",
      "Epoch: 116000, Losses: BARE: -5.979104546308517, PE: -6.2302282911539075\n",
      "Epoch: 117000, Losses: BARE: -5.969599938154221, PE: -6.231245555341244\n",
      "Epoch: 118000, Losses: BARE: -5.9587332842350005, PE: -6.229735135316849\n",
      "Epoch: 119000, Losses: BARE: -5.972540543973446, PE: -6.241629960536957\n",
      "Epoch: 120000, Losses: BARE: -5.979571395635605, PE: -6.243244230866432\n",
      "Epoch: 121000, Losses: BARE: -5.959556227385998, PE: -6.233573236823082\n",
      "Epoch: 122000, Losses: BARE: -5.961660374522209, PE: -6.224925896048545\n",
      "Epoch: 123000, Losses: BARE: -5.960406967043877, PE: -6.233951674818993\n",
      "Epoch: 124000, Losses: BARE: -5.991855357170105, PE: -6.2551981638669965\n",
      "Epoch: 125000, Losses: BARE: -5.982793744206429, PE: -6.2449963337183\n",
      "Epoch: 126000, Losses: BARE: -5.979347865581513, PE: -6.244397228002549\n",
      "Epoch: 127000, Losses: BARE: -5.991573236584664, PE: -6.247856580972671\n",
      "Epoch: 128000, Losses: BARE: -6.006253141283989, PE: -6.262012872338295\n",
      "Epoch: 129000, Losses: BARE: -6.0066345835328105, PE: -6.266526579022408\n",
      "Epoch: 130000, Losses: BARE: -5.988924450039864, PE: -6.255802590310574\n",
      "Epoch: 131000, Losses: BARE: -5.994255038321018, PE: -6.264228977560997\n",
      "Epoch: 132000, Losses: BARE: -6.005297210931778, PE: -6.26455178642273\n",
      "Epoch: 133000, Losses: BARE: -5.993303365111351, PE: -6.249806813597679\n",
      "Epoch: 134000, Losses: BARE: -5.997050345897675, PE: -6.265028394818306\n",
      "Epoch: 135000, Losses: BARE: -6.004676988899708, PE: -6.252576074182987\n",
      "Epoch: 136000, Losses: BARE: -5.990789238810539, PE: -6.273644557952881\n",
      "Epoch: 137000, Losses: BARE: -6.000719316363335, PE: -6.272462562084198\n",
      "Epoch: 138000, Losses: BARE: -5.99651400899887, PE: -6.272625450968742\n",
      "Epoch: 139000, Losses: BARE: -6.0082708649635315, PE: -6.267377145528793\n",
      "Epoch: 140000, Losses: BARE: -6.0184622115492825, PE: -6.284174479007721\n",
      "Epoch: 141000, Losses: BARE: -6.000986039757729, PE: -6.2663610435724255\n",
      "Epoch: 142000, Losses: BARE: -6.0204463057518005, PE: -6.2855072040557864\n",
      "Epoch: 143000, Losses: BARE: -5.998294618964195, PE: -6.264402594685555\n",
      "Epoch: 144000, Losses: BARE: -6.019644551336765, PE: -6.276150022864342\n",
      "Epoch: 145000, Losses: BARE: -6.017640105485916, PE: -6.283177258729935\n",
      "Epoch: 146000, Losses: BARE: -6.013454646468163, PE: -6.275525392889977\n",
      "Epoch: 147000, Losses: BARE: -6.006688171744346, PE: -6.290775715351105\n",
      "Epoch: 148000, Losses: BARE: -6.039011689662933, PE: -6.288900763809681\n",
      "Epoch: 149000, Losses: BARE: -6.01660027384758, PE: -6.281516078829766\n",
      "Epoch: 150000, Losses: BARE: -6.023270167231559, PE: -6.2921220347881315\n",
      "Epoch: 151000, Losses: BARE: -6.007568515241146, PE: -6.287162424087525\n",
      "Epoch: 152000, Losses: BARE: -6.021905644476414, PE: -6.285427144527436\n",
      "Epoch: 153000, Losses: BARE: -6.01831793808937, PE: -6.288206880271435\n",
      "Epoch: 154000, Losses: BARE: -6.027613365650177, PE: -6.285221375107765\n",
      "Epoch: 155000, Losses: BARE: -6.036615427970887, PE: -6.310165924191475\n",
      "Epoch: 156000, Losses: BARE: -6.036184005916119, PE: -6.3065674694776535\n",
      "Epoch: 157000, Losses: BARE: -6.027060932397842, PE: -6.295166570842266\n",
      "Epoch: 158000, Losses: BARE: -6.0372634105682375, PE: -6.295300374984741\n",
      "Epoch: 159000, Losses: BARE: -6.033862363696098, PE: -6.295367975473404\n",
      "Epoch: 160000, Losses: BARE: -6.01536743941158, PE: -6.304571856498718\n",
      "Epoch: 161000, Losses: BARE: -6.035399757027626, PE: -6.2970442615747455\n",
      "Epoch: 162000, Losses: BARE: -6.042931408524513, PE: -6.297329710304737\n",
      "Epoch: 163000, Losses: BARE: -6.046067940533161, PE: -6.2980754437446596\n",
      "Epoch: 164000, Losses: BARE: -6.042496220827102, PE: -6.303377711057663\n",
      "Epoch: 165000, Losses: BARE: -6.039912871956825, PE: -6.300786921143532\n",
      "Epoch: 166000, Losses: BARE: -6.037792216539383, PE: -6.309789820373059\n",
      "Epoch: 167000, Losses: BARE: -6.044869237184525, PE: -6.315076308786869\n",
      "Epoch: 168000, Losses: BARE: -6.023978671610355, PE: -6.299247955977917\n",
      "Epoch: 169000, Losses: BARE: -6.029622145950794, PE: -6.30838974905014\n",
      "Epoch: 170000, Losses: BARE: -6.043053736805915, PE: -6.299315518021584\n",
      "Epoch: 171000, Losses: BARE: -6.065508473873138, PE: -6.313353842139244\n",
      "Epoch: 172000, Losses: BARE: -6.062342283129692, PE: -6.322306982874871\n",
      "Epoch: 173000, Losses: BARE: -6.052986070871353, PE: -6.321504930853844\n",
      "Epoch: 174000, Losses: BARE: -6.049921882271767, PE: -6.312962662100792\n",
      "Epoch: 175000, Losses: BARE: -6.0568518416881565, PE: -6.32231572496891\n",
      "Epoch: 176000, Losses: BARE: -6.0581363748312, PE: -6.322146555066109\n",
      "Epoch: 177000, Losses: BARE: -6.068148748040199, PE: -6.3246957640647885\n",
      "Epoch: 178000, Losses: BARE: -6.063447407603264, PE: -6.338586567163468\n",
      "Epoch: 179000, Losses: BARE: -6.062738725662231, PE: -6.331161290645599\n",
      "Epoch: 180000, Losses: BARE: -6.061246910691262, PE: -6.328773524403572\n",
      "Epoch: 181000, Losses: BARE: -6.067999748587608, PE: -6.325607783555984\n",
      "Epoch: 182000, Losses: BARE: -6.074183999896049, PE: -6.332939087629319\n",
      "Epoch: 183000, Losses: BARE: -6.060961603045463, PE: -6.333664566636085\n",
      "Epoch: 184000, Losses: BARE: -6.07083992099762, PE: -6.327993454098701\n",
      "Epoch: 185000, Losses: BARE: -6.056770357728005, PE: -6.336912332296372\n",
      "Epoch: 186000, Losses: BARE: -6.074378971457481, PE: -6.316167088359594\n",
      "Epoch: 187000, Losses: BARE: -6.070535649061203, PE: -6.33383704161644\n",
      "Epoch: 188000, Losses: BARE: -6.078735069513321, PE: -6.344184045195579\n",
      "Epoch: 189000, Losses: BARE: -6.078473435044288, PE: -6.347343458294868\n",
      "Epoch: 190000, Losses: BARE: -6.0740315560102465, PE: -6.348006390690804\n",
      "Epoch: 191000, Losses: BARE: -6.066878053665161, PE: -6.343187175035476\n",
      "Epoch: 192000, Losses: BARE: -6.0584046522378925, PE: -6.339002120018005\n",
      "Epoch: 193000, Losses: BARE: -6.0787884479761125, PE: -6.348462619543076\n",
      "Epoch: 194000, Losses: BARE: -6.064984924137592, PE: -6.347354428172111\n",
      "Epoch: 195000, Losses: BARE: -6.063307863354683, PE: -6.344165401995182\n",
      "Epoch: 196000, Losses: BARE: -6.085680823802948, PE: -6.346000911355018\n",
      "Epoch: 197000, Losses: BARE: -6.092874467670917, PE: -6.358947557330132\n",
      "Epoch: 198000, Losses: BARE: -6.085505792975426, PE: -6.351701464891434\n",
      "Epoch: 199000, Losses: BARE: -6.0676981399059295, PE: -6.348568524956703\n",
      "Epoch: 200000, Losses: BARE: -6.08692643558979, PE: -6.349061155676842\n",
      "Epoch: 201000, Losses: BARE: -6.091300708055496, PE: -6.360913733839989\n",
      "Epoch: 202000, Losses: BARE: -6.088886199593544, PE: -6.355845376253128\n",
      "Epoch: 203000, Losses: BARE: -6.079448789179325, PE: -6.359267845869065\n",
      "Epoch: 204000, Losses: BARE: -6.092584924817086, PE: -6.352105083465577\n",
      "Epoch: 205000, Losses: BARE: -6.091600110411644, PE: -6.355976473689079\n",
      "Epoch: 206000, Losses: BARE: -6.095698804736138, PE: -6.369047506570816\n",
      "Epoch: 207000, Losses: BARE: -6.1001276921629906, PE: -6.370637290000915\n",
      "Epoch: 208000, Losses: BARE: -6.091353375971317, PE: -6.36921444106102\n",
      "Epoch: 209000, Losses: BARE: -6.104346349239349, PE: -6.368199491143226\n",
      "Epoch: 210000, Losses: BARE: -6.101400591135025, PE: -6.345277110993862\n",
      "Epoch: 211000, Losses: BARE: -6.102561722815037, PE: -6.3650223991870885\n",
      "Epoch: 212000, Losses: BARE: -6.094925408363342, PE: -6.366592479586601\n",
      "Epoch: 213000, Losses: BARE: -6.104119535446167, PE: -6.374533301115036\n",
      "Epoch: 214000, Losses: BARE: -6.093624664068222, PE: -6.372878560185432\n",
      "Epoch: 215000, Losses: BARE: -6.112763543844223, PE: -6.376342619657517\n",
      "Epoch: 216000, Losses: BARE: -6.10620002514124, PE: -6.369885216116905\n",
      "Epoch: 217000, Losses: BARE: -6.111092744708062, PE: -6.376702646613121\n",
      "Epoch: 218000, Losses: BARE: -6.111304060697556, PE: -6.382695339202881\n",
      "Epoch: 219000, Losses: BARE: -6.110106218159199, PE: -6.364729700922966\n",
      "Epoch: 220000, Losses: BARE: -6.111341121673584, PE: -6.38179637748003\n",
      "Epoch: 221000, Losses: BARE: -6.116878413200379, PE: -6.367316868722439\n",
      "Epoch: 222000, Losses: BARE: -6.12462291097641, PE: -6.3864624300003054\n",
      "Epoch: 223000, Losses: BARE: -6.1121373307704925, PE: -6.3838612011671065\n",
      "Epoch: 224000, Losses: BARE: -6.120406543254853, PE: -6.391028400123119\n",
      "Epoch: 225000, Losses: BARE: -6.108390383005142, PE: -6.376835571527481\n",
      "Epoch: 226000, Losses: BARE: -6.120979698777199, PE: -6.394061579465866\n",
      "Epoch: 227000, Losses: BARE: -6.114611576914787, PE: -6.387971522092819\n",
      "Epoch: 228000, Losses: BARE: -6.130476756930351, PE: -6.382972645044327\n",
      "Epoch: 229000, Losses: BARE: -6.115945836663246, PE: -6.36505375379324\n",
      "Epoch: 230000, Losses: BARE: -6.114310474395752, PE: -6.398443859577179\n",
      "Epoch: 231000, Losses: BARE: -6.115322692453861, PE: -6.395056738376617\n",
      "Epoch: 232000, Losses: BARE: -6.112838550209999, PE: -6.38667203104496\n",
      "Epoch: 233000, Losses: BARE: -6.12367942416668, PE: -6.395129665374756\n",
      "Epoch: 234000, Losses: BARE: -6.12331349682808, PE: -6.394123521208763\n",
      "Epoch: 235000, Losses: BARE: -6.1218613290786745, PE: -6.402983290910721\n",
      "Epoch: 236000, Losses: BARE: -6.1219688667058945, PE: -6.394231880545616\n",
      "Epoch: 237000, Losses: BARE: -6.121951250314712, PE: -6.399359482169151\n",
      "Epoch: 238000, Losses: BARE: -6.129807246923447, PE: -6.4023760466575625\n",
      "Epoch: 239000, Losses: BARE: -6.127336849093437, PE: -6.397794085264206\n",
      "Epoch: 240000, Losses: BARE: -6.12988985055685, PE: -6.401478028059006\n",
      "Epoch: 241000, Losses: BARE: -6.134233433961868, PE: -6.397837235182524\n",
      "Epoch: 242000, Losses: BARE: -6.133920276463032, PE: -6.408112308621407\n",
      "Epoch: 243000, Losses: BARE: -6.139306017041206, PE: -6.400760127842426\n",
      "Epoch: 244000, Losses: BARE: -6.135606521129608, PE: -6.399712173342705\n",
      "Epoch: 245000, Losses: BARE: -6.143059911608696, PE: -6.403794665455818\n",
      "Epoch: 246000, Losses: BARE: -6.129154033899307, PE: -6.404281198501587\n",
      "Epoch: 247000, Losses: BARE: -6.124164914369583, PE: -6.393249584078789\n",
      "Epoch: 248000, Losses: BARE: -6.13836727976799, PE: -6.397463466763496\n",
      "Epoch: 249000, Losses: BARE: -6.1437968970537185, PE: -6.407351623058319\n",
      "Epoch: 250000, Losses: BARE: -6.143456388413906, PE: -6.411403951406479\n",
      "Epoch: 251000, Losses: BARE: -6.144320804834366, PE: -6.404292633652687\n",
      "Epoch: 252000, Losses: BARE: -6.154967908859253, PE: -6.413785714745521\n",
      "Epoch: 253000, Losses: BARE: -6.153751203596592, PE: -6.406356575846672\n",
      "Epoch: 254000, Losses: BARE: -6.1398379052877425, PE: -6.411702534914017\n",
      "Epoch: 255000, Losses: BARE: -6.15367948949337, PE: -6.4112088842988015\n",
      "Epoch: 256000, Losses: BARE: -6.1477425556182865, PE: -6.41392603456974\n",
      "Epoch: 257000, Losses: BARE: -6.145491172909737, PE: -6.402549314975738\n",
      "Epoch: 258000, Losses: BARE: -6.149771062731743, PE: -6.42049785399437\n",
      "Epoch: 259000, Losses: BARE: -6.152289842486382, PE: -6.408750614285469\n",
      "Epoch: 260000, Losses: BARE: -6.145861077427864, PE: -6.404716932624578\n",
      "Epoch: 261000, Losses: BARE: -6.154037463545799, PE: -6.40834591460228\n",
      "Epoch: 262000, Losses: BARE: -6.153830623984337, PE: -6.415140778064727\n",
      "Epoch: 263000, Losses: BARE: -6.1530158959627155, PE: -6.408129751563072\n",
      "Epoch: 264000, Losses: BARE: -6.147475337266922, PE: -6.414631937742233\n",
      "Epoch: 265000, Losses: BARE: -6.155496089220047, PE: -6.419628232121467\n",
      "Epoch: 266000, Losses: BARE: -6.152103616833687, PE: -6.404680322915316\n",
      "Epoch: 267000, Losses: BARE: -6.15430184829235, PE: -6.419869222879409\n",
      "Epoch: 268000, Losses: BARE: -6.140161812841892, PE: -6.4162784349918365\n",
      "Epoch: 269000, Losses: BARE: -6.1660615680217745, PE: -6.431006258130074\n",
      "Epoch: 270000, Losses: BARE: -6.1583705369234085, PE: -6.423398509979248\n",
      "Epoch: 271000, Losses: BARE: -6.16053970515728, PE: -6.427334296822548\n",
      "Epoch: 272000, Losses: BARE: -6.164912106633186, PE: -6.431272982120514\n",
      "Epoch: 273000, Losses: BARE: -6.162482425570488, PE: -6.429054518699646\n",
      "Epoch: 274000, Losses: BARE: -6.161934296250343, PE: -6.425819279909134\n",
      "Epoch: 275000, Losses: BARE: -6.159139083504677, PE: -6.425542126536369\n",
      "Epoch: 276000, Losses: BARE: -6.180297412991524, PE: -6.424533472180366\n",
      "Epoch: 277000, Losses: BARE: -6.164860856533051, PE: -6.4282806140184405\n",
      "Epoch: 278000, Losses: BARE: -6.160963772654533, PE: -6.43626836502552\n",
      "Epoch: 279000, Losses: BARE: -6.170136213302612, PE: -6.4326630693674085\n",
      "Epoch: 280000, Losses: BARE: -6.171232809662819, PE: -6.424910829722881\n",
      "Epoch: 281000, Losses: BARE: -6.1653821325898175, PE: -6.427964389562606\n",
      "Epoch: 282000, Losses: BARE: -6.166527472138405, PE: -6.420560767412185\n",
      "Epoch: 283000, Losses: BARE: -6.169472636461258, PE: -6.439556833744049\n",
      "Epoch: 284000, Losses: BARE: -6.169755467176437, PE: -6.435819922208786\n",
      "Epoch: 285000, Losses: BARE: -6.15858686208725, PE: -6.430512321829796\n",
      "Epoch: 286000, Losses: BARE: -6.169186963915825, PE: -6.423025263369083\n",
      "Epoch: 287000, Losses: BARE: -6.173398155093193, PE: -6.427569667577743\n",
      "Epoch: 288000, Losses: BARE: -6.181618963003158, PE: -6.430102420210838\n",
      "Epoch: 289000, Losses: BARE: -6.166475845098495, PE: -6.439874290585518\n",
      "Epoch: 290000, Losses: BARE: -6.183756292819977, PE: -6.43337768894434\n",
      "Epoch: 291000, Losses: BARE: -6.183985640048981, PE: -6.417979595899582\n",
      "Epoch: 292000, Losses: BARE: -6.17670368206501, PE: -6.437748084306717\n",
      "Epoch: 293000, Losses: BARE: -6.190670580863952, PE: -6.446685581684113\n",
      "Epoch: 294000, Losses: BARE: -6.183688604772091, PE: -6.431477190375328\n",
      "Epoch: 295000, Losses: BARE: -6.177568538069725, PE: -6.439463860630989\n",
      "Epoch: 296000, Losses: BARE: -6.188215088248253, PE: -6.447621601819992\n",
      "Epoch: 297000, Losses: BARE: -6.178306775540113, PE: -6.441019800424576\n",
      "Epoch: 298000, Losses: BARE: -6.183754480183125, PE: -6.444440604150295\n",
      "Epoch: 299000, Losses: BARE: -6.197320459246636, PE: -6.438857817649842\n",
      "Epoch: 300000, Losses: BARE: -6.182880244970321, PE: -6.403399279624224\n",
      "Epoch: 301000, Losses: BARE: -6.192388642549515, PE: -6.43499339723587\n",
      "Epoch: 302000, Losses: BARE: -6.186757974028588, PE: -6.446248389601707\n",
      "Epoch: 303000, Losses: BARE: -6.192225249648094, PE: -6.4258307226896285\n",
      "Epoch: 304000, Losses: BARE: -6.18324624300003, PE: -6.442225224614143\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "timestamp = int(time.time())\n",
    "root_folder = f'../outputs/xarm/bare_pe/{str(timestamp)}/'\n",
    "\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "\n",
    "if not os.path.exists(f'{root_folder}saved_models/'):\n",
    "    os.makedirs(f'{root_folder}saved_models/')\n",
    "\n",
    "img_folder = f'{root_folder}img/'\n",
    "if not os.path.exists(img_folder):\n",
    "    os.makedirs(img_folder)\n",
    "\n",
    "\n",
    "# save network architectures in a txt file:\n",
    "with open(f'{root_folder}networks.txt', 'w') as f:\n",
    "    f.write(str(enc_dims) + '\\n' + str(dec_dims) + '\\n')\n",
    "\n",
    "\n",
    "torch.save(x_train, f'{root_folder}x.pt')\n",
    "torch.save(y_train, f'{root_folder}y.pt')\n",
    "torch.save(g_train, f'{root_folder}g.pt')\n",
    "torch.save(x_test, f'{root_folder}x_test.pt')\n",
    "torch.save(y_test, f'{root_folder}y_test.pt')\n",
    "torch.save(g_test, f'{root_folder}g_test.pt')\n",
    "\n",
    "\n",
    "epochs = 1_000_000\n",
    "epoch_iter = num_demos // batch_size\n",
    "test_epoch_iter = num_test//batch_size\n",
    "avg_loss0, avg_loss1 = 0, 0\n",
    "loss_report_interval = 1000\n",
    "test_per_epoch = 5000\n",
    "min_test_loss0, min_test_loss1 = 1000000, 1000000\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "plot_test = True\n",
    "\n",
    "l0, l1 = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss0, epoch_loss1 = 0, 0\n",
    "\n",
    "    traj_ids = torch.randperm(num_demos)[:batch_size * epoch_iter].chunk(epoch_iter)\n",
    "\n",
    "    for i in range(epoch_iter):\n",
    "        prepare_masked_batch(y_train, traj_ids[i])\n",
    "\n",
    "        opt0.zero_grad()\n",
    "        pred0 = m0(obs0, tar_x0, obs_mask)\n",
    "        loss0 = m0.loss(pred0, tar_y, tar_mask)\n",
    "        loss0.backward()\n",
    "        opt0.step()\n",
    "\n",
    "        epoch_loss0 += loss0.item()\n",
    "\n",
    "\n",
    "        opt1.zero_grad()\n",
    "        pred1 = m1(obs1, tar_x1, obs_mask)\n",
    "        loss1 = m1.loss(pred1, tar_y, tar_mask)\n",
    "        loss1.backward()\n",
    "        opt1.step()\n",
    "\n",
    "        epoch_loss1 += loss1.item()\n",
    "\n",
    "\n",
    "    if epoch % test_per_epoch == 0:# and epoch > 0:\n",
    "        test_traj_ids = torch.randperm(num_test)[:batch_size*test_epoch_iter].chunk(test_epoch_iter)\n",
    "        test_loss0, test_loss1 = 0, 0\n",
    "\n",
    "        for j in range(test_epoch_iter):\n",
    "            prepare_masked_test_batch(y_test, test_traj_ids[j])\n",
    "\n",
    "            pred0 = m0.val(test_obs0, test_tar_x0, test_obs_mask)  # (batch_size, t_steps, 2*dy)\n",
    "            pred1 = m1.val(test_obs1, test_tar_x1, test_obs_mask)  # (batch_size, t_steps, 2*dy)\n",
    "            \n",
    "            if plot_test:\n",
    "                epoch_code = str(epoch).zfill(len(str(epochs))-1)\n",
    "                for k in range(batch_size):\n",
    "                    current_n = test_obs_mask[k].sum().item()  # n points inside the condition\n",
    "                    plt_x_data = x_test[test_traj_ids[j][0], :]  # common for all plots\n",
    "                    fig, ax = plt.subplots(2, dy, figsize=(dy*plt_size_coeff, 2*plt_size_coeff))\n",
    "                    for dimension in range(dy):\n",
    "                        pred_cnmp = pred0[k, :, dimension].cpu().numpy()\n",
    "                        pred_pemp = pred1[k, :, dimension].cpu().numpy()\n",
    "                        # max_y = max(np.max(pred_cnmp), np.max(pred_pemp))\n",
    "                        # min_y = min(np.min(pred_cnmp), np.min(pred_pemp))\n",
    "\n",
    "                        ax[0, dimension].set_ylim(min_y, max_y)\n",
    "\n",
    "                        ax[0, dimension].scatter(test_obs0[k, :current_n, :dx].cpu().numpy(), test_obs0[k, :current_n, dx+dg+dimension].cpu().numpy(), color='black', s=30)\n",
    "                        ax[0, dimension].plot(plt_x_data, test_tar_y[k, :, dimension].cpu().numpy(), color=dark_gray)\n",
    "                        ax[0, dimension].plot(plt_x_data, pred_cnmp, color=colors[0])  # bare prediction\n",
    "                        ax[0, dimension].grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "                        ax[1, dimension].set_ylim(min_y, max_y)\n",
    "\n",
    "                        ax[1, dimension].scatter(test_obs0[k, :current_n, :dx].cpu().numpy(), test_obs0[k, :current_n, dx+dg+dimension].cpu().numpy(), color='black', s=30)\n",
    "                        ax[1, dimension].plot(plt_x_data, test_tar_y[k, :, dimension].cpu().numpy(), color=dark_gray)\n",
    "                        ax[1, dimension].plot(plt_x_data, pred_pemp, color=colors[1])  # pemp prediction\n",
    "                        ax[1, dimension].grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "                    fig.suptitle(f'Epoch: {epoch}', fontsize=24)\n",
    "                    fig.legend(handles=handles, loc='upper right', fontsize=24, frameon=True, framealpha=1, prop=dict(weight='bold'), handlelength=3, handleheight=2)\n",
    "                    plt.savefig(f'{img_folder}{epoch_code}_{test_traj_ids[j][k]}.png')\n",
    "                    plt.close()                    \n",
    "\n",
    "            test_loss0 += mse_loss(pred0[:, :, :m0.output_dim], test_tar_y).item()\n",
    "            test_loss1 += mse_loss(pred1[:, :, :m1.output_dim], test_tar_y).item()\n",
    "        \n",
    "        test_loss0 /= test_epoch_iter\n",
    "        test_loss1 /= test_epoch_iter\n",
    "            \n",
    "        if test_loss0 < min_test_loss0:\n",
    "            min_test_loss0 = test_loss0\n",
    "            print(f'New BARE best: {min_test_loss0}, PE best: {min_test_loss1}')\n",
    "            torch.save(m0_.state_dict(), f'{root_folder}saved_models/bare.pt')\n",
    "\n",
    "        if test_loss1 < min_test_loss1:\n",
    "            min_test_loss1 = test_loss1\n",
    "            print(f'New PE best: {min_test_loss1}, BARE best: {min_test_loss0}')\n",
    "            torch.save(m1_.state_dict(), f'{root_folder}saved_models/pe.pt')\n",
    "\n",
    "\n",
    "    epoch_loss0 /= epoch_iter\n",
    "    epoch_loss1 /= epoch_iter\n",
    "\n",
    "    avg_loss0 += epoch_loss0\n",
    "    avg_loss1 += epoch_loss1\n",
    "\n",
    "    l0.append(epoch_loss0)\n",
    "    l1.append(epoch_loss1)\n",
    "\n",
    "    if epoch % loss_report_interval == 0:\n",
    "        print(\"Epoch: {}, Losses: BARE: {}, PE: {}\".format(epoch, avg_loss0/loss_report_interval, avg_loss1/loss_report_interval))\n",
    "        avg_loss0, avg_loss1 = 0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(l0, f'{root_folder}losses_bare.pt')\n",
    "torch.save(l1, f'{root_folder}losses_pe.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
