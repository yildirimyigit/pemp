{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "folder_path = '../models/'\n",
    "if folder_path not in sys.path:\n",
    "    sys.path.append(folder_path)\n",
    "\n",
    "folder_path = '../data/'\n",
    "if folder_path not in sys.path:\n",
    "    sys.path.append(folder_path)\n",
    "\n",
    "from cnmp import CNMP\n",
    "\n",
    "from data_generators import *\n",
    "from positional_encoders import *\n",
    "from plotters import *\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "def get_free_gpu():\n",
    "    gpu_util = []\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_device(i)  # Switch GPU\n",
    "#        gpu_util.append((i, torch.cuda.memory_stats()['reserved_bytes.all.current'] / (1024 ** 2)))\n",
    "        gpu_util.append((i, torch.cuda.utilization()))\n",
    "    gpu_util.sort(key=lambda x: x[1])\n",
    "    return gpu_util[0][0]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    available_gpu = get_free_gpu()\n",
    "    if available_gpu == 0:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{available_gpu}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(\"Device :\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([180, 1200, 1]), y_train shape: torch.Size([180, 1200, 1]), g_train shape: torch.Size([180, 1])\n",
      "x_test shape: torch.Size([20, 1200, 1]), y_test shape: torch.Size([20, 1200, 1]), g_test shape: torch.Size([20, 1])\n"
     ]
    }
   ],
   "source": [
    "dx, dy, dg, dph, dpe = 1, 1, 1, 0, 27\n",
    "num_demos, num_test = 180, 20\n",
    "num_trajs = num_demos + num_test\n",
    "t_steps = 1200\n",
    "n_max, m_max = 100, 100\n",
    "max_freq = 5\n",
    "\n",
    "trajectories, _, freqs = generate_combined_cyclic_trajectories_with_random_freqs(num_trajs=num_trajs, max_freq=max_freq, freq=True)\n",
    "\n",
    "perm_ids = torch.randperm(num_trajs)\n",
    "train_ids, test_ids = perm_ids[:num_demos], perm_ids[num_demos:]\n",
    "\n",
    "all_x = torch.linspace(0, 1, t_steps).unsqueeze(-1).unsqueeze(0).repeat(num_trajs,1,1)\n",
    "\n",
    "x_train, x_test = all_x[train_ids], all_x[test_ids]\n",
    "y_train, y_test = trajectories[train_ids], trajectories[test_ids]\n",
    "g_train, g_test = freqs[train_ids]/max_freq, freqs[test_ids]/max_freq\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}, g_train shape: {g_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}, g_test shape: {g_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = generate_positional_encoding(t_steps, dpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bare:  34050\n",
      "PE:  40706\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "\n",
    "enc_dims = [256,256]\n",
    "dec_dims = [256,256]\n",
    "\n",
    "m0_ = CNMP(input_dim=dx+dg, output_dim=dy, n_max=n_max, m_max=m_max, encoder_hidden_dims=enc_dims, decoder_hidden_dims=dec_dims, batch_size=batch_size, device=device)\n",
    "opt0 = torch.optim.Adam(lr=3e-4, params=m0_.parameters())\n",
    "\n",
    "m1_ = CNMP(input_dim=dpe+dg, output_dim=dy, n_max=n_max, m_max=m_max, encoder_hidden_dims=enc_dims, decoder_hidden_dims=dec_dims, batch_size=batch_size, device=device)\n",
    "opt1 = torch.optim.Adam(lr=3e-4, params=m1_.parameters())\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in m0_.parameters())\n",
    "print('Bare: ', pytorch_total_params)\n",
    "pytorch_total_params = sum(p.numel() for p in m1_.parameters())\n",
    "print('PE: ', pytorch_total_params)\n",
    "\n",
    "if torch.__version__ >= \"2.0\":\n",
    "    m0, m1 = torch.compile(m0_), torch.compile(m1_)\n",
    "else:\n",
    "    m0, m1 = m0_, m1_\n",
    "\n",
    "\n",
    "# save network architectures in a txt file:\n",
    "with open('networks.txt', 'w') as f:\n",
    "    f.write(str(enc_dims) + '\\n' + str(dec_dims) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs0 = torch.zeros((batch_size, n_max, dx+dg+dy), dtype=torch.float32, device=device)\n",
    "tar_x0 = torch.zeros((batch_size, m_max, dx+dg), dtype=torch.float32, device=device)\n",
    "\n",
    "obs1 = torch.zeros((batch_size, n_max, dpe+dg+dy), dtype=torch.float32, device=device)\n",
    "tar_x1 = torch.zeros((batch_size, m_max, dpe+dg), dtype=torch.float32, device=device)\n",
    "\n",
    "tar_y = torch.zeros((batch_size, m_max, dy), dtype=torch.float32, device=device)\n",
    "obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "tar_mask = torch.zeros((batch_size, m_max), dtype=torch.bool, device=device)\n",
    "\n",
    "def prepare_masked_batch(t: list, traj_ids: list):\n",
    "    global obs0, tar_x0, obs1, tar_x1, tar_y, obs_mask, tar_mask\n",
    "    obs0.fill_(0)\n",
    "    tar_x0.fill_(0)\n",
    "    obs1.fill_(0)\n",
    "    tar_x1.fill_(0)\n",
    "    tar_y.fill_(0)\n",
    "    obs_mask.fill_(False)\n",
    "    tar_mask.fill_(False)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = t[traj_id]\n",
    "\n",
    "        n = torch.randint(1, n_max+1, (1,)).item()\n",
    "        m = torch.randint(1, m_max+1, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = permuted_ids[n:n+m]\n",
    "\n",
    "        obs0[i, :n, :dx] = x_train[traj_id, n_ids]  # t\n",
    "        obs0[i, :n, dx:dx+dg] = g_train[traj_id]  # gamma\n",
    "        obs0[i, :n, dx+dg:] = traj[n_ids]  # SM(t)\n",
    "\n",
    "        obs1[i, :n, :dpe] = pe[n_ids]  # PE(t)\n",
    "        obs1[i, :n, dpe:dpe+dg] = g_train[traj_id]  # gamma\n",
    "        obs1[i, :n, dpe+dg:] = traj[n_ids]  # SM(t)\n",
    "\n",
    "        obs_mask[i, :n] = True\n",
    "        \n",
    "        tar_x0[i, :m, :dx] = x_train[traj_id, m_ids]\n",
    "        tar_x0[i, :m, dx:] = g_train[traj_id]\n",
    "        tar_x1[i, :m, :dpe] = pe[m_ids]\n",
    "        tar_x1[i, :m, dpe:] = g_train[traj_id]        \n",
    "        \n",
    "        tar_y[i, :m] = traj[m_ids]\n",
    "        tar_mask[i, :m] = True\n",
    "\n",
    "\n",
    "test_obs0 = torch.zeros((batch_size, n_max, dx+dg+dy), dtype=torch.float32, device=device)\n",
    "test_tar_x0 = torch.zeros((batch_size, t_steps, dx+dg), dtype=torch.float32, device=device)\n",
    "\n",
    "test_obs1 = torch.zeros((batch_size, n_max, dpe+dg+dy), dtype=torch.float32, device=device)\n",
    "test_tar_x1 = torch.zeros((batch_size, t_steps, dpe+dg), dtype=torch.float32, device=device)\n",
    "\n",
    "test_tar_y = torch.zeros((batch_size, t_steps, dy), dtype=torch.float32, device=device)\n",
    "test_obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "last_obs_vals = torch.zeros((batch_size, n_max, dx), dtype=torch.int32, device=device)  # only for plotting\n",
    "\n",
    "def prepare_masked_test_batch(t: list, traj_ids: list, fixed_ind=None):\n",
    "    global test_obs0, test_tar_x0, test_obs1, test_tar_x1, test_tar_y, test_obs_mask, last_obs_vals\n",
    "    test_obs0.fill_(0)\n",
    "    test_tar_x0.fill_(0)\n",
    "    test_obs1.fill_(0)\n",
    "    test_tar_x1.fill_(0)\n",
    "    test_tar_y.fill_(0)\n",
    "    test_obs_mask.fill_(False)\n",
    "    last_obs_vals.fill_(0)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = t[traj_id]\n",
    "\n",
    "        # n = num_peaks #torch.randint(5, n_max, (1,)).item()\n",
    "        n = torch.randint(1, n_max+1, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = torch.arange(t_steps)\n",
    "\n",
    "        if fixed_ind != None:\n",
    "            for p in range(n):\n",
    "                n_ids[p] = fixed_ind[i, p]\n",
    "            # n_ids[-1] = fixed_ind[i]\n",
    "\n",
    "        test_obs0[i, :n, :dx] = x_test[traj_id, n_ids]  # t\n",
    "        test_obs0[i, :n, dx:dx+dg] = g_test[traj_id]\n",
    "        test_obs0[i, :n, dx+dg:] = traj[n_ids]  # SM(t)\n",
    "\n",
    "        test_obs1[i, :n, :dpe] = pe[n_ids]  # PE(t)\n",
    "        test_obs1[i, :n, dpe:dpe+dg] = g_test[traj_id]\n",
    "        test_obs1[i, :n, dpe+dg:] = traj[n_ids]\n",
    "\n",
    "        last_obs_vals[i, :n] = n_ids.unsqueeze(-1)\n",
    "        test_obs_mask[i, :n] = True\n",
    "        \n",
    "        test_tar_x0[i, :, :dx] = x_test[traj_id, m_ids]\n",
    "        test_tar_x1[i, :, :dpe] = pe[m_ids]\n",
    "\n",
    "        test_tar_y[i] = traj[m_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New BARE best: 0.4805751144886017, PE best: 1000000\n",
      "New PE best: 0.4823389947414398, BARE best: 0.4805751144886017\n",
      "Epoch: 0, Losses: BARE: 0.0020947991477118597, PE: 0.0020996497207217747\n",
      "Epoch: 500, Losses: BARE: 0.9823245603375976, PE: 0.782776571151283\n",
      "New BARE best: 0.40679073333740234, PE best: 0.4823389947414398\n",
      "New PE best: 0.25278159976005554, BARE best: 0.40679073333740234\n",
      "Epoch: 1000, Losses: BARE: 0.8681922046608398, PE: 0.5393350491458365\n",
      "Epoch: 1500, Losses: BARE: 0.7526092399855447, PE: 0.2803082454632623\n",
      "New BARE best: 0.3631131947040558, PE best: 0.25278159976005554\n",
      "New PE best: 0.17135368287563324, BARE best: 0.3631131947040558\n",
      "Epoch: 2000, Losses: BARE: 0.6616379880176656, PE: 0.11877294044570343\n",
      "Epoch: 2500, Losses: BARE: 0.6188881224485427, PE: 0.0004599067205110271\n",
      "Epoch: 3000, Losses: BARE: 0.5931179508942755, PE: -0.08925180146675682\n",
      "Epoch: 3500, Losses: BARE: 0.5727869559701716, PE: -0.14591847250833365\n",
      "New BARE best: 0.3516524136066437, PE best: 0.17135368287563324\n",
      "New PE best: 0.11499936133623123, BARE best: 0.3516524136066437\n",
      "Epoch: 4000, Losses: BARE: 0.5562319113491742, PE: -0.1933944062854751\n",
      "Epoch: 4500, Losses: BARE: 0.5370531943594625, PE: -0.23677904670922553\n",
      "Epoch: 5000, Losses: BARE: 0.5255348382614329, PE: -0.2802353904898434\n",
      "Epoch: 5500, Losses: BARE: 0.5145726981605014, PE: -0.30530229678763\n",
      "New BARE best: 0.33914661407470703, PE best: 0.11499936133623123\n",
      "New PE best: 0.10804709792137146, BARE best: 0.33914661407470703\n",
      "Epoch: 6000, Losses: BARE: 0.49631608482873274, PE: -0.3416782672976582\n",
      "Epoch: 6500, Losses: BARE: 0.48654695749937826, PE: -0.37081638481600215\n",
      "New PE best: 0.0936553105711937, BARE best: 0.33914661407470703\n",
      "Epoch: 7000, Losses: BARE: 0.47296481298228416, PE: -0.4107772257857982\n",
      "Epoch: 7500, Losses: BARE: 0.4648149753294139, PE: -0.43618990030034915\n",
      "Epoch: 8000, Losses: BARE: 0.45188754098709766, PE: -0.47371255984641697\n",
      "Epoch: 8500, Losses: BARE: 0.4468995392482419, PE: -0.4976974971288969\n",
      "Epoch: 9000, Losses: BARE: 0.43141772983706644, PE: -0.5275549371715429\n",
      "Epoch: 9500, Losses: BARE: 0.4216607188330698, PE: -0.5571903659378101\n",
      "New PE best: 0.08363635838031769, BARE best: 0.33914661407470703\n",
      "Epoch: 10000, Losses: BARE: 0.40907719463077685, PE: -0.5827631103461092\n",
      "Epoch: 10500, Losses: BARE: 0.4013000566470617, PE: -0.6069473461234011\n",
      "Epoch: 11000, Losses: BARE: 0.3920274673532243, PE: -0.6303105344031423\n",
      "Epoch: 11500, Losses: BARE: 0.3795417258352583, PE: -0.6539707391629666\n",
      "New PE best: 0.07194998860359192, BARE best: 0.33914661407470703\n",
      "Epoch: 12000, Losses: BARE: 0.373276689850787, PE: -0.6757125175072715\n",
      "Epoch: 12500, Losses: BARE: 0.36155217056503475, PE: -0.6938632504075459\n",
      "New PE best: 0.06539715826511383, BARE best: 0.33914661407470703\n",
      "Epoch: 13000, Losses: BARE: 0.34667571203449526, PE: -0.7182708178265531\n",
      "Epoch: 13500, Losses: BARE: 0.33983402103261107, PE: -0.7422664931960814\n",
      "Epoch: 14000, Losses: BARE: 0.32915124503722, PE: -0.7611348275782338\n",
      "Epoch: 14500, Losses: BARE: 0.3195336112832757, PE: -0.7848483850310245\n",
      "New PE best: 0.060348499566316605, BARE best: 0.33914661407470703\n",
      "Epoch: 15000, Losses: BARE: 0.31090661772811673, PE: -0.8018277703771994\n",
      "Epoch: 15500, Losses: BARE: 0.29777194280474867, PE: -0.8261335251501037\n",
      "New PE best: 0.04676838219165802, BARE best: 0.33914661407470703\n",
      "Epoch: 16000, Losses: BARE: 0.2900708574787235, PE: -0.8435419257672299\n",
      "Epoch: 16500, Losses: BARE: 0.28015193113159004, PE: -0.8631139519529214\n",
      "New PE best: 0.037230584770441055, BARE best: 0.33914661407470703\n",
      "Epoch: 17000, Losses: BARE: 0.2740504325122492, PE: -0.8831320209602511\n",
      "Epoch: 17500, Losses: BARE: 0.26060717321732013, PE: -0.9001972795658638\n",
      "Epoch: 18000, Losses: BARE: 0.2525409098547108, PE: -0.9190495773255815\n",
      "Epoch: 18500, Losses: BARE: 0.24175985402764077, PE: -0.9364753547973107\n",
      "New PE best: 0.028165100142359734, BARE best: 0.33914661407470703\n",
      "Epoch: 19000, Losses: BARE: 0.23279027265699948, PE: -0.9525841808517777\n",
      "Epoch: 19500, Losses: BARE: 0.22333573499297213, PE: -0.9712865201334151\n",
      "New PE best: 0.02075992524623871, BARE best: 0.33914661407470703\n",
      "Epoch: 20000, Losses: BARE: 0.21146725311841522, PE: -0.9892615600824356\n",
      "Epoch: 20500, Losses: BARE: 0.1993585476565722, PE: -1.009469217485852\n",
      "Epoch: 21000, Losses: BARE: 0.18507433945997998, PE: -1.0229462427165776\n",
      "Epoch: 21500, Losses: BARE: 0.17945856323018156, PE: -1.0345533650517458\n",
      "New PE best: 0.01774727553129196, BARE best: 0.33914661407470703\n",
      "Epoch: 22000, Losses: BARE: 0.1696387314125369, PE: -1.0488581223885218\n",
      "Epoch: 22500, Losses: BARE: 0.154154914364906, PE: -1.0600904305544168\n",
      "Epoch: 23000, Losses: BARE: 0.14830096019845276, PE: -1.0683398009869787\n",
      "Epoch: 23500, Losses: BARE: 0.13231980924253486, PE: -1.0786843826240968\n",
      "Epoch: 24000, Losses: BARE: 0.12530259088552098, PE: -1.0894625937740003\n",
      "Epoch: 24500, Losses: BARE: 0.1145059161243197, PE: -1.0956264650954146\n",
      "Epoch: 25000, Losses: BARE: 0.10481366441688526, PE: -1.105313544366095\n",
      "Epoch: 25500, Losses: BARE: 0.09671975320083587, PE: -1.1137318612072207\n",
      "Epoch: 26000, Losses: BARE: 0.09272251803830862, PE: -1.1150385248594812\n",
      "Epoch: 26500, Losses: BARE: 0.07829401327289137, PE: -1.1243820301757914\n",
      "Epoch: 27000, Losses: BARE: 0.06926096894402144, PE: -1.1369649915628968\n",
      "Epoch: 27500, Losses: BARE: 0.05959285378769955, PE: -1.1325425247185752\n",
      "Epoch: 28000, Losses: BARE: 0.054687617033669866, PE: -1.146502737217478\n",
      "Epoch: 28500, Losses: BARE: 0.04088074366358946, PE: -1.1520845438639324\n",
      "Epoch: 29000, Losses: BARE: 0.03578753050477741, PE: -1.1591065385043624\n",
      "Epoch: 29500, Losses: BARE: 0.030085248434705933, PE: -1.1632611387968061\n",
      "Epoch: 30000, Losses: BARE: 0.02469384143789404, PE: -1.1629756905568969\n",
      "Epoch: 30500, Losses: BARE: 0.017212186753387655, PE: -1.1784245160553193\n",
      "Epoch: 31000, Losses: BARE: 0.012517278006761442, PE: -1.1776534554825888\n",
      "Epoch: 31500, Losses: BARE: 0.0065816107766845065, PE: -1.1805393767754235\n",
      "Epoch: 32000, Losses: BARE: 0.004094955672543114, PE: -1.1942548217243618\n",
      "Epoch: 32500, Losses: BARE: -0.005461993160283269, PE: -1.194299848490292\n",
      "New PE best: 0.01739690639078617, BARE best: 0.33914661407470703\n",
      "Epoch: 33000, Losses: BARE: -0.005714534925878979, PE: -1.1925740214321352\n",
      "Epoch: 33500, Losses: BARE: -0.014943004585260893, PE: -1.2007095908588818\n",
      "New PE best: 0.011923188343644142, BARE best: 0.33914661407470703\n",
      "Epoch: 34000, Losses: BARE: -0.018807576436188845, PE: -1.2045893306202344\n",
      "Epoch: 34500, Losses: BARE: -0.024114833383306007, PE: -1.207052029450733\n",
      "Epoch: 35000, Losses: BARE: -0.025727231106919757, PE: -1.2113532087604206\n",
      "Epoch: 35500, Losses: BARE: -0.029634280131398953, PE: -1.2166228337287903\n",
      "Epoch: 36000, Losses: BARE: -0.03652488960480777, PE: -1.2183581813176458\n",
      "Epoch: 36500, Losses: BARE: -0.040594062837871654, PE: -1.2235423922273854\n",
      "Epoch: 37000, Losses: BARE: -0.04223821840215776, PE: -1.2223068299558415\n",
      "Epoch: 37500, Losses: BARE: -0.04754200106107195, PE: -1.2263774236970488\n",
      "Epoch: 38000, Losses: BARE: -0.049277571439313744, PE: -1.2329640847908123\n",
      "Epoch: 38500, Losses: BARE: -0.05322525585740127, PE: -1.2388683537708391\n",
      "Epoch: 39000, Losses: BARE: -0.05795639829677791, PE: -1.2424613783160836\n",
      "Epoch: 39500, Losses: BARE: -0.06096923355427303, PE: -1.2403262235456056\n",
      "Epoch: 40000, Losses: BARE: -0.06704652965926002, PE: -1.245531660550169\n",
      "Epoch: 40500, Losses: BARE: -0.0738134374163653, PE: -1.2547997110154896\n",
      "New PE best: 0.010647820308804512, BARE best: 0.33914661407470703\n",
      "Epoch: 41000, Losses: BARE: -0.07272424095976424, PE: -1.2488227670590086\n",
      "Epoch: 41500, Losses: BARE: -0.07594570779161752, PE: -1.2508753448062473\n",
      "Epoch: 42000, Losses: BARE: -0.08055200418901223, PE: -1.2622130675713223\n",
      "Epoch: 42500, Losses: BARE: -0.08195497385845757, PE: -1.2601068290852846\n",
      "Epoch: 43000, Losses: BARE: -0.08146121091986105, PE: -1.2578853581812643\n",
      "Epoch: 43500, Losses: BARE: -0.09234545097660475, PE: -1.267944559084044\n",
      "Epoch: 44000, Losses: BARE: -0.09031135964251413, PE: -1.271875778271093\n",
      "Epoch: 44500, Losses: BARE: -0.09031654177043913, PE: -1.2704322090148914\n",
      "Epoch: 45000, Losses: BARE: -0.09281903693649919, PE: -1.2798360732661367\n",
      "Epoch: 45500, Losses: BARE: -0.09777765732372863, PE: -1.2761141466167243\n",
      "New PE best: 0.009865920059382915, BARE best: 0.33914661407470703\n",
      "Epoch: 46000, Losses: BARE: -0.1004717796454369, PE: -1.280499161468612\n",
      "Epoch: 46500, Losses: BARE: -0.10041043843693878, PE: -1.2727456673913535\n",
      "Epoch: 47000, Losses: BARE: -0.10602019733628436, PE: -1.2850759512715872\n",
      "Epoch: 47500, Losses: BARE: -0.10685181736921485, PE: -1.2895166368749413\n",
      "Epoch: 48000, Losses: BARE: -0.11418112888132284, PE: -1.2933394721680211\n",
      "Epoch: 48500, Losses: BARE: -0.11436146308203626, PE: -1.2893410225179482\n",
      "Epoch: 49000, Losses: BARE: -0.11601779636606807, PE: -1.2924652826454905\n",
      "Epoch: 49500, Losses: BARE: -0.11957430325940928, PE: -1.296721628824869\n",
      "Epoch: 50000, Losses: BARE: -0.12166541172324723, PE: -1.2920253265168935\n",
      "Epoch: 50500, Losses: BARE: -0.12223817065793742, PE: -1.2956430320739756\n",
      "Epoch: 51000, Losses: BARE: -0.126850394981547, PE: -1.30164742033018\n",
      "Epoch: 51500, Losses: BARE: -0.12826245736000902, PE: -1.30214853612582\n",
      "Epoch: 52000, Losses: BARE: -0.12709746623331902, PE: -1.3060034181409417\n",
      "Epoch: 52500, Losses: BARE: -0.13544192618883769, PE: -1.3096479925711948\n",
      "Epoch: 53000, Losses: BARE: -0.13413917103726436, PE: -1.3059096628824878\n",
      "Epoch: 53500, Losses: BARE: -0.13488340709064472, PE: -1.3090110427406103\n",
      "Epoch: 54000, Losses: BARE: -0.13991702537150108, PE: -1.313298985461393\n",
      "Epoch: 54500, Losses: BARE: -0.14670667394466333, PE: -1.3208204804791337\n",
      "Epoch: 55000, Losses: BARE: -0.14710243271526038, PE: -1.318372535142634\n",
      "Epoch: 55500, Losses: BARE: -0.14663640546829995, PE: -1.3229616168604954\n",
      "Epoch: 56000, Losses: BARE: -0.1505857211394097, PE: -1.3207176954083986\n",
      "Epoch: 56500, Losses: BARE: -0.15250415661935945, PE: -1.321458327790102\n",
      "Epoch: 57000, Losses: BARE: -0.15702752587656146, PE: -1.325625697745218\n",
      "Epoch: 57500, Losses: BARE: -0.1561404243655593, PE: -1.3237162433597778\n",
      "Epoch: 58000, Losses: BARE: -0.15986648145647214, PE: -1.3299935792817008\n",
      "Epoch: 58500, Losses: BARE: -0.1658757085352522, PE: -1.3330788761377332\n",
      "Epoch: 59000, Losses: BARE: -0.16475076640583236, PE: -1.3357572301891119\n",
      "Epoch: 59500, Losses: BARE: -0.16905555030182579, PE: -1.3284604779548121\n",
      "Epoch: 60000, Losses: BARE: -0.1682274893783113, PE: -1.3372997953097019\n",
      "Epoch: 60500, Losses: BARE: -0.17521861215328438, PE: -1.3362159064875698\n",
      "Epoch: 61000, Losses: BARE: -0.17454822537128015, PE: -1.339489477965567\n",
      "Epoch: 61500, Losses: BARE: -0.1790820883080572, PE: -1.3441216915316048\n",
      "Epoch: 62000, Losses: BARE: -0.175264907205238, PE: -1.3368669139279266\n",
      "Epoch: 62500, Losses: BARE: -0.17520393760770733, PE: -1.3399708498186533\n",
      "Epoch: 63000, Losses: BARE: -0.18248800680815797, PE: -1.3468624128103264\n",
      "Epoch: 63500, Losses: BARE: -0.18411896450941767, PE: -1.343373550852138\n",
      "Epoch: 64000, Losses: BARE: -0.18761403621635836, PE: -1.3459780789746156\n",
      "Epoch: 64500, Losses: BARE: -0.1864727007526123, PE: -1.3531184102561735\n",
      "Epoch: 65000, Losses: BARE: -0.19038940885960298, PE: -1.3555859088765254\n",
      "Epoch: 65500, Losses: BARE: -0.18944896044141177, PE: -1.355003289699555\n",
      "Epoch: 66000, Losses: BARE: -0.1931408065595121, PE: -1.354078104628457\n",
      "Epoch: 66500, Losses: BARE: -0.19211809834040966, PE: -1.3553927471902647\n",
      "Epoch: 67000, Losses: BARE: -0.19561739254674312, PE: -1.3519154729710696\n",
      "Epoch: 67500, Losses: BARE: -0.19935369776839307, PE: -1.3583734200663036\n",
      "Epoch: 68000, Losses: BARE: -0.20315324058789228, PE: -1.3616159422662515\n",
      "Epoch: 68500, Losses: BARE: -0.20405402107171391, PE: -1.3585782789521745\n",
      "Epoch: 69000, Losses: BARE: -0.2037623429096291, PE: -1.3556902189519657\n",
      "Epoch: 69500, Losses: BARE: -0.2051390768517923, PE: -1.3659485223558219\n",
      "Epoch: 70000, Losses: BARE: -0.20728773913389065, PE: -1.3662597799963423\n",
      "Epoch: 70500, Losses: BARE: -0.21156895997087447, PE: -1.363378787444699\n",
      "Epoch: 71000, Losses: BARE: -0.21281272906591528, PE: -1.3675970036321217\n",
      "Epoch: 71500, Losses: BARE: -0.21378963114805638, PE: -1.366624399450091\n",
      "Epoch: 72000, Losses: BARE: -0.21929621261659005, PE: -1.371158569176991\n",
      "Epoch: 72500, Losses: BARE: -0.2157030753420349, PE: -1.36577632621924\n",
      "Epoch: 73000, Losses: BARE: -0.22038426773489073, PE: -1.3723705573810465\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m traj_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandperm(num_demos)[:batch_size \u001b[38;5;241m*\u001b[39m epoch_iter]\u001b[38;5;241m.\u001b[39mchunk(epoch_iter)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch_iter):\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mprepare_masked_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraj_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     opt0\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     46\u001b[0m     pred0 \u001b[38;5;241m=\u001b[39m m0(obs0, tar_x0, obs_mask)\n",
      "Cell \u001b[0;32mIn[5], line 47\u001b[0m, in \u001b[0;36mprepare_masked_batch\u001b[0;34m(t, traj_ids)\u001b[0m\n\u001b[1;32m     44\u001b[0m tar_x1[i, :m, dpe:] \u001b[38;5;241m=\u001b[39m g_train[traj_id]        \n\u001b[1;32m     46\u001b[0m tar_y[i, :m] \u001b[38;5;241m=\u001b[39m traj[m_ids]\n\u001b[0;32m---> 47\u001b[0m \u001b[43mtar_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "timestamp = int(time.time())\n",
    "root_folder = f'../outputs/comparison/mind_change/freq/combined/bare_pe_promp_gmm/{str(timestamp)}/'\n",
    "\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "\n",
    "if not os.path.exists(f'{root_folder}saved_models/'):\n",
    "    os.makedirs(f'{root_folder}saved_models/')\n",
    "\n",
    "img_folder = f'{root_folder}img/'\n",
    "if not os.path.exists(img_folder):\n",
    "    os.makedirs(img_folder)\n",
    "\n",
    "torch.save(x_train, f'{root_folder}x.pt')\n",
    "torch.save(y_train, f'{root_folder}y.pt')\n",
    "torch.save(g_train, f'{root_folder}g.pt')\n",
    "torch.save(x_test, f'{root_folder}x_test.pt')\n",
    "torch.save(y_test, f'{root_folder}y_test.pt')\n",
    "torch.save(g_test, f'{root_folder}g_test.pt')\n",
    "\n",
    "\n",
    "epochs = 1_000_000\n",
    "epoch_iter = num_demos // batch_size\n",
    "test_epoch_iter = num_test//batch_size\n",
    "avg_loss0, avg_loss1 = 0, 0\n",
    "loss_report_interval = 2000\n",
    "test_per_epoch = 2000\n",
    "min_test_loss0, min_test_loss1 = 1000000, 1000000\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "plot_test = False\n",
    "\n",
    "l0, l1 = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss0, epoch_loss1 = 0, 0\n",
    "\n",
    "    traj_ids = torch.randperm(num_demos)[:batch_size * epoch_iter].chunk(epoch_iter)\n",
    "\n",
    "    for i in range(epoch_iter):\n",
    "        prepare_masked_batch(y_train, traj_ids[i])\n",
    "\n",
    "        opt0.zero_grad()\n",
    "        pred0 = m0(obs0, tar_x0, obs_mask)\n",
    "        loss0 = m0.loss(pred0, tar_y, tar_mask)\n",
    "        loss0.backward()\n",
    "        opt0.step()\n",
    "\n",
    "        epoch_loss0 += loss0.item()\n",
    "\n",
    "\n",
    "        opt1.zero_grad()\n",
    "        pred1 = m1(obs1, tar_x1, obs_mask)\n",
    "        loss1 = m1.loss(pred1, tar_y, tar_mask)\n",
    "        loss1.backward()\n",
    "        opt1.step()\n",
    "\n",
    "        epoch_loss1 += loss1.item()\n",
    "\n",
    "\n",
    "    if epoch % test_per_epoch == 0:# and epoch > 0:\n",
    "        test_traj_ids = torch.randperm(num_test)[:batch_size*test_epoch_iter].chunk(test_epoch_iter)\n",
    "        test_loss0, test_loss1 = 0, 0\n",
    "\n",
    "        for j in range(test_epoch_iter):\n",
    "            prepare_masked_test_batch(y_test, test_traj_ids[j])\n",
    "\n",
    "            pred0 = m0.val(test_obs0, test_tar_x0, test_obs_mask)\n",
    "            pred1 = m1.val(test_obs1, test_tar_x1, test_obs_mask)\n",
    "            \n",
    "            if plot_test:\n",
    "                for k in range(batch_size):\n",
    "                    current_n = test_obs_mask[k].sum().item()\n",
    "                    plt.scatter(last_obs_vals[k, :current_n, :dx].cpu().numpy(), test_obs0[k, :current_n, dx+dg:].cpu().numpy(), label='Condition')\n",
    "                    plt.plot(test_tar_y[k, :, 0].cpu().numpy(), label=f\"Groundtruth\")\n",
    "                    plt.plot(pred0[k, :, 0].cpu().numpy(), label=f\"Prediction\")\n",
    "                    \n",
    "                    plt.legend(loc='upper left')\n",
    "                    plt.title(f'Epoch: {epoch}', fontsize=20)\n",
    "                    plt.savefig(f'{img_folder}{epoch}_{test_traj_ids[j][k]}_bare.png')\n",
    "                    plt.clf()\n",
    "\n",
    "                    plt.scatter(last_obs_vals[k, :current_n, :dx].cpu().numpy(), test_obs1[k, :current_n, dpe+dg:].cpu().numpy(), label='Condition')\n",
    "                    plt.plot(test_tar_y[k, :, 0].cpu().numpy(), label=f\"Groundtruth\")\n",
    "                    plt.plot(pred1[k, :, 0].cpu().numpy(), label=f\"Prediction\")\n",
    "                    \n",
    "                    plt.legend(loc='upper left')\n",
    "                    plt.title(f'Epoch: {epoch}', fontsize=20)\n",
    "                    plt.savefig(f'{img_folder}{epoch}_{test_traj_ids[j][k]}_pe.png')\n",
    "                    plt.clf()\n",
    "                    \n",
    "\n",
    "            test_loss0 += mse_loss(pred0[:, :, :m0.output_dim], test_tar_y).item()\n",
    "            test_loss1 += mse_loss(pred1[:, :, :m1.output_dim], test_tar_y).item()\n",
    "        \n",
    "        test_loss0 /= test_epoch_iter\n",
    "        test_loss1 /= test_epoch_iter\n",
    "            \n",
    "        if test_loss0 < min_test_loss0:\n",
    "            min_test_loss0 = test_loss0\n",
    "            print(f'New BARE best: {min_test_loss0}, PE best: {min_test_loss1}')\n",
    "            torch.save(m0_.state_dict(), f'{root_folder}saved_models/bare.pt')\n",
    "\n",
    "        if test_loss1 < min_test_loss1:\n",
    "            min_test_loss1 = test_loss1\n",
    "            print(f'New PE best: {min_test_loss1}, BARE best: {min_test_loss0}')\n",
    "            torch.save(m1_.state_dict(), f'{root_folder}saved_models/pe.pt')\n",
    "\n",
    "\n",
    "    epoch_loss0 /= epoch_iter\n",
    "    epoch_loss1 /= epoch_iter\n",
    "\n",
    "    avg_loss0 += epoch_loss0\n",
    "    avg_loss1 += epoch_loss1\n",
    "\n",
    "    l0.append(epoch_loss0)\n",
    "    l1.append(epoch_loss1)\n",
    "\n",
    "    if epoch % loss_report_interval == 0:\n",
    "        print(\"Epoch: {}, Losses: BARE: {}, PE: {}\".format(epoch, avg_loss0/loss_report_interval, avg_loss1/loss_report_interval))\n",
    "        avg_loss0, avg_loss1 = 0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(l0, f'{root_folder}losses_bare.pt')\n",
    "torch.save(l1, f'{root_folder}losses_pe.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
