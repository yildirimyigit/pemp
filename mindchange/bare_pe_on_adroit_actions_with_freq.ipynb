{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "folder_path = '../models/'\n",
    "if folder_path not in sys.path:\n",
    "    sys.path.append(folder_path)\n",
    "\n",
    "folder_path = '../data/'\n",
    "if folder_path not in sys.path:\n",
    "    sys.path.append(folder_path)\n",
    "\n",
    "from cnmp import CNMP\n",
    "from positional_encoders import *\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "def get_free_gpu():\n",
    "    gpu_util = []\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_device(i)  # Switch GPU\n",
    "#        gpu_util.append((i, torch.cuda.memory_stats()['reserved_bytes.all.current'] / (1024 ** 2)))\n",
    "        gpu_util.append((i, torch.cuda.utilization()))\n",
    "    gpu_util.sort(key=lambda x: x[1])\n",
    "    return gpu_util[0][0]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    available_gpu = get_free_gpu()\n",
    "    if available_gpu == 0:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{available_gpu}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(\"Device :\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([8, 200, 1]), y_train shape: torch.Size([8, 200, 26]), g_train shape: torch.Size([8])\n",
      "x_test shape: torch.Size([2, 200, 1]), y_test shape: torch.Size([2, 200, 26]), g_test shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "dx, dy, dg, dph, dpe = 1, 26, 1, 0, 27\n",
    "num_demos, num_test = 8, 2\n",
    "num_trajs = num_demos + num_test\n",
    "t_steps = 200\n",
    "n_max, m_max = 20, 20\n",
    "\n",
    "trajectories, freqs = torch.from_numpy(np.load('../sim/data/adroit_actions_10.npy')), torch.from_numpy(np.load('../sim/data/adroit_freqs_10.npy'))\n",
    "max_freq = max(freqs)\n",
    "\n",
    "perm_ids = torch.randperm(num_trajs)\n",
    "train_ids, test_ids = perm_ids[:num_demos], perm_ids[num_demos:]\n",
    "\n",
    "all_x = torch.linspace(0, 1, t_steps).unsqueeze(-1).unsqueeze(0).repeat(num_trajs,1,1)\n",
    "\n",
    "x_train, x_test = all_x[train_ids], all_x[test_ids]\n",
    "y_train, y_test = trajectories[train_ids], trajectories[test_ids]\n",
    "g_train, g_test = freqs[train_ids], freqs[test_ids]\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}, g_train shape: {g_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}, g_test shape: {g_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = generate_positional_encoding(t_steps, dpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bare:  284468\n",
      "PE:  297780\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "enc_dims = [256,256,256]\n",
    "dec_dims = [256,256,256]\n",
    "\n",
    "m0_ = CNMP(input_dim=dx+dg, output_dim=dy, n_max=n_max, m_max=m_max, encoder_hidden_dims=enc_dims, decoder_hidden_dims=dec_dims, batch_size=batch_size, device=device)\n",
    "opt0 = torch.optim.Adam(lr=3e-4, params=m0_.parameters())\n",
    "\n",
    "m1_ = CNMP(input_dim=dpe+dg, output_dim=dy, n_max=n_max, m_max=m_max, encoder_hidden_dims=enc_dims, decoder_hidden_dims=dec_dims, batch_size=batch_size, device=device)\n",
    "opt1 = torch.optim.Adam(lr=3e-4, params=m1_.parameters())\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in m0_.parameters())\n",
    "print('Bare: ', pytorch_total_params)\n",
    "pytorch_total_params = sum(p.numel() for p in m1_.parameters())\n",
    "print('PE: ', pytorch_total_params)\n",
    "\n",
    "if torch.__version__ >= \"2.0\":\n",
    "    m0, m1 = torch.compile(m0_), torch.compile(m1_)\n",
    "else:\n",
    "    m0, m1 = m0_, m1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs0 = torch.zeros((batch_size, n_max, dx+dg+dy), dtype=torch.float32, device=device)\n",
    "tar_x0 = torch.zeros((batch_size, m_max, dx+dg), dtype=torch.float32, device=device)\n",
    "\n",
    "obs1 = torch.zeros((batch_size, n_max, dpe+dg+dy), dtype=torch.float32, device=device)\n",
    "tar_x1 = torch.zeros((batch_size, m_max, dpe+dg), dtype=torch.float32, device=device)\n",
    "\n",
    "tar_y = torch.zeros((batch_size, m_max, dy), dtype=torch.float32, device=device)\n",
    "obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "tar_mask = torch.zeros((batch_size, m_max), dtype=torch.bool, device=device)\n",
    "\n",
    "def prepare_masked_batch(t: list, traj_ids: list):\n",
    "    global obs0, tar_x0, obs1, tar_x1, tar_y, obs_mask, tar_mask\n",
    "    obs0.fill_(0)\n",
    "    tar_x0.fill_(0)\n",
    "    obs1.fill_(0)\n",
    "    tar_x1.fill_(0)\n",
    "    tar_y.fill_(0)\n",
    "    obs_mask.fill_(False)\n",
    "    tar_mask.fill_(False)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = t[traj_id]\n",
    "\n",
    "        n = torch.randint(1, n_max+1, (1,)).item()\n",
    "        m = torch.randint(1, m_max+1, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = permuted_ids[n:n+m]\n",
    "\n",
    "        obs0[i, :n, :dx] = x_train[traj_id, n_ids]  # t\n",
    "        obs0[i, :n, dx:dx+dg] = g_train[traj_id]  # gamma\n",
    "        obs0[i, :n, dx+dg:] = traj[n_ids]  # SM(t)\n",
    "\n",
    "        obs1[i, :n, :dpe] = pe[n_ids]  # PE(t)\n",
    "        obs1[i, :n, dpe:dpe+dg] = g_train[traj_id]  # gamma\n",
    "        obs1[i, :n, dpe+dg:] = traj[n_ids]  # SM(t)\n",
    "\n",
    "        obs_mask[i, :n] = True\n",
    "        \n",
    "        tar_x0[i, :m, :dx] = x_train[traj_id, m_ids]\n",
    "        tar_x0[i, :m, dx:] = g_train[traj_id]\n",
    "        tar_x1[i, :m, :dpe] = pe[m_ids]\n",
    "        tar_x1[i, :m, dpe:] = g_train[traj_id]        \n",
    "        \n",
    "        tar_y[i, :m] = traj[m_ids]\n",
    "        tar_mask[i, :m] = True\n",
    "\n",
    "\n",
    "test_obs0 = torch.zeros((batch_size, n_max, dx+dg+dy), dtype=torch.float32, device=device)\n",
    "test_tar_x0 = torch.zeros((batch_size, t_steps, dx+dg), dtype=torch.float32, device=device)\n",
    "\n",
    "test_obs1 = torch.zeros((batch_size, n_max, dpe+dg+dy), dtype=torch.float32, device=device)\n",
    "test_tar_x1 = torch.zeros((batch_size, t_steps, dpe+dg), dtype=torch.float32, device=device)\n",
    "\n",
    "test_tar_y = torch.zeros((batch_size, t_steps, dy), dtype=torch.float32, device=device)\n",
    "test_obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "last_obs_vals = torch.zeros((batch_size, n_max, dx), dtype=torch.int32, device=device)  # only for plotting\n",
    "\n",
    "def prepare_masked_test_batch(t: list, traj_ids: list, fixed_ind=None):\n",
    "    global test_obs0, test_tar_x0, test_obs1, test_tar_x1, test_tar_y, test_obs_mask, last_obs_vals\n",
    "    test_obs0.fill_(0)\n",
    "    test_tar_x0.fill_(0)\n",
    "    test_obs1.fill_(0)\n",
    "    test_tar_x1.fill_(0)\n",
    "    test_tar_y.fill_(0)\n",
    "    test_obs_mask.fill_(False)\n",
    "    last_obs_vals.fill_(0)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = t[traj_id]\n",
    "\n",
    "        # n = num_peaks #torch.randint(5, n_max, (1,)).item()\n",
    "        n = torch.randint(1, n_max+1, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = torch.arange(t_steps)\n",
    "\n",
    "        if fixed_ind != None:\n",
    "            for p in range(n):\n",
    "                n_ids[p] = fixed_ind[i, p]\n",
    "            # n_ids[-1] = fixed_ind[i]\n",
    "\n",
    "        test_obs0[i, :n, :dx] = x_test[traj_id, n_ids]  # t\n",
    "        test_obs0[i, :n, dx:dx+dg] = g_test[traj_id]\n",
    "        test_obs0[i, :n, dx+dg:] = traj[n_ids]  # SM(t)\n",
    "\n",
    "        test_obs1[i, :n, :dpe] = pe[n_ids]  # PE(t)\n",
    "        test_obs1[i, :n, dpe:dpe+dg] = g_test[traj_id]\n",
    "        test_obs1[i, :n, dpe+dg:] = traj[n_ids]\n",
    "\n",
    "        last_obs_vals[i, :n] = n_ids.unsqueeze(-1)\n",
    "        test_obs_mask[i, :n] = True\n",
    "        \n",
    "        test_tar_x0[i, :, :dx] = x_test[traj_id, m_ids]\n",
    "        test_tar_x1[i, :, :dpe] = pe[m_ids]\n",
    "\n",
    "        test_tar_y[i] = traj[m_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 20:16:01.390000 5151 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New BARE best: 0.726684033870697, PE best: 1000000\n",
      "New PE best: 0.7228080034255981, BARE best: 0.726684033870697\n",
      "Epoch: 0, Losses: BARE: 0.00262143337726593, PE: 0.0026146227121353147\n",
      "Epoch: 500, Losses: BARE: 0.28264610717922917, PE: -0.1394380020330427\n",
      "New BARE best: 0.3435661792755127, PE best: 0.7228080034255981\n",
      "New PE best: 0.011049019172787666, BARE best: 0.3435661792755127\n",
      "Epoch: 1000, Losses: BARE: 0.20177183074696223, PE: -1.0932853024257347\n",
      "Epoch: 1500, Losses: BARE: 0.08573703569755889, PE: -1.4863670998737215\n",
      "New BARE best: 0.29354098439216614, PE best: 0.011049019172787666\n",
      "New PE best: 0.00615084171295166, BARE best: 0.29354098439216614\n",
      "Epoch: 2000, Losses: BARE: -0.014865175706683657, PE: -1.6581996026933192\n",
      "Epoch: 2500, Losses: BARE: -0.12301926732331049, PE: -1.9694706524740904\n",
      "New BARE best: 0.2910517156124115, PE best: 0.00615084171295166\n",
      "New PE best: 0.002676875563338399, BARE best: 0.2910517156124115\n",
      "Epoch: 3000, Losses: BARE: -0.1642832631850615, PE: -2.159065267311409\n",
      "Epoch: 3500, Losses: BARE: -0.11288110737944954, PE: -2.269321221984923\n",
      "New BARE best: 0.272916316986084, PE best: 0.002676875563338399\n",
      "New PE best: 0.0015812298515811563, BARE best: 0.272916316986084\n",
      "Epoch: 4000, Losses: BARE: -0.2198237234590997, PE: -2.339751327343285\n",
      "Epoch: 4500, Losses: BARE: -0.3235117831095122, PE: -2.4533479252681136\n",
      "New BARE best: 0.23757554590702057, PE best: 0.0015812298515811563\n",
      "New PE best: 0.0011257410515099764, BARE best: 0.23757554590702057\n",
      "Epoch: 5000, Losses: BARE: -0.5063696407936513, PE: -2.5655278779957444\n",
      "Epoch: 5500, Losses: BARE: -0.6549851453732699, PE: -2.6576622735857964\n",
      "New BARE best: 0.17337480187416077, PE best: 0.0011257410515099764\n",
      "Epoch: 6000, Losses: BARE: -0.697295547792688, PE: -2.7347628087624907\n",
      "Epoch: 6500, Losses: BARE: -0.8297709881439805, PE: -2.768987765699625\n",
      "New BARE best: 0.1363491415977478, PE best: 0.0011257410515099764\n",
      "New PE best: 0.000911157054360956, BARE best: 0.1363491415977478\n",
      "Epoch: 7000, Losses: BARE: -0.9347442412786185, PE: -2.8500965382866563\n",
      "Epoch: 7500, Losses: BARE: -0.9795687702791765, PE: -2.927225883461535\n",
      "New BARE best: 0.11645989120006561, PE best: 0.000911157054360956\n",
      "New PE best: 0.0008953530923463404, BARE best: 0.11645989120006561\n",
      "Epoch: 8000, Losses: BARE: -1.07262314135395, PE: -2.8362839062809946\n",
      "Epoch: 8500, Losses: BARE: -1.1253849288336932, PE: -2.9621316492296756\n",
      "New BARE best: 0.10343102365732193, PE best: 0.0008953530923463404\n",
      "Epoch: 9000, Losses: BARE: -1.2056151755005122, PE: -2.9600522986501456\n",
      "Epoch: 9500, Losses: BARE: -1.0806665273658, PE: -3.0426356042698024\n",
      "Epoch: 10000, Losses: BARE: -1.1768174963891507, PE: -2.97936114692688\n",
      "Epoch: 10500, Losses: BARE: -1.286532632432878, PE: -3.0246151940226556\n",
      "New BARE best: 0.08882953971624374, PE best: 0.0008953530923463404\n",
      "Epoch: 11000, Losses: BARE: -1.3934575219899417, PE: -3.095012130767107\n",
      "Epoch: 11500, Losses: BARE: -1.3489757409635932, PE: -3.1004609598368407\n",
      "Epoch: 12000, Losses: BARE: -1.4547716255839913, PE: -3.1427706238925457\n",
      "Epoch: 12500, Losses: BARE: -1.5078064600378276, PE: -3.018970564411953\n",
      "Epoch: 13000, Losses: BARE: -1.5613923997841774, PE: -3.177657124735415\n",
      "Epoch: 13500, Losses: BARE: -1.5761360580082984, PE: -3.1340482222139836\n",
      "Epoch: 14000, Losses: BARE: -1.6493786401860415, PE: -3.153872982367873\n",
      "Epoch: 14500, Losses: BARE: -1.5852637723200023, PE: -3.180700787037611\n",
      "Epoch: 15000, Losses: BARE: -1.733435908775311, PE: -3.2263018602877853\n",
      "Epoch: 15500, Losses: BARE: -1.7646356950700284, PE: -3.2468186006695032\n",
      "Epoch: 16000, Losses: BARE: -1.8200272318199278, PE: -3.264639179207385\n",
      "Epoch: 16500, Losses: BARE: -1.8591288424283265, PE: -3.2709886530488728\n",
      "Epoch: 17000, Losses: BARE: -1.8912849722653626, PE: -3.312933537989855\n",
      "Epoch: 17500, Losses: BARE: -1.847987008497119, PE: -3.298849263653159\n",
      "Epoch: 18000, Losses: BARE: -1.8959133693352341, PE: -3.3348396720588207\n",
      "Epoch: 18500, Losses: BARE: -1.9504112267941236, PE: -3.3543898013234137\n",
      "Epoch: 19000, Losses: BARE: -1.9565110868886113, PE: -3.344026397690177\n",
      "Epoch: 19500, Losses: BARE: -1.9789959212131798, PE: -3.2723425530791284\n",
      "Epoch: 20000, Losses: BARE: -2.0345888980701567, PE: -3.3502179023846983\n",
      "Epoch: 20500, Losses: BARE: -1.9949571463074536, PE: -3.3603681294247507\n",
      "New PE best: 0.0008031540783122182, BARE best: 0.08882953971624374\n",
      "Epoch: 21000, Losses: BARE: -2.0016884715259073, PE: -3.3697906531989577\n",
      "Epoch: 21500, Losses: BARE: -2.052008919775486, PE: -3.399961255028844\n",
      "Epoch: 22000, Losses: BARE: -2.1212667453736067, PE: -3.41945961406827\n",
      "Epoch: 22500, Losses: BARE: -2.1252456471435726, PE: -3.429161186248064\n",
      "Epoch: 23000, Losses: BARE: -2.1544956875164063, PE: -3.4269801024198534\n",
      "Epoch: 23500, Losses: BARE: -2.1989330831095577, PE: -3.454503402143717\n",
      "Epoch: 24000, Losses: BARE: -2.195820388033986, PE: -3.4407237533926964\n",
      "Epoch: 24500, Losses: BARE: -2.092951813042164, PE: -3.448178118161857\n",
      "Epoch: 25000, Losses: BARE: -2.2404794611483814, PE: -3.489466248422861\n",
      "Epoch: 25500, Losses: BARE: -2.1507656121551992, PE: -3.464222727447748\n",
      "Epoch: 26000, Losses: BARE: -2.19794085149467, PE: -3.424264227323234\n",
      "Epoch: 26500, Losses: BARE: -2.206519443824887, PE: -3.50213188880682\n",
      "Epoch: 27000, Losses: BARE: -2.291232749603689, PE: -3.4955191766768694\n",
      "Epoch: 27500, Losses: BARE: -2.226229860320687, PE: -3.432456599570811\n",
      "Epoch: 28000, Losses: BARE: -2.2731259552910923, PE: -3.494928554683924\n",
      "Epoch: 28500, Losses: BARE: -2.3272997045107187, PE: -3.5693133678883315\n",
      "Epoch: 29000, Losses: BARE: -2.3346598858982324, PE: -3.5030281255692244\n",
      "Epoch: 29500, Losses: BARE: -2.304213172119111, PE: -3.551660446230322\n",
      "Epoch: 30000, Losses: BARE: -2.242327580213547, PE: -3.4871696862205863\n",
      "Epoch: 30500, Losses: BARE: -2.3255736541077496, PE: -3.3835258471518754\n",
      "Epoch: 31000, Losses: BARE: -2.369288688018918, PE: -3.514661867454648\n",
      "Epoch: 31500, Losses: BARE: -2.3839442800013346, PE: -3.5892410391122103\n",
      "Epoch: 32000, Losses: BARE: -2.3330070759654045, PE: -3.61472422748059\n",
      "Epoch: 32500, Losses: BARE: -2.357708520784974, PE: -3.463653853222728\n",
      "Epoch: 33000, Losses: BARE: -2.4056263675391674, PE: -3.539134514577687\n",
      "Epoch: 33500, Losses: BARE: -2.3929550100713968, PE: -3.5987867126464845\n",
      "Epoch: 34000, Losses: BARE: -2.3425954889059066, PE: -3.5811822330504657\n",
      "Epoch: 34500, Losses: BARE: -2.3447445115298033, PE: -3.5158251138925554\n",
      "Epoch: 35000, Losses: BARE: -2.4067507824599743, PE: -3.6095566972494124\n",
      "Epoch: 35500, Losses: BARE: -2.4515470521450045, PE: -3.5595331415943803\n",
      "Epoch: 36000, Losses: BARE: -2.3760176523029806, PE: -3.5613748497962954\n",
      "Epoch: 36500, Losses: BARE: -2.48628761318326, PE: -3.683981313467026\n",
      "Epoch: 37000, Losses: BARE: -2.4574267788231374, PE: -3.65259641122818\n",
      "Epoch: 37500, Losses: BARE: -2.472484626159072, PE: -3.624242140330374\n",
      "Epoch: 38000, Losses: BARE: -2.498820386789739, PE: -3.6275506421551107\n",
      "Epoch: 38500, Losses: BARE: -2.4483160533979533, PE: -3.665132120460272\n",
      "Epoch: 39000, Losses: BARE: -2.5236295486795717, PE: -3.6818099959790707\n",
      "Epoch: 39500, Losses: BARE: -2.5338659009337423, PE: -3.6763801742494104\n",
      "Epoch: 40000, Losses: BARE: -2.4571558768693356, PE: -3.6465013692378996\n",
      "Epoch: 40500, Losses: BARE: -2.533867137461901, PE: -3.6185995184481143\n",
      "Epoch: 41000, Losses: BARE: -2.5606976323649286, PE: -3.6525194610953333\n",
      "Epoch: 41500, Losses: BARE: -2.5390694447159765, PE: -3.6639011702835558\n",
      "Epoch: 42000, Losses: BARE: -2.6147573160082103, PE: -3.6918556586205957\n",
      "Epoch: 42500, Losses: BARE: -2.51240372062847, PE: -3.713066232010722\n",
      "Epoch: 43000, Losses: BARE: -2.5810037111118436, PE: -3.7181622174605726\n",
      "Epoch: 43500, Losses: BARE: -2.574844942137599, PE: -3.717406751796603\n",
      "Epoch: 44000, Losses: BARE: -2.600705719985068, PE: -3.6977831990420817\n",
      "Epoch: 44500, Losses: BARE: -2.5883730335533617, PE: -3.6998920805752276\n",
      "Epoch: 45000, Losses: BARE: -2.334988700952381, PE: -3.751656507074833\n",
      "Epoch: 45500, Losses: BARE: -2.571903984874487, PE: -2.4363605412840843\n",
      "Epoch: 46000, Losses: BARE: -2.3747567120715978, PE: -3.558941359654069\n",
      "Epoch: 46500, Losses: BARE: -2.4541741927564145, PE: -3.605031898111105\n",
      "Epoch: 47000, Losses: BARE: -2.6124276635199783, PE: -3.60529526623711\n",
      "Epoch: 47500, Losses: BARE: -2.5989259065538644, PE: -3.605039342008531\n",
      "Epoch: 48000, Losses: BARE: -2.6149034921824934, PE: -3.690982932329178\n",
      "Epoch: 48500, Losses: BARE: -2.607598611938767, PE: -3.6477889619022608\n",
      "Epoch: 49000, Losses: BARE: -2.5996601446568968, PE: -3.680623472198844\n",
      "Epoch: 49500, Losses: BARE: -2.571946897625923, PE: -3.694978383541107\n",
      "Epoch: 50000, Losses: BARE: -2.572626028850675, PE: -3.6487321093119682\n",
      "Epoch: 50500, Losses: BARE: -2.5849350528120993, PE: -3.6910445526838305\n",
      "Epoch: 51000, Losses: BARE: -2.698061655141413, PE: -3.7110121996998786\n",
      "Epoch: 51500, Losses: BARE: -2.6237704003602267, PE: -3.7227805126309397\n",
      "Epoch: 52000, Losses: BARE: -2.6251011938415467, PE: -3.722823925033212\n",
      "Epoch: 52500, Losses: BARE: -2.6828517009690405, PE: -3.7232312722206116\n",
      "Epoch: 53000, Losses: BARE: -2.6834379297345876, PE: -3.742665695257485\n",
      "Epoch: 53500, Losses: BARE: -2.631304944485426, PE: -3.740082736633718\n",
      "Epoch: 54000, Losses: BARE: -2.70361828635633, PE: -3.76340803784132\n",
      "Epoch: 54500, Losses: BARE: -2.7149504420496524, PE: -3.77296836745739\n",
      "Epoch: 55000, Losses: BARE: -2.686047537341714, PE: -3.7680083939284086\n",
      "Epoch: 55500, Losses: BARE: -2.714553992241621, PE: -3.7600465246737005\n",
      "Epoch: 56000, Losses: BARE: -2.71442032431066, PE: -3.7588010008484125\n",
      "Epoch: 56500, Losses: BARE: -2.7281873404234647, PE: -3.777793314859271\n",
      "Epoch: 57000, Losses: BARE: -2.711531377956271, PE: -3.7505965549051763\n",
      "Epoch: 57500, Losses: BARE: -2.7303202621117233, PE: -3.7963942923396825\n",
      "Epoch: 58000, Losses: BARE: -2.7405691194012762, PE: -3.8086301656626165\n",
      "Epoch: 58500, Losses: BARE: -2.728224469386041, PE: -3.7667890064120293\n",
      "Epoch: 59000, Losses: BARE: -2.7429421374350786, PE: -3.782488207757473\n",
      "Epoch: 59500, Losses: BARE: -2.7681647638827562, PE: -3.82002582257241\n",
      "Epoch: 60000, Losses: BARE: -2.735867631748319, PE: -3.8206888799369336\n",
      "Epoch: 60500, Losses: BARE: -2.7627874938249586, PE: -3.7827710894942284\n",
      "Epoch: 61000, Losses: BARE: -2.665613946985453, PE: -3.8759653855264187\n",
      "Epoch: 61500, Losses: BARE: -2.7298482621610165, PE: -3.791361566759646\n",
      "Epoch: 62000, Losses: BARE: -2.7427733396664262, PE: -3.775399705350399\n",
      "Epoch: 62500, Losses: BARE: -2.7830291829258202, PE: -3.8179309273604303\n",
      "Epoch: 63000, Losses: BARE: -2.8136869404138998, PE: -3.8675027928948404\n",
      "Epoch: 63500, Losses: BARE: -2.3270879301130774, PE: -3.843327913083136\n",
      "Epoch: 64000, Losses: BARE: -2.6811206864714623, PE: -3.847495596885681\n",
      "Epoch: 64500, Losses: BARE: -2.7331374034136533, PE: -3.8690981138125062\n",
      "Epoch: 65000, Losses: BARE: -2.774402458615601, PE: -3.8188790335655214\n",
      "Epoch: 65500, Losses: BARE: -2.7906736040860416, PE: -3.860472555398941\n",
      "Epoch: 66000, Losses: BARE: -2.7642502235919237, PE: -3.773456835448742\n",
      "Epoch: 66500, Losses: BARE: -2.803891359269619, PE: -3.826699373304844\n",
      "Epoch: 67000, Losses: BARE: -2.8128385273963215, PE: -3.838204871356487\n",
      "Epoch: 67500, Losses: BARE: -2.8400847445651887, PE: -3.8501367909014226\n",
      "Epoch: 68000, Losses: BARE: -2.8332830332666634, PE: -3.8402263881685212\n",
      "Epoch: 68500, Losses: BARE: -2.7548961872681974, PE: -3.8768060478940605\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "timestamp = int(time.time())\n",
    "root_folder = f'../outputs/sim/adroit/bare_pe/{str(timestamp)}/'\n",
    "\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "\n",
    "if not os.path.exists(f'{root_folder}saved_models/'):\n",
    "    os.makedirs(f'{root_folder}saved_models/')\n",
    "\n",
    "img_folder = f'{root_folder}img/'\n",
    "if not os.path.exists(img_folder):\n",
    "    os.makedirs(img_folder)\n",
    "\n",
    "torch.save(y_train, f'{root_folder}y.pt')\n",
    "\n",
    "\n",
    "epochs = 10_000_000\n",
    "epoch_iter = num_demos // batch_size\n",
    "test_epoch_iter = num_test//batch_size\n",
    "avg_loss0, avg_loss1 = 0, 0\n",
    "loss_report_interval = 500\n",
    "test_per_epoch = 1000\n",
    "min_test_loss0, min_test_loss1 = 1000000, 1000000\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "plot_test = True\n",
    "\n",
    "l0, l1 = [], []\n",
    "\n",
    "\n",
    "# if plot_test == True\n",
    "\n",
    "if dy > 1:\n",
    "    bare_plot_start, bare_plot_end = dx+dg+1, dx+dg+2\n",
    "    pe_plot_start, pe_plot_end = dpe+dg+1, dpe+dg+2\n",
    "else:\n",
    "    bare_plot_start, bare_plot_end = dx+dg, dx+dg+1\n",
    "    pe_plot_start, pe_plot_end = dpe+dg, dpe+dg+1\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss0, epoch_loss1 = 0, 0\n",
    "\n",
    "    traj_ids = torch.randperm(num_demos)[:batch_size * epoch_iter].chunk(epoch_iter)\n",
    "\n",
    "    for i in range(epoch_iter):\n",
    "        prepare_masked_batch(y_train, traj_ids[i])\n",
    "\n",
    "        opt0.zero_grad()\n",
    "        pred0 = m0(obs0, tar_x0, obs_mask)\n",
    "        loss0 = m0.loss(pred0, tar_y, tar_mask)\n",
    "        loss0.backward()\n",
    "        opt0.step()\n",
    "\n",
    "        epoch_loss0 += loss0.item()\n",
    "\n",
    "\n",
    "        opt1.zero_grad()\n",
    "        pred1 = m1(obs1, tar_x1, obs_mask)\n",
    "        loss1 = m1.loss(pred1, tar_y, tar_mask)\n",
    "        loss1.backward()\n",
    "        opt1.step()\n",
    "\n",
    "        epoch_loss1 += loss1.item()\n",
    "\n",
    "\n",
    "    if epoch % test_per_epoch == 0:# and epoch > 0:\n",
    "        test_traj_ids = torch.randperm(num_test)[:batch_size*test_epoch_iter].chunk(test_epoch_iter)\n",
    "        test_loss0, test_loss1 = 0, 0\n",
    "\n",
    "        for j in range(test_epoch_iter):\n",
    "            prepare_masked_test_batch(y_test, test_traj_ids[j])\n",
    "\n",
    "            pred0 = m0.val(test_obs0, test_tar_x0, test_obs_mask)\n",
    "            pred1 = m1.val(test_obs1, test_tar_x1, test_obs_mask)\n",
    "            \n",
    "            if plot_test:\n",
    "                for k in range(batch_size):\n",
    "                    current_n = test_obs_mask[k].sum().item()\n",
    "                    plt.scatter(last_obs_vals[k, :current_n, :dx].cpu().numpy(), test_obs0[k, :current_n, bare_plot_start:bare_plot_end].cpu().numpy(), label='Condition')\n",
    "                    plt.plot(test_tar_y[k, :, 0].cpu().numpy(), label=f\"Groundtruth\")\n",
    "                    plt.plot(pred0[k, :, 0].cpu().numpy(), label=f\"Prediction\")\n",
    "                    \n",
    "                    plt.legend(loc='upper left')\n",
    "                    plt.title(f'Epoch: {epoch}', fontsize=20)\n",
    "                    plt.savefig(f'{img_folder}{epoch}_{test_traj_ids[j][k]}_bare.png')\n",
    "                    plt.clf()\n",
    "\n",
    "                    plt.scatter(last_obs_vals[k, :current_n, :dx].cpu().numpy(), test_obs1[k, :current_n, pe_plot_start:pe_plot_end].cpu().numpy(), label='Condition')\n",
    "                    plt.plot(test_tar_y[k, :, 0].cpu().numpy(), label=f\"Groundtruth\")\n",
    "                    plt.plot(pred1[k, :, 0].cpu().numpy(), label=f\"Prediction\")\n",
    "                    \n",
    "                    plt.legend(loc='upper left')\n",
    "                    plt.title(f'Epoch: {epoch}', fontsize=20)\n",
    "                    plt.savefig(f'{img_folder}{epoch}_{test_traj_ids[j][k]}_ph.png')\n",
    "                    plt.clf()\n",
    "                    \n",
    "\n",
    "            test_loss0 += mse_loss(pred0[:, :, :m0.output_dim], test_tar_y).item()\n",
    "            test_loss1 += mse_loss(pred1[:, :, :m1.output_dim], test_tar_y).item()\n",
    "        \n",
    "        test_loss0 /= test_epoch_iter\n",
    "        test_loss1 /= test_epoch_iter\n",
    "            \n",
    "        if test_loss0 < min_test_loss0:\n",
    "            min_test_loss0 = test_loss0\n",
    "            print(f'New BARE best: {min_test_loss0}, PE best: {min_test_loss1}')\n",
    "            torch.save(m0_.state_dict(), f'{root_folder}saved_models/bare.pt')\n",
    "\n",
    "        if test_loss1 < min_test_loss1:\n",
    "            min_test_loss1 = test_loss1\n",
    "            print(f'New PE best: {min_test_loss1}, BARE best: {min_test_loss0}')\n",
    "            torch.save(m1_.state_dict(), f'{root_folder}saved_models/pe.pt')\n",
    "\n",
    "\n",
    "    epoch_loss0 /= epoch_iter\n",
    "    epoch_loss1 /= epoch_iter\n",
    "\n",
    "    avg_loss0 += epoch_loss0\n",
    "    avg_loss1 += epoch_loss1\n",
    "\n",
    "    l0.append(epoch_loss0)\n",
    "    l1.append(epoch_loss1)\n",
    "\n",
    "    if epoch % loss_report_interval == 0:\n",
    "        print(\"Epoch: {}, Losses: BARE: {}, PE: {}\".format(epoch, avg_loss0/loss_report_interval, avg_loss1/loss_report_interval))\n",
    "        avg_loss0, avg_loss1 = 0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last_obs_vals.shape\n",
    "test_obs0[k, current_n, dx:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(l0, f'{root_folder}losses_bare.pt')\n",
    "torch.save(l1, f'{root_folder}losses_pe.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
