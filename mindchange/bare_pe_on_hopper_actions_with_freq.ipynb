{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "folder_path = '../models/'\n",
    "if folder_path not in sys.path:\n",
    "    sys.path.append(folder_path)\n",
    "\n",
    "folder_path = '../data/'\n",
    "if folder_path not in sys.path:\n",
    "    sys.path.append(folder_path)\n",
    "\n",
    "from cnmp import CNMP\n",
    "from positional_encoders import *\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "def get_free_gpu():\n",
    "    gpu_util = []\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_device(i)  # Switch GPU\n",
    "#        gpu_util.append((i, torch.cuda.memory_stats()['reserved_bytes.all.current'] / (1024 ** 2)))\n",
    "        gpu_util.append((i, torch.cuda.utilization()))\n",
    "    gpu_util.sort(key=lambda x: x[1])\n",
    "    return gpu_util[0][0]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    available_gpu = get_free_gpu()\n",
    "    if available_gpu == 0:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{available_gpu}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(\"Device :\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([8, 600, 1]), y_train shape: torch.Size([8, 600, 3]), g_train shape: torch.Size([8])\n",
      "x_test shape: torch.Size([2, 600, 1]), y_test shape: torch.Size([2, 600, 3]), g_test shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "dx, dy, dg, dph, dpe = 1, 3, 1, 0, 27\n",
    "num_demos, num_test = 4, 1\n",
    "num_trajs = num_demos + num_test\n",
    "t_steps = 400\n",
    "n_max, m_max = 40, 40\n",
    "\n",
    "trajectories, freqs = torch.from_numpy(np.load('../sim/data/hopper_interpolated_actions.npy')), torch.from_numpy(np.load('../sim/data/hopper_freqs.npy'))\n",
    "max_freq = max(freqs)\n",
    "\n",
    "perm_ids = torch.randperm(num_trajs)\n",
    "train_ids, test_ids = perm_ids[:num_demos], perm_ids[num_demos:]\n",
    "\n",
    "all_x = torch.linspace(0, 1, t_steps).unsqueeze(-1).unsqueeze(0).repeat(num_trajs,1,1)\n",
    "\n",
    "x_train, x_test = all_x[train_ids], all_x[test_ids]\n",
    "y_train, y_test = trajectories[train_ids], trajectories[test_ids]\n",
    "g_train, g_test = freqs[train_ids]/max_freq, freqs[test_ids]/max_freq\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}, g_train shape: {g_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}, g_test shape: {g_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = generate_positional_encoding(t_steps, dpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bare:  135174\n",
      "PE:  148486\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "enc_dims = [128,128]\n",
    "dec_dims = [128,128]\n",
    "\n",
    "m0_ = CNMP(input_dim=dx+dg, output_dim=dy, n_max=n_max, m_max=m_max, encoder_hidden_dims=enc_dims, decoder_hidden_dims=dec_dims, batch_size=batch_size, device=device)\n",
    "opt0 = torch.optim.Adam(lr=3e-4, params=m0_.parameters())\n",
    "\n",
    "m1_ = CNMP(input_dim=dpe+dg, output_dim=dy, n_max=n_max, m_max=m_max, encoder_hidden_dims=enc_dims, decoder_hidden_dims=dec_dims, batch_size=batch_size, device=device)\n",
    "opt1 = torch.optim.Adam(lr=3e-4, params=m1_.parameters())\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in m0_.parameters())\n",
    "print('Bare: ', pytorch_total_params)\n",
    "pytorch_total_params = sum(p.numel() for p in m1_.parameters())\n",
    "print('PE: ', pytorch_total_params)\n",
    "\n",
    "if torch.__version__ >= \"2.0\":\n",
    "    m0, m1 = torch.compile(m0_), torch.compile(m1_)\n",
    "else:\n",
    "    m0, m1 = m0_, m1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs0 = torch.zeros((batch_size, n_max, dx+dg+dy), dtype=torch.float32, device=device)\n",
    "tar_x0 = torch.zeros((batch_size, m_max, dx+dg), dtype=torch.float32, device=device)\n",
    "\n",
    "obs1 = torch.zeros((batch_size, n_max, dpe+dg+dy), dtype=torch.float32, device=device)\n",
    "tar_x1 = torch.zeros((batch_size, m_max, dpe+dg), dtype=torch.float32, device=device)\n",
    "\n",
    "tar_y = torch.zeros((batch_size, m_max, dy), dtype=torch.float32, device=device)\n",
    "obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "tar_mask = torch.zeros((batch_size, m_max), dtype=torch.bool, device=device)\n",
    "\n",
    "def prepare_masked_batch(t: list, traj_ids: list):\n",
    "    global obs0, tar_x0, obs1, tar_x1, tar_y, obs_mask, tar_mask\n",
    "    obs0.fill_(0)\n",
    "    tar_x0.fill_(0)\n",
    "    obs1.fill_(0)\n",
    "    tar_x1.fill_(0)\n",
    "    tar_y.fill_(0)\n",
    "    obs_mask.fill_(False)\n",
    "    tar_mask.fill_(False)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = t[traj_id]\n",
    "\n",
    "        n = torch.randint(1, n_max+1, (1,)).item()\n",
    "        m = torch.randint(1, m_max+1, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = permuted_ids[n:n+m]\n",
    "\n",
    "        obs0[i, :n, :dx] = x_train[traj_id, n_ids]  # t\n",
    "        obs0[i, :n, dx:dx+dg] = g_train[traj_id]  # gamma\n",
    "        obs0[i, :n, dx+dg:] = traj[n_ids]  # SM(t)\n",
    "\n",
    "        obs1[i, :n, :dpe] = pe[n_ids]  # PE(t)\n",
    "        obs1[i, :n, dpe:dpe+dg] = g_train[traj_id]  # gamma\n",
    "        obs1[i, :n, dpe+dg:] = traj[n_ids]  # SM(t)\n",
    "\n",
    "        obs_mask[i, :n] = True\n",
    "        \n",
    "        tar_x0[i, :m, :dx] = x_train[traj_id, m_ids]\n",
    "        tar_x0[i, :m, dx:] = g_train[traj_id]\n",
    "        tar_x1[i, :m, :dpe] = pe[m_ids]\n",
    "        tar_x1[i, :m, dpe:] = g_train[traj_id]        \n",
    "        \n",
    "        tar_y[i, :m] = traj[m_ids]\n",
    "        tar_mask[i, :m] = True\n",
    "\n",
    "\n",
    "test_obs0 = torch.zeros((batch_size, n_max, dx+dg+dy), dtype=torch.float32, device=device)\n",
    "test_tar_x0 = torch.zeros((batch_size, t_steps, dx+dg), dtype=torch.float32, device=device)\n",
    "\n",
    "test_obs1 = torch.zeros((batch_size, n_max, dpe+dg+dy), dtype=torch.float32, device=device)\n",
    "test_tar_x1 = torch.zeros((batch_size, t_steps, dpe+dg), dtype=torch.float32, device=device)\n",
    "\n",
    "test_tar_y = torch.zeros((batch_size, t_steps, dy), dtype=torch.float32, device=device)\n",
    "test_obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "last_obs_vals = torch.zeros((batch_size, n_max, dx), dtype=torch.int32, device=device)  # only for plotting\n",
    "\n",
    "def prepare_masked_test_batch(t: list, traj_ids: list, fixed_ind=None):\n",
    "    global test_obs0, test_tar_x0, test_obs1, test_tar_x1, test_tar_y, test_obs_mask, last_obs_vals\n",
    "    test_obs0.fill_(0)\n",
    "    test_tar_x0.fill_(0)\n",
    "    test_obs1.fill_(0)\n",
    "    test_tar_x1.fill_(0)\n",
    "    test_tar_y.fill_(0)\n",
    "    test_obs_mask.fill_(False)\n",
    "    last_obs_vals.fill_(0)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = t[traj_id]\n",
    "\n",
    "        # n = num_peaks #torch.randint(5, n_max, (1,)).item()\n",
    "        n = torch.randint(1, n_max+1, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = torch.arange(t_steps)\n",
    "\n",
    "        if fixed_ind != None:\n",
    "            for p in range(n):\n",
    "                n_ids[p] = fixed_ind[i, p]\n",
    "            # n_ids[-1] = fixed_ind[i]\n",
    "\n",
    "        test_obs0[i, :n, :dx] = x_test[traj_id, n_ids]  # t\n",
    "        test_obs0[i, :n, dx:dx+dg] = g_test[traj_id]\n",
    "        test_obs0[i, :n, dx+dg:] = traj[n_ids]  # SM(t)\n",
    "\n",
    "        test_obs1[i, :n, :dpe] = pe[n_ids]  # PE(t)\n",
    "        test_obs1[i, :n, dpe:dpe+dg] = g_test[traj_id]\n",
    "        test_obs1[i, :n, dpe+dg:] = traj[n_ids]\n",
    "\n",
    "        last_obs_vals[i, :n] = n_ids.unsqueeze(-1)\n",
    "        test_obs_mask[i, :n] = True\n",
    "        \n",
    "        test_tar_x0[i, :, :dx] = x_test[traj_id, m_ids]\n",
    "        test_tar_x1[i, :, :dpe] = pe[m_ids]\n",
    "\n",
    "        test_tar_y[i] = traj[m_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New BARE best: 0.4437295198440552, PE best: 1000000\n",
      "New PE best: 0.438871830701828, BARE best: 0.4437295198440552\n",
      "Epoch: 0, Losses: BARE: 0.001959403157234192, PE: 0.0019306052327156067\n",
      "Epoch: 500, Losses: BARE: 0.8399373095035553, PE: 0.6970482487455011\n",
      "New BARE best: 0.4434361159801483, PE best: 0.438871830701828\n",
      "Epoch: 1000, Losses: BARE: 0.8408923134803772, PE: 0.42782461030362173\n",
      "Epoch: 1500, Losses: BARE: 0.8354429938793182, PE: 0.2432000889768824\n",
      "New BARE best: 0.4433012008666992, PE best: 0.438871830701828\n",
      "New PE best: 0.4354000687599182, BARE best: 0.4433012008666992\n",
      "Epoch: 2000, Losses: BARE: 0.8330122887194157, PE: 0.18494774474599399\n",
      "Epoch: 2500, Losses: BARE: 0.8270865197479725, PE: 0.0620878695352003\n",
      "New BARE best: 0.43708741664886475, PE best: 0.4354000687599182\n",
      "New PE best: 0.41785719990730286, BARE best: 0.43708741664886475\n",
      "Epoch: 3000, Losses: BARE: 0.8174449216723442, PE: 0.031171181712998076\n",
      "Epoch: 3500, Losses: BARE: 0.8118214184045791, PE: -0.031040510441642254\n",
      "New PE best: 0.4025043547153473, BARE best: 0.43708741664886475\n",
      "Epoch: 4000, Losses: BARE: 0.8013363698869944, PE: -0.075497398275882\n",
      "Epoch: 4500, Losses: BARE: 0.7914039018899203, PE: -0.11056209720578045\n",
      "New PE best: 0.3923560380935669, BARE best: 0.43708741664886475\n",
      "Epoch: 5000, Losses: BARE: 0.78538150203228, PE: -0.17933054856583477\n",
      "Epoch: 5500, Losses: BARE: 0.7743934441655874, PE: -0.19424224813235924\n",
      "New PE best: 0.38586291670799255, BARE best: 0.43708741664886475\n",
      "Epoch: 6000, Losses: BARE: 0.7765630296170711, PE: -0.23667320045386442\n",
      "Epoch: 6500, Losses: BARE: 0.7652230686694383, PE: -0.2556736218233127\n",
      "Epoch: 7000, Losses: BARE: 0.7674749826788902, PE: -0.19302117573004215\n",
      "Epoch: 7500, Losses: BARE: 0.7628070302605628, PE: -0.2605818110315595\n",
      "Epoch: 8000, Losses: BARE: 0.7575883136093616, PE: -0.3079160006660968\n",
      "Epoch: 8500, Losses: BARE: 0.7563746048808098, PE: -0.33965196875017134\n",
      "Epoch: 9000, Losses: BARE: 0.7492998645156622, PE: -0.33325108377542345\n",
      "Epoch: 9500, Losses: BARE: 0.7458389613553882, PE: -0.37383407187322154\n",
      "New PE best: 0.37622231245040894, BARE best: 0.43708741664886475\n",
      "Epoch: 10000, Losses: BARE: 0.7495394672900438, PE: -0.39073707966925575\n",
      "Epoch: 10500, Losses: BARE: 0.7403165993914008, PE: -0.4058725815431681\n",
      "Epoch: 11000, Losses: BARE: 0.733694852091372, PE: -0.3626368379900232\n",
      "Epoch: 11500, Losses: BARE: 0.726409831404686, PE: -0.40362211219826716\n",
      "Epoch: 12000, Losses: BARE: 0.7241468959972263, PE: -0.4527510705767199\n",
      "Epoch: 12500, Losses: BARE: 0.7215698126778006, PE: -0.42849027023930103\n",
      "Epoch: 13000, Losses: BARE: 0.712651873216033, PE: -0.47465952049335464\n",
      "Epoch: 13500, Losses: BARE: 0.7121633227840066, PE: -0.45375871860410555\n",
      "Epoch: 14000, Losses: BARE: 0.7111341098546982, PE: -0.49086802355991677\n",
      "Epoch: 14500, Losses: BARE: 0.7036837966814637, PE: -0.4834182854546234\n",
      "Epoch: 15000, Losses: BARE: 0.6989055466875433, PE: -0.48807272963854487\n",
      "Epoch: 15500, Losses: BARE: 0.6932226983979345, PE: -0.4971169069525786\n",
      "Epoch: 16000, Losses: BARE: 0.6918889621570706, PE: -0.5433220831872896\n",
      "Epoch: 16500, Losses: BARE: 0.683366997860372, PE: -0.5196964409635402\n",
      "New PE best: 0.3739824891090393, BARE best: 0.43708741664886475\n",
      "Epoch: 17000, Losses: BARE: 0.6768997366949916, PE: -0.570924910291098\n",
      "Epoch: 17500, Losses: BARE: 0.6774272803068161, PE: -0.5688872014470399\n",
      "Epoch: 18000, Losses: BARE: 0.6713332399874926, PE: -0.5269010573551058\n",
      "Epoch: 18500, Losses: BARE: 0.6676786571443081, PE: -0.5559477943684906\n",
      "Epoch: 19000, Losses: BARE: 0.6631074171066285, PE: -0.5691602529254742\n",
      "Epoch: 19500, Losses: BARE: 0.6596532189138233, PE: -0.5967175762718543\n",
      "Epoch: 20000, Losses: BARE: 0.6575100179202855, PE: -0.5753644848586992\n",
      "Epoch: 20500, Losses: BARE: 0.6560363727360964, PE: -0.6144677301887422\n",
      "Epoch: 21000, Losses: BARE: 0.6469382486231625, PE: -0.6138001634059473\n",
      "Epoch: 21500, Losses: BARE: 0.6451477799452842, PE: -0.626954904904589\n",
      "Epoch: 22000, Losses: BARE: 0.6396912591680884, PE: -0.5627128740348853\n",
      "Epoch: 22500, Losses: BARE: 0.6430827112905682, PE: -0.6475016734041273\n",
      "Epoch: 23000, Losses: BARE: 0.6323400571867823, PE: -0.659681305769831\n",
      "Epoch: 23500, Losses: BARE: 0.6335758678987622, PE: -0.6493921745726838\n",
      "Epoch: 24000, Losses: BARE: 0.6322639429438859, PE: -0.6543036627816037\n",
      "Epoch: 24500, Losses: BARE: 0.6205995086841285, PE: -0.6459146142676473\n",
      "Epoch: 25000, Losses: BARE: 0.6256577300988138, PE: -0.6890242004934698\n",
      "Epoch: 25500, Losses: BARE: 0.6212628304213286, PE: -0.6605941158356144\n",
      "Epoch: 26000, Losses: BARE: 0.617292146306485, PE: -0.6696551139624789\n",
      "Epoch: 26500, Losses: BARE: 0.6112061162851751, PE: -0.6659249523617328\n",
      "Epoch: 27000, Losses: BARE: 0.6092002735063433, PE: -0.7047760285958647\n",
      "Epoch: 27500, Losses: BARE: 0.6030250078924, PE: -0.7139242937378585\n",
      "Epoch: 28000, Losses: BARE: 0.5964584568981082, PE: -0.7193680317848921\n",
      "Epoch: 28500, Losses: BARE: 0.5964020539149641, PE: -0.7450491976789199\n",
      "Epoch: 29000, Losses: BARE: 0.5911528734564782, PE: -0.7326992635149508\n",
      "Epoch: 29500, Losses: BARE: 0.5896414995212108, PE: -0.7421134329242631\n",
      "Epoch: 30000, Losses: BARE: 0.5832953842543066, PE: -0.7525461140088737\n",
      "Epoch: 30500, Losses: BARE: 0.5769974419921636, PE: -0.7488120606122538\n",
      "Epoch: 31000, Losses: BARE: 0.5807156887091697, PE: -0.7112556034428998\n",
      "Epoch: 31500, Losses: BARE: 0.5745030494835228, PE: -0.761393114876002\n",
      "Epoch: 32000, Losses: BARE: 0.5760575848892331, PE: -0.7067984721334651\n",
      "Epoch: 32500, Losses: BARE: 0.58301693460159, PE: -0.7666027510426938\n",
      "Epoch: 33000, Losses: BARE: 0.5778017363958061, PE: -0.6982148388670757\n",
      "Epoch: 33500, Losses: BARE: 0.5756011935546994, PE: -0.7522321585956961\n",
      "Epoch: 34000, Losses: BARE: 0.5673971666097641, PE: -0.7573069229964167\n",
      "Epoch: 34500, Losses: BARE: 0.564203089825809, PE: -0.7822805411079898\n",
      "Epoch: 35000, Losses: BARE: 0.5638742675874382, PE: -0.795307356884703\n",
      "Epoch: 35500, Losses: BARE: 0.5617767718415707, PE: -0.7459549624817445\n",
      "Epoch: 36000, Losses: BARE: 0.5519682981520891, PE: -0.799927531096153\n",
      "Epoch: 36500, Losses: BARE: 0.5470948012284934, PE: -0.8256101352861152\n",
      "Epoch: 37000, Losses: BARE: 0.5461406198553741, PE: -0.7691116417257581\n",
      "Epoch: 37500, Losses: BARE: 0.5554951485227794, PE: -0.7684821411240846\n",
      "Epoch: 38000, Losses: BARE: 0.5527125025317073, PE: -0.8002361201366875\n",
      "Epoch: 38500, Losses: BARE: 0.5439805231168866, PE: -0.8502883886154741\n",
      "Epoch: 39000, Losses: BARE: 0.5406269494742155, PE: -0.8312457411698997\n",
      "Epoch: 39500, Losses: BARE: 0.5413971872190013, PE: -0.7704491223298247\n",
      "Epoch: 40000, Losses: BARE: 0.5359373666252941, PE: -0.793113904855214\n",
      "Epoch: 40500, Losses: BARE: 0.5453129353672266, PE: -0.8288012930899858\n",
      "Epoch: 41000, Losses: BARE: 0.5363010844290257, PE: -0.79732208029693\n",
      "Epoch: 41500, Losses: BARE: 0.5329642350003123, PE: -0.8441269267890602\n",
      "Epoch: 42000, Losses: BARE: 0.5242115063667298, PE: -0.8012213454875163\n",
      "Epoch: 42500, Losses: BARE: 0.5310572255831212, PE: -0.8462139950944111\n",
      "Epoch: 43000, Losses: BARE: 0.5406856029499322, PE: -0.7963540105000139\n",
      "Epoch: 43500, Losses: BARE: 0.5248989377878607, PE: -0.6823505509747192\n",
      "Epoch: 44000, Losses: BARE: 0.521952122328803, PE: -0.7900928210942074\n",
      "Epoch: 44500, Losses: BARE: 0.5142942817686126, PE: -0.8357848423812538\n",
      "Epoch: 45000, Losses: BARE: 0.5149438880141824, PE: -0.8326128781414591\n",
      "Epoch: 45500, Losses: BARE: 0.5202509113587439, PE: -0.8442322696996852\n",
      "Epoch: 46000, Losses: BARE: 0.5214226970952004, PE: -0.8361867669587955\n",
      "Epoch: 46500, Losses: BARE: 0.512274092130363, PE: -0.8351866198740899\n",
      "Epoch: 47000, Losses: BARE: 0.5175899121463299, PE: -0.8572382102352567\n",
      "Epoch: 47500, Losses: BARE: 0.5046519727357663, PE: -0.8271286481507123\n",
      "Epoch: 48000, Losses: BARE: 0.5089588199667633, PE: -0.8381561648743228\n",
      "Epoch: 48500, Losses: BARE: 0.4987901092786342, PE: -0.8316923819445073\n",
      "Epoch: 49000, Losses: BARE: 0.503864341121167, PE: -0.8347789169149473\n",
      "Epoch: 49500, Losses: BARE: 0.500832066833973, PE: -0.8669856191445142\n",
      "Epoch: 50000, Losses: BARE: 0.49644537797942756, PE: -0.851549099967815\n",
      "Epoch: 50500, Losses: BARE: 0.5006614943612366, PE: -0.8267522476376035\n",
      "Epoch: 51000, Losses: BARE: 0.5016080235801638, PE: -0.8301205349452794\n",
      "Epoch: 51500, Losses: BARE: 0.4912185871843249, PE: -0.8761663979608566\n",
      "Epoch: 52000, Losses: BARE: 0.49455494023486973, PE: -0.8312207227777689\n",
      "Epoch: 52500, Losses: BARE: 0.49519658304192127, PE: -0.852236394353211\n",
      "Epoch: 53000, Losses: BARE: 0.49165587466582655, PE: -0.839397446623072\n",
      "Epoch: 53500, Losses: BARE: 0.49216850331798195, PE: -0.8233437329250155\n",
      "Epoch: 54000, Losses: BARE: 0.4807258799318224, PE: -0.8425760864987969\n",
      "Epoch: 54500, Losses: BARE: 0.47790216870419683, PE: -0.8923796608299017\n",
      "Epoch: 55000, Losses: BARE: 0.47888212705776095, PE: -0.8382271977276541\n",
      "Epoch: 55500, Losses: BARE: 0.48017353406362234, PE: -0.7299319028295577\n",
      "Epoch: 56000, Losses: BARE: 0.4810630247760564, PE: -0.8647624483611435\n",
      "Epoch: 56500, Losses: BARE: 0.46885844561085105, PE: -0.8974609092213214\n",
      "Epoch: 57000, Losses: BARE: 0.48748748648911716, PE: -0.701857758234255\n",
      "Epoch: 57500, Losses: BARE: 0.4746010860260576, PE: -0.8594929577368312\n",
      "Epoch: 58000, Losses: BARE: 0.47448940959386526, PE: -0.8471835109321401\n",
      "Epoch: 58500, Losses: BARE: 0.467644298132509, PE: -0.8931588197695092\n",
      "Epoch: 59000, Losses: BARE: 0.46025141306500883, PE: -0.9133084545526654\n",
      "Epoch: 59500, Losses: BARE: 0.4741303751319647, PE: -0.8831517694229261\n",
      "Epoch: 60000, Losses: BARE: 0.46565991335734724, PE: -0.9284179948829114\n",
      "Epoch: 60500, Losses: BARE: 0.45801290848851206, PE: -0.7353336644698866\n",
      "Epoch: 61000, Losses: BARE: 0.45886475971434265, PE: -0.8289670919852214\n",
      "Epoch: 61500, Losses: BARE: 0.4688056680541485, PE: -0.8817481325007975\n",
      "Epoch: 62000, Losses: BARE: 0.4646509725563228, PE: -0.8977619942929596\n",
      "Epoch: 62500, Losses: BARE: 0.46212073776870966, PE: -0.9063323191646486\n",
      "Epoch: 63000, Losses: BARE: 0.45929890184849503, PE: -0.8639921400099992\n",
      "Epoch: 63500, Losses: BARE: 0.45293355964496734, PE: -0.8299981481106952\n",
      "Epoch: 64000, Losses: BARE: 0.44879544293507934, PE: -0.8361606895094738\n",
      "Epoch: 64500, Losses: BARE: 0.4742739185150713, PE: -0.8176723634155932\n",
      "Epoch: 65000, Losses: BARE: 0.4651766909100115, PE: -0.8966130891945213\n",
      "Epoch: 65500, Losses: BARE: 0.47078337224572897, PE: -0.8762854132745415\n",
      "Epoch: 66000, Losses: BARE: 0.4557290826570243, PE: -0.8281261257603765\n",
      "Epoch: 66500, Losses: BARE: 0.4659816580079496, PE: -0.9026650861580856\n",
      "Epoch: 67000, Losses: BARE: 0.4572660747561604, PE: -0.9053990492913871\n",
      "Epoch: 67500, Losses: BARE: 0.4374056157022715, PE: -0.9292193653043359\n",
      "Epoch: 68000, Losses: BARE: 0.46634189641941337, PE: -0.929058961449191\n",
      "Epoch: 68500, Losses: BARE: 0.4497436477169395, PE: -0.8100083393813111\n",
      "Epoch: 69000, Losses: BARE: 0.4538423045706004, PE: -0.8723145107254386\n",
      "Epoch: 69500, Losses: BARE: 0.45279314670339227, PE: -0.9351938613597304\n",
      "Epoch: 70000, Losses: BARE: 0.4485020332671702, PE: -0.9499868740588426\n",
      "Epoch: 70500, Losses: BARE: 0.44379017177596686, PE: -0.8221401946516708\n",
      "Epoch: 71000, Losses: BARE: 0.4500592149682343, PE: -0.9258479784522206\n",
      "Epoch: 71500, Losses: BARE: 0.42786240549199284, PE: -0.9363150600641966\n",
      "Epoch: 72000, Losses: BARE: 0.4351389409676194, PE: -0.9258565694447607\n",
      "Epoch: 72500, Losses: BARE: 0.44191085604857655, PE: -0.8744813667684793\n",
      "Epoch: 73000, Losses: BARE: 0.43817976522259416, PE: -0.9119728366043419\n",
      "Epoch: 73500, Losses: BARE: 0.4480592992324382, PE: -0.9445109298201277\n",
      "Epoch: 74000, Losses: BARE: 0.45535078044980765, PE: -0.9113521785363555\n",
      "Epoch: 74500, Losses: BARE: 0.431856787500903, PE: -0.9417831711191684\n",
      "Epoch: 75000, Losses: BARE: 0.4698856309838593, PE: -0.9370658716252074\n",
      "Epoch: 75500, Losses: BARE: 0.4384015555232763, PE: -0.9536549077862874\n",
      "Epoch: 76000, Losses: BARE: 0.43827459508925676, PE: -0.9297801930597052\n",
      "Epoch: 76500, Losses: BARE: 0.4308294732011855, PE: -0.9479951637750491\n",
      "Epoch: 77000, Losses: BARE: 0.42554230154957623, PE: -0.9759591541849076\n",
      "Epoch: 77500, Losses: BARE: 0.43282813915237783, PE: -0.997903436016757\n",
      "Epoch: 78000, Losses: BARE: 0.43351927018724384, PE: -0.9388150526518002\n",
      "Epoch: 78500, Losses: BARE: 0.43522949927672744, PE: -0.9380045479629189\n",
      "Epoch: 79000, Losses: BARE: 0.4194797130310908, PE: -0.9180027159787715\n",
      "Epoch: 79500, Losses: BARE: 0.4110873651523143, PE: -0.9369958022125066\n",
      "Epoch: 80000, Losses: BARE: 0.4205294924490154, PE: -0.938431863905862\n",
      "Epoch: 80500, Losses: BARE: 0.4257572508007288, PE: -0.9786313328612596\n",
      "Epoch: 81000, Losses: BARE: 0.41650584495812654, PE: -0.9494590812621172\n",
      "Epoch: 81500, Losses: BARE: 0.4159625857695937, PE: -0.9576887979268213\n",
      "Epoch: 82000, Losses: BARE: 0.41696454837359487, PE: -0.9524815343758092\n",
      "Epoch: 82500, Losses: BARE: 0.4232374865654856, PE: -0.8422723156698048\n",
      "Epoch: 83000, Losses: BARE: 0.42543446700461207, PE: -0.7773850599909201\n",
      "Epoch: 83500, Losses: BARE: 0.4176223055254668, PE: -0.9868916617287323\n",
      "Epoch: 84000, Losses: BARE: 0.4224606382958591, PE: -0.9302576913340017\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m     loss1 \u001b[38;5;241m=\u001b[39m m1\u001b[38;5;241m.\u001b[39mloss(pred1, tar_y, tar_mask)\n\u001b[1;32m     54\u001b[0m     loss1\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 55\u001b[0m     \u001b[43mopt1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     epoch_loss1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss1\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m test_per_epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\u001b[38;5;66;03m# and epoch > 0:\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py:216\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    213\u001b[0m     state_steps: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     adam(\n\u001b[1;32m    227\u001b[0m         params_with_grad,\n\u001b[1;32m    228\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    247\u001b[0m     )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py:172\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    169\u001b[0m exp_avgs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    170\u001b[0m exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m    173\u001b[0m     max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mrequires_grad:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "timestamp = int(time.time())\n",
    "root_folder = f'../outputs/sim/hopper/bare_pe/{str(timestamp)}/'\n",
    "\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "\n",
    "if not os.path.exists(f'{root_folder}saved_models/'):\n",
    "    os.makedirs(f'{root_folder}saved_models/')\n",
    "\n",
    "img_folder = f'{root_folder}img/'\n",
    "if not os.path.exists(img_folder):\n",
    "    os.makedirs(img_folder)\n",
    "\n",
    "torch.save(y_train, f'{root_folder}y.pt')\n",
    "\n",
    "\n",
    "epochs = 1_000_000\n",
    "epoch_iter = num_demos // batch_size\n",
    "test_epoch_iter = num_test//batch_size\n",
    "avg_loss0, avg_loss1 = 0, 0\n",
    "loss_report_interval = 500\n",
    "test_per_epoch = 1000\n",
    "min_test_loss0, min_test_loss1 = 1000000, 1000000\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "plot_test = True\n",
    "\n",
    "l0, l1 = [], []\n",
    "\n",
    "\n",
    "# if plot_test == True\n",
    "\n",
    "if dy > 1:\n",
    "    bare_plot_start, bare_plot_end = dx+dg+1, dx+dg+2\n",
    "    pe_plot_start, pe_plot_end = dpe+dg+1, dpe+dg+2\n",
    "else:\n",
    "    bare_plot_start, bare_plot_end = dx+dg, dx+dg+1\n",
    "    pe_plot_start, pe_plot_end = dpe+dg, dpe+dg+1\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss0, epoch_loss1 = 0, 0\n",
    "\n",
    "    traj_ids = torch.randperm(num_demos)[:batch_size * epoch_iter].chunk(epoch_iter)\n",
    "\n",
    "    for i in range(epoch_iter):\n",
    "        prepare_masked_batch(y_train, traj_ids[i])\n",
    "\n",
    "        opt0.zero_grad()\n",
    "        pred0 = m0(obs0, tar_x0, obs_mask)\n",
    "        loss0 = m0.loss(pred0, tar_y, tar_mask)\n",
    "        loss0.backward()\n",
    "        opt0.step()\n",
    "\n",
    "        epoch_loss0 += loss0.item()\n",
    "\n",
    "\n",
    "        opt1.zero_grad()\n",
    "        pred1 = m1(obs1, tar_x1, obs_mask)\n",
    "        loss1 = m1.loss(pred1, tar_y, tar_mask)\n",
    "        loss1.backward()\n",
    "        opt1.step()\n",
    "\n",
    "        epoch_loss1 += loss1.item()\n",
    "\n",
    "\n",
    "    if epoch % test_per_epoch == 0:# and epoch > 0:\n",
    "        test_traj_ids = torch.randperm(num_test)[:batch_size*test_epoch_iter].chunk(test_epoch_iter)\n",
    "        test_loss0, test_loss1 = 0, 0\n",
    "\n",
    "        for j in range(test_epoch_iter):\n",
    "            prepare_masked_test_batch(y_test, test_traj_ids[j])\n",
    "\n",
    "            pred0 = m0.val(test_obs0, test_tar_x0, test_obs_mask)\n",
    "            pred1 = m1.val(test_obs1, test_tar_x1, test_obs_mask)\n",
    "            \n",
    "            if plot_test:\n",
    "                for k in range(batch_size):\n",
    "                    current_n = test_obs_mask[k].sum().item()\n",
    "                    plt.scatter(last_obs_vals[k, :current_n, :dx].cpu().numpy(), test_obs0[k, :current_n, bare_plot_start:bare_plot_end].cpu().numpy(), label='Condition')\n",
    "                    plt.plot(test_tar_y[k, :, 0].cpu().numpy(), label=f\"Groundtruth\")\n",
    "                    plt.plot(pred0[k, :, 0].cpu().numpy(), label=f\"Prediction\")\n",
    "                    \n",
    "                    plt.legend(loc='upper left')\n",
    "                    plt.title(f'Epoch: {epoch}', fontsize=20)\n",
    "                    plt.savefig(f'{img_folder}{epoch}_{test_traj_ids[j][k]}_bare.png')\n",
    "                    plt.clf()\n",
    "\n",
    "                    plt.scatter(last_obs_vals[k, :current_n, :dx].cpu().numpy(), test_obs1[k, :current_n, pe_plot_start:pe_plot_end].cpu().numpy(), label='Condition')\n",
    "                    plt.plot(test_tar_y[k, :, 0].cpu().numpy(), label=f\"Groundtruth\")\n",
    "                    plt.plot(pred1[k, :, 0].cpu().numpy(), label=f\"Prediction\")\n",
    "                    \n",
    "                    plt.legend(loc='upper left')\n",
    "                    plt.title(f'Epoch: {epoch}', fontsize=20)\n",
    "                    plt.savefig(f'{img_folder}{epoch}_{test_traj_ids[j][k]}_ph.png')\n",
    "                    plt.clf()\n",
    "                    \n",
    "\n",
    "            test_loss0 += mse_loss(pred0[:, :, :m0.output_dim], test_tar_y).item()\n",
    "            test_loss1 += mse_loss(pred1[:, :, :m1.output_dim], test_tar_y).item()\n",
    "        \n",
    "        test_loss0 /= test_epoch_iter\n",
    "        test_loss1 /= test_epoch_iter\n",
    "            \n",
    "        if test_loss0 < min_test_loss0:\n",
    "            min_test_loss0 = test_loss0\n",
    "            print(f'New BARE best: {min_test_loss0}, PE best: {min_test_loss1}')\n",
    "            torch.save(m0_.state_dict(), f'{root_folder}saved_models/bare.pt')\n",
    "\n",
    "        if test_loss1 < min_test_loss1:\n",
    "            min_test_loss1 = test_loss1\n",
    "            print(f'New PE best: {min_test_loss1}, BARE best: {min_test_loss0}')\n",
    "            torch.save(m1_.state_dict(), f'{root_folder}saved_models/pe.pt')\n",
    "\n",
    "\n",
    "    epoch_loss0 /= epoch_iter\n",
    "    epoch_loss1 /= epoch_iter\n",
    "\n",
    "    avg_loss0 += epoch_loss0\n",
    "    avg_loss1 += epoch_loss1\n",
    "\n",
    "    l0.append(epoch_loss0)\n",
    "    l1.append(epoch_loss1)\n",
    "\n",
    "    if epoch % loss_report_interval == 0:\n",
    "        print(\"Epoch: {}, Losses: BARE: {}, PE: {}\".format(epoch, avg_loss0/loss_report_interval, avg_loss1/loss_report_interval))\n",
    "        avg_loss0, avg_loss1 = 0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last_obs_vals.shape\n",
    "test_obs0[k, current_n, dx:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(l0, f'{root_folder}losses_bare.pt')\n",
    "torch.save(l1, f'{root_folder}losses_pe.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
