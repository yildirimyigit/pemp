{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "folder_path = '../models/'\n",
    "if folder_path not in sys.path:\n",
    "    sys.path.append(folder_path)\n",
    "\n",
    "folder_path = '../data/'\n",
    "if folder_path not in sys.path:\n",
    "    sys.path.append(folder_path)\n",
    "\n",
    "from cnmp import CNMP\n",
    "\n",
    "from data_generators import *\n",
    "from positional_encoders import *\n",
    "from plotters import *\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "def get_free_gpu():\n",
    "    gpu_util = []\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_device(i)  # Switch GPU\n",
    "#        gpu_util.append((i, torch.cuda.memory_stats()['reserved_bytes.all.current'] / (1024 ** 2)))\n",
    "        gpu_util.append((i, torch.cuda.utilization()))\n",
    "    gpu_util.sort(key=lambda x: x[1])\n",
    "    return gpu_util[0][0]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    available_gpu = get_free_gpu()\n",
    "    if available_gpu == 0:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{available_gpu}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(\"Device :\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([180, 1200, 1]), y_train shape: torch.Size([180, 1200, 1])\n",
      "x_test shape: torch.Size([20, 1200, 1]), y_test shape: torch.Size([20, 1200, 1])\n"
     ]
    }
   ],
   "source": [
    "dx, dy, dg, dph, dpe = 1, 1, 0, 1, 27\n",
    "num_demos, num_test = 180, 20\n",
    "num_trajs = num_demos + num_test\n",
    "t_steps = 1200\n",
    "n_max, m_max = 100, 100\n",
    "\n",
    "trajectories, phases = generate_cyclic_trajectories_with_random_cycles(num_trajs=num_trajs)\n",
    "\n",
    "perm_ids = torch.randperm(num_trajs)\n",
    "train_ids, test_ids = perm_ids[:num_demos], perm_ids[num_demos:]\n",
    "\n",
    "all_x = torch.linspace(0, 1, t_steps).unsqueeze(-1).unsqueeze(0).repeat(num_trajs,1,1)\n",
    "\n",
    "x_train, x_test = all_x[train_ids], all_x[test_ids]\n",
    "y_train, y_test = trajectories[train_ids], trajectories[test_ids]\n",
    "p_train, p_test = phases[train_ids], phases[test_ids]\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "phe_train = torch.zeros((num_demos, t_steps, dpe))\n",
    "for i in range(num_demos):\n",
    "    phe_train[i] = generate_positional_encoding_for_phase(p_train[i], dpe)\n",
    "\n",
    "phe_test = torch.zeros((num_test, t_steps, dpe))\n",
    "for i in range(num_test):\n",
    "    phe_test[i] = generate_positional_encoding_for_phase(p_test[i], dpe)\n",
    "\n",
    "pe = generate_positional_encoding(t_steps, dpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bare:  33794\n",
      "PH:  33794\n",
      "PE:  40450\n",
      "PHE:  40450\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "\n",
    "enc_dims = [128,128]\n",
    "dec_dims = [128,128]\n",
    "\n",
    "m0_ = CNMP(input_dim=dx, output_dim=dy, n_max=n_max, m_max=m_max, encoder_hidden_dims=enc_dims, decoder_hidden_dims=dec_dims, batch_size=batch_size, device=device)\n",
    "opt0 = torch.optim.Adam(lr=3e-4, params=m0_.parameters())\n",
    "\n",
    "m1_ = CNMP(input_dim=dx, output_dim=dy, n_max=n_max, m_max=m_max, encoder_hidden_dims=enc_dims, decoder_hidden_dims=dec_dims, batch_size=batch_size, device=device)\n",
    "opt1 = torch.optim.Adam(lr=3e-4, params=m1_.parameters())\n",
    "\n",
    "m2_ = CNMP(input_dim=dpe, output_dim=dy, n_max=n_max, m_max=m_max, encoder_hidden_dims=enc_dims, decoder_hidden_dims=dec_dims, batch_size=batch_size, device=device)\n",
    "opt2 = torch.optim.Adam(lr=3e-4, params=m2_.parameters())\n",
    "\n",
    "m3_ = CNMP(input_dim=dpe, output_dim=dy, n_max=n_max, m_max=m_max, encoder_hidden_dims=enc_dims, decoder_hidden_dims=dec_dims, batch_size=batch_size, device=device)\n",
    "opt3 = torch.optim.Adam(lr=3e-4, params=m3_.parameters())\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in m0_.parameters())\n",
    "print('Bare: ', pytorch_total_params)\n",
    "pytorch_total_params = sum(p.numel() for p in m1_.parameters())\n",
    "print('PH: ', pytorch_total_params)\n",
    "pytorch_total_params = sum(p.numel() for p in m2_.parameters())\n",
    "print('PE: ', pytorch_total_params)\n",
    "pytorch_total_params = sum(p.numel() for p in m3_.parameters())\n",
    "print('PHE: ', pytorch_total_params)\n",
    "\n",
    "if torch.__version__ >= \"2.0\":\n",
    "    m0, m1, m2, m3 = torch.compile(m0_), torch.compile(m1_), torch.compile(m2_), torch.compile(m3_)\n",
    "else:\n",
    "    m0, m1, m2, m3 = m0_, m1_, m2_, m3_\n",
    "# m0, m1, m2 = m0_, m1_, m2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs0 = torch.zeros((batch_size, n_max, dx+dg+dy), dtype=torch.float32, device=device)\n",
    "tar_x0 = torch.zeros((batch_size, m_max, dx+dg), dtype=torch.float32, device=device)\n",
    "\n",
    "obs1 = torch.zeros((batch_size, n_max, dph+dy), dtype=torch.float32, device=device)\n",
    "tar_x1 = torch.zeros((batch_size, m_max, dph), dtype=torch.float32, device=device)\n",
    "\n",
    "obs2 = torch.zeros((batch_size, n_max, dpe+dy), dtype=torch.float32, device=device)\n",
    "tar_x2 = torch.zeros((batch_size, m_max, dpe), dtype=torch.float32, device=device)\n",
    "\n",
    "obs3 = torch.zeros((batch_size, n_max, dpe+dy), dtype=torch.float32, device=device)\n",
    "tar_x3 = torch.zeros((batch_size, m_max, dpe), dtype=torch.float32, device=device)\n",
    "\n",
    "tar_y = torch.zeros((batch_size, m_max, dy), dtype=torch.float32, device=device)\n",
    "obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "tar_mask = torch.zeros((batch_size, m_max), dtype=torch.bool, device=device)\n",
    "\n",
    "def prepare_masked_batch(t: list, traj_ids: list):\n",
    "    global obs0, tar_x0, obs1, tar_x1, obs2, tar_x2, obs3, tar_x3, tar_y, obs_mask, tar_mask\n",
    "    obs0.fill_(0)\n",
    "    tar_x0.fill_(0)\n",
    "    obs1.fill_(0)\n",
    "    tar_x1.fill_(0)\n",
    "    obs2.fill_(0)\n",
    "    tar_x2.fill_(0)\n",
    "    obs3.fill_(0)\n",
    "    tar_x3.fill_(0)\n",
    "    tar_y.fill_(0)\n",
    "    obs_mask.fill_(False)\n",
    "    tar_mask.fill_(False)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = t[traj_id]\n",
    "\n",
    "        n = torch.randint(1, n_max+1, (1,)).item()\n",
    "        m = torch.randint(1, m_max+1, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = permuted_ids[n:n+m]\n",
    "\n",
    "        obs0[i, :n, :dx] = x_train[traj_id, n_ids]  # t\n",
    "        obs1[i, :n, :dph] = p_train[traj_id, n_ids]  # phase(t)\n",
    "        obs2[i, :n, :dpe] = pe[n_ids]  # PE(t)\n",
    "        obs3[i, :n, :dpe] = phe_train[traj_id, n_ids]  # PE(phase(t))\n",
    "\n",
    "        obs0[i, :n, dx:] = traj[n_ids]  # SM(t)\n",
    "        obs1[i, :n, dph:] = traj[n_ids]  # SM(t)\n",
    "        obs2[i, :n, dpe:] = traj[n_ids]  # SM(t)\n",
    "        obs3[i, :n, dpe:] = traj[n_ids]  # SM(t)\n",
    "\n",
    "        obs_mask[i, :n] = True\n",
    "        \n",
    "        tar_x0[i, :m, :dx] = x_train[traj_id, m_ids]\n",
    "        tar_x1[i, :m, :dph] = p_train[traj_id, m_ids]\n",
    "        tar_x2[i, :m, :dpe] = pe[m_ids]\n",
    "        tar_x3[i, :m, :dpe] = phe_train[traj_id, m_ids]\n",
    "\n",
    "        tar_y[i, :m] = traj[m_ids]\n",
    "        tar_mask[i, :m] = True\n",
    "\n",
    "\n",
    "test_obs0 = torch.zeros((batch_size, n_max, dx+dg+dy), dtype=torch.float32, device=device)\n",
    "test_tar_x0 = torch.zeros((batch_size, t_steps, dx+dg), dtype=torch.float32, device=device)\n",
    "\n",
    "test_obs1 = torch.zeros((batch_size, n_max, dph+dy), dtype=torch.float32, device=device)\n",
    "test_tar_x1 = torch.zeros((batch_size, t_steps, dph), dtype=torch.float32, device=device)\n",
    "\n",
    "test_obs2 = torch.zeros((batch_size, n_max, dpe+dy), dtype=torch.float32, device=device)\n",
    "test_tar_x2 = torch.zeros((batch_size, t_steps, dpe), dtype=torch.float32, device=device)\n",
    "\n",
    "test_obs3 = torch.zeros((batch_size, n_max, dpe+dy), dtype=torch.float32, device=device)\n",
    "test_tar_x3 = torch.zeros((batch_size, t_steps, dpe), dtype=torch.float32, device=device)\n",
    "\n",
    "test_tar_y = torch.zeros((batch_size, t_steps, dy), dtype=torch.float32, device=device)\n",
    "test_obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "last_obs_vals = torch.zeros((batch_size, n_max, dx), dtype=torch.int32, device=device)  # only for plotting\n",
    "\n",
    "def prepare_masked_test_batch(t: list, traj_ids: list, fixed_ind=None):\n",
    "    global test_obs0, test_tar_x0, test_obs1, test_tar_x1, test_obs2, test_tar_x2, test_obs3, test_tar_x3, test_tar_y, test_obs_mask, last_obs_vals\n",
    "    test_obs0.fill_(0)\n",
    "    test_tar_x0.fill_(0)\n",
    "    test_obs1.fill_(0)\n",
    "    test_tar_x1.fill_(0)\n",
    "    test_obs2.fill_(0)\n",
    "    test_tar_x2.fill_(0)\n",
    "    test_obs3.fill_(0)\n",
    "    test_tar_x3.fill_(0)\n",
    "    test_tar_y.fill_(0)\n",
    "    test_obs_mask.fill_(False)\n",
    "    last_obs_vals.fill_(0)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = t[traj_id]\n",
    "\n",
    "        # n = num_peaks #torch.randint(5, n_max, (1,)).item()\n",
    "        n = torch.randint(1, n_max+1, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = torch.arange(t_steps)\n",
    "\n",
    "        if fixed_ind != None:\n",
    "            for p in range(n):\n",
    "                n_ids[p] = fixed_ind[i, p]\n",
    "            # n_ids[-1] = fixed_ind[i]\n",
    "\n",
    "        test_obs0[i, :n, :dx] = x_test[traj_id, n_ids]  # t\n",
    "        test_obs1[i, :n, :dph] = p_test[traj_id, n_ids]  # phase(t)\n",
    "        test_obs2[i, :n, :dpe] = pe[n_ids]  # PE(t)\n",
    "        test_obs3[i, :n, :dpe] = phe_test[traj_id, n_ids]  # PE(phase(t))\n",
    "\n",
    "        test_obs0[i, :n, dx:] = traj[n_ids]  # SM(t)\n",
    "        test_obs1[i, :n, dph:] = traj[n_ids]  # SM(t)\n",
    "        test_obs2[i, :n, dpe:] = traj[n_ids]  # SM(t)\n",
    "        test_obs3[i, :n, dpe:] = traj[n_ids]  # SM(t)\n",
    "\n",
    "        last_obs_vals[i, :n] = n_ids.unsqueeze(-1)\n",
    "        test_obs_mask[i, :n] = True\n",
    "        \n",
    "        test_tar_x0[i, :, :dx] = x_test[traj_id, m_ids]\n",
    "        test_tar_x1[i, :, :dph] = p_test[traj_id, m_ids]\n",
    "        test_tar_x2[i, :, :dpe] = pe[m_ids]\n",
    "        test_tar_x3[i, :, :dpe] = phe_test[traj_id, m_ids]\n",
    "\n",
    "        test_tar_y[i] = traj[m_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New BARE best: 0.4815363585948944, PH best: 1000000, PE best: 1000000, PHE best: 1000000\n",
      "New PH best: 0.48102301359176636, PE best: 1000000, PHE best: 1000000, BARE best: 0.4815363585948944\n",
      "New PE best: 0.4839184582233429, PHE best: 1000000, BARE best: 0.4815363585948944, PH best: 0.48102301359176636\n",
      "New PHE best: 0.4808761477470398, BARE best: 0.4815363585948944, PH best: 0.48102301359176636, PE best: 0.4839184582233429\n",
      "Epoch: 0, Losses: BARE: 0.002115851481755575, PH: 0.002114037116368612, PE: 0.0021317189004686145, PHE: 0.002115969181060791\n",
      "Epoch: 500, Losses: BARE: 1.0198801622788116, PH: 0.4024481524215725, PE: 0.6983761041561765, PHE: 0.9977713931269118\n",
      "New BARE best: 0.43105190992355347, PH best: 0.48102301359176636, PE best: 0.4839184582233429, PHE best: 0.4808761477470398\n",
      "New PH best: 0.02761613391339779, PE best: 0.4839184582233429, PHE best: 0.4808761477470398, BARE best: 0.43105190992355347\n",
      "New PE best: 0.20064330101013184, PHE best: 0.4808761477470398, BARE best: 0.43105190992355347, PH best: 0.02761613391339779\n",
      "New PHE best: 0.3324883282184601, BARE best: 0.43105190992355347, PH best: 0.02761613391339779, PE best: 0.20064330101013184\n",
      "Epoch: 1000, Losses: BARE: 0.9611333276828141, PH: -0.5987058926116187, PE: 0.41602256941648624, PHE: 0.6547381176385609\n",
      "Epoch: 1500, Losses: BARE: 0.8996153414116956, PH: -0.8740276984190566, PE: 0.29238684160858663, PHE: 0.26074855591395585\n",
      "New PH best: 0.013850810006260872, PE best: 0.20064330101013184, PHE best: 0.3324883282184601, BARE best: 0.43105190992355347\n",
      "New PE best: 0.13200250267982483, PHE best: 0.3324883282184601, BARE best: 0.43105190992355347, PH best: 0.013850810006260872\n",
      "New PHE best: 0.09349045902490616, BARE best: 0.43105190992355347, PH best: 0.013850810006260872, PE best: 0.13200250267982483\n",
      "Epoch: 2000, Losses: BARE: 0.870706223342154, PH: -1.006076227648391, PE: 0.16639914876250103, PHE: -0.1351310052985975\n",
      "Epoch: 2500, Losses: BARE: 0.847982011112902, PH: -1.0817119767652668, PE: 0.036274353610624956, PHE: -0.43156509797286746\n",
      "New BARE best: 0.4250257909297943, PH best: 0.013850810006260872, PE best: 0.13200250267982483, PHE best: 0.09349045902490616\n",
      "New PH best: 0.00991163868457079, PE best: 0.13200250267982483, PHE best: 0.09349045902490616, BARE best: 0.4250257909297943\n",
      "New PE best: 0.07973527163267136, PHE best: 0.09349045902490616, BARE best: 0.4250257909297943, PH best: 0.00991163868457079\n",
      "New PHE best: 0.034047681838274, BARE best: 0.4250257909297943, PH best: 0.00991163868457079, PE best: 0.07973527163267136\n",
      "Epoch: 3000, Losses: BARE: 0.8801672028634281, PH: -1.1393018492847269, PE: -0.0901186753786759, PHE: -0.6347228362859075\n",
      "Epoch: 3500, Losses: BARE: 0.8225131663812542, PH: -1.1906053313275602, PE: -0.1850011238207427, PHE: -0.7397296351882269\n",
      "New BARE best: 0.42450010776519775, PH best: 0.00991163868457079, PE best: 0.07973527163267136, PHE best: 0.034047681838274\n",
      "New PH best: 0.00801802147179842, PE best: 0.07973527163267136, PHE best: 0.034047681838274, BARE best: 0.42450010776519775\n",
      "New PE best: 0.06149129197001457, PHE best: 0.034047681838274, BARE best: 0.42450010776519775, PH best: 0.00801802147179842\n",
      "New PHE best: 0.016039274632930756, BARE best: 0.42450010776519775, PH best: 0.00801802147179842, PE best: 0.06149129197001457\n",
      "Epoch: 4000, Losses: BARE: 0.7915125820371842, PH: -1.2271591945025662, PE: -0.25574281609312544, PHE: -0.8050850661470325\n",
      "Epoch: 4500, Losses: BARE: 0.768643070658048, PH: -1.259038245187865, PE: -0.30758996674767425, PHE: -0.8482819142102582\n",
      "New BARE best: 0.3810742199420929, PH best: 0.00801802147179842, PE best: 0.06149129197001457, PHE best: 0.016039274632930756\n",
      "New PH best: 0.00398665526881814, PE best: 0.06149129197001457, PHE best: 0.016039274632930756, BARE best: 0.3810742199420929\n",
      "New PE best: 0.04178592562675476, PHE best: 0.016039274632930756, BARE best: 0.3810742199420929, PH best: 0.00398665526881814\n",
      "New PHE best: 0.013710252940654755, BARE best: 0.3810742199420929, PH best: 0.00398665526881814, PE best: 0.04178592562675476\n",
      "Epoch: 5000, Losses: BARE: 0.7465048526293704, PH: -1.2980377791126565, PE: -0.35896503968346916, PHE: -0.8891093345272042\n",
      "Epoch: 5500, Losses: BARE: 0.7395861510170834, PH: -1.333328624765079, PE: -0.40210384367910496, PHE: -0.920511863059882\n",
      "New BARE best: 0.3596532344818115, PH best: 0.00398665526881814, PE best: 0.04178592562675476, PHE best: 0.013710252940654755\n",
      "Epoch: 6000, Losses: BARE: 0.7179251079459983, PH: -1.3781389325385283, PE: -0.43677154409155866, PHE: -0.9529166725943283\n",
      "Epoch: 6500, Losses: BARE: 0.7037138091814182, PH: -1.4126912944912893, PE: -0.4632809332378417, PHE: -0.974067504752014\n",
      "New BARE best: 0.35374706983566284, PH best: 0.00398665526881814, PE best: 0.04178592562675476, PHE best: 0.013710252940654755\n",
      "New PE best: 0.023860501125454903, PHE best: 0.013710252940654755, BARE best: 0.35374706983566284, PH best: 0.00398665526881814\n",
      "New PHE best: 0.01306175347417593, BARE best: 0.35374706983566284, PH best: 0.00398665526881814, PE best: 0.023860501125454903\n",
      "Epoch: 7000, Losses: BARE: 0.7123663574225365, PH: -1.4521897082395019, PE: -0.48984695316851135, PHE: -0.9991972468164225\n",
      "Epoch: 7500, Losses: BARE: 0.6873447287860844, PH: -1.485785401768153, PE: -0.511496436894315, PHE: -1.015805451101727\n",
      "New PH best: 0.003615389112383127, PE best: 0.023860501125454903, PHE best: 0.01306175347417593, BARE best: 0.35374706983566284\n",
      "New PHE best: 0.01044873520731926, BARE best: 0.35374706983566284, PH best: 0.003615389112383127, PE best: 0.023860501125454903\n",
      "Epoch: 8000, Losses: BARE: 0.6810607420330244, PH: -1.526665283289222, PE: -0.5318138056377515, PHE: -1.0321561017326184\n",
      "Epoch: 8500, Losses: BARE: 0.6585826922737885, PH: -1.552734796656502, PE: -0.5432023140336906, PHE: -1.053389792324561\n",
      "New BARE best: 0.3482581377029419, PH best: 0.003615389112383127, PE best: 0.023860501125454903, PHE best: 0.01044873520731926\n",
      "Epoch: 9000, Losses: BARE: 0.6432019286942153, PH: -1.5780178721547118, PE: -0.5588800419419231, PHE: -1.0705103114512238\n",
      "Epoch: 9500, Losses: BARE: 0.6305948922876267, PH: -1.6085116166671123, PE: -0.5727231492723547, PHE: -1.084551025057832\n",
      "Epoch: 10000, Losses: BARE: 0.6134761571041825, PH: -1.6306467966900924, PE: -0.5792274506406655, PHE: -1.0943580780890259\n",
      "Epoch: 10500, Losses: BARE: 0.6125413505441195, PH: -1.6583696683247882, PE: -0.5922745018584865, PHE: -1.1149781462285255\n",
      "Epoch: 11000, Losses: BARE: 0.5872325596295096, PH: -1.670672447443007, PE: -0.6067492767435806, PHE: -1.1216997208429702\n",
      "Epoch: 11500, Losses: BARE: 0.5744350529455886, PH: -1.687692613363267, PE: -0.6133664654773765, PHE: -1.1401379106971956\n",
      "New PHE best: 0.008574428968131542, BARE best: 0.3482581377029419, PH best: 0.003615389112383127, PE best: 0.023860501125454903\n",
      "Epoch: 12000, Losses: BARE: 0.5650201442327558, PH: -1.706944168501429, PE: -0.6221127156334947, PHE: -1.1586578495643203\n",
      "Epoch: 12500, Losses: BARE: 0.55605091929891, PH: -1.725113419532776, PE: -0.6365108329272157, PHE: -1.1703066356182104\n",
      "New BARE best: 0.3390062153339386, PH best: 0.003615389112383127, PE best: 0.023860501125454903, PHE best: 0.008574428968131542\n",
      "Epoch: 13000, Losses: BARE: 0.5523129594497278, PH: -1.7366579103204938, PE: -0.6412601457674887, PHE: -1.1866737248400847\n",
      "Epoch: 13500, Losses: BARE: 0.5311374369742502, PH: -1.7556739184591508, PE: -0.6495166331388892, PHE: -1.1947194775756866\n",
      "New BARE best: 0.311989426612854, PH best: 0.003615389112383127, PE best: 0.023860501125454903, PHE best: 0.008574428968131542\n",
      "New PH best: 0.0025874096900224686, PE best: 0.023860501125454903, PHE best: 0.008574428968131542, BARE best: 0.311989426612854\n",
      "Epoch: 14000, Losses: BARE: 0.5203201090719344, PH: -1.7681619037787115, PE: -0.6549802389914785, PHE: -1.205727409389284\n",
      "Epoch: 14500, Losses: BARE: 0.5033341499761778, PH: -1.7797528554333595, PE: -0.6616126433154563, PHE: -1.2171643026073784\n",
      "New BARE best: 0.30161628127098083, PH best: 0.0025874096900224686, PE best: 0.023860501125454903, PHE best: 0.008574428968131542\n",
      "New PH best: 0.002106953877955675, PE best: 0.023860501125454903, PHE best: 0.008574428968131542, BARE best: 0.30161628127098083\n",
      "New PHE best: 0.0045920503325760365, BARE best: 0.30161628127098083, PH best: 0.002106953877955675, PE best: 0.023860501125454903\n",
      "Epoch: 15000, Losses: BARE: 0.4981365218784548, PH: -1.789831468131806, PE: -0.6709103412015576, PHE: -1.2285248058636993\n",
      "Epoch: 15500, Losses: BARE: 0.49039672624498754, PH: -1.791196498950324, PE: -0.6794264598006583, PHE: -1.2356261270509838\n",
      "Epoch: 16000, Losses: BARE: 0.47380932578606827, PH: -1.8073702997101675, PE: -0.6859863803239749, PHE: -1.2467759110927585\n",
      "Epoch: 16500, Losses: BARE: 0.4602356087797219, PH: -1.8164468366834856, PE: -0.691795099598045, PHE: -1.2545373498068921\n",
      "Epoch: 17000, Losses: BARE: 0.46004505076220575, PH: -1.8245129298104177, PE: -0.6979936863523396, PHE: -1.2588225050502346\n",
      "Epoch: 17500, Losses: BARE: 0.43838420894342317, PH: -1.8335395732720698, PE: -0.7020864512750673, PHE: -1.2718094624347147\n",
      "Epoch: 18000, Losses: BARE: 0.43899871019095504, PH: -1.8371256974538164, PE: -0.7006616145997417, PHE: -1.2765668622387794\n",
      "Epoch: 18500, Losses: BARE: 0.4210350311203575, PH: -1.848458841482797, PE: -0.7093482715301221, PHE: -1.2898982253869362\n",
      "New BARE best: 0.264377236366272, PH best: 0.002106953877955675, PE best: 0.023860501125454903, PHE best: 0.0045920503325760365\n",
      "New PE best: 0.023320721462368965, PHE best: 0.0045920503325760365, BARE best: 0.264377236366272, PH best: 0.002106953877955675\n",
      "Epoch: 19000, Losses: BARE: 0.40918266558367766, PH: -1.853362147225274, PE: -0.7121020352748137, PHE: -1.2942258202102446\n",
      "Epoch: 19500, Losses: BARE: 0.4045269813173764, PH: -1.8615306424829696, PE: -0.7178411311780407, PHE: -1.3021694880127916\n",
      "Epoch: 20000, Losses: BARE: 0.3946042979938276, PH: -1.858238403479257, PE: -0.7222126830501685, PHE: -1.2995135313788833\n",
      "Epoch: 20500, Losses: BARE: 0.3815586300393201, PH: -1.8696770431200675, PE: -0.7205466902918285, PHE: -1.3058597581982634\n",
      "Epoch: 21000, Losses: BARE: 0.36827250926648014, PH: -1.874925892353058, PE: -0.7257778731245133, PHE: -1.3189157396554942\n",
      "Epoch: 21500, Losses: BARE: 0.3618651955621317, PH: -1.875955327934689, PE: -0.7351912928182215, PHE: -1.3174898735384155\n",
      "New BARE best: 0.2484886646270752, PH best: 0.002106953877955675, PE best: 0.023320721462368965, PHE best: 0.0045920503325760365\n",
      "New PH best: 0.001475642086006701, PE best: 0.023320721462368965, PHE best: 0.0045920503325760365, BARE best: 0.2484886646270752\n",
      "New PE best: 0.011869426816701889, PHE best: 0.0045920503325760365, BARE best: 0.2484886646270752, PH best: 0.001475642086006701\n",
      "New PHE best: 0.004190187435597181, BARE best: 0.2484886646270752, PH best: 0.001475642086006701, PE best: 0.011869426816701889\n",
      "Epoch: 22000, Losses: BARE: 0.3600478277265016, PH: -1.8858469627168435, PE: -0.7376981506591865, PHE: -1.3345446892446935\n",
      "Epoch: 22500, Losses: BARE: 0.3407307941333334, PH: -1.8924411863485981, PE: -0.7400681975707406, PHE: -1.329156575739382\n",
      "Epoch: 23000, Losses: BARE: 0.333844389559145, PH: -1.8967113877667332, PE: -0.7472453677844675, PHE: -1.3452219448884337\n",
      "Epoch: 23500, Losses: BARE: 0.31397580703654804, PH: -1.9040498312844178, PE: -0.749104438631485, PHE: -1.3476791491508482\n",
      "Epoch: 24000, Losses: BARE: 0.3117867933422625, PH: -1.905951528602175, PE: -0.7490312751266691, PHE: -1.3500479014184736\n",
      "Epoch: 24500, Losses: BARE: 0.30880323071108756, PH: -1.9131929925017879, PE: -0.7541674116286966, PHE: -1.3604644687970493\n",
      "New PH best: 0.0014575425302609801, PE best: 0.011869426816701889, PHE best: 0.004190187435597181, BARE best: 0.2484886646270752\n",
      "Epoch: 25000, Losses: BARE: 0.30367615729138603, PH: -1.9161285606755147, PE: -0.7581714299354286, PHE: -1.3623180069790954\n",
      "Epoch: 25500, Losses: BARE: 0.29223983341270987, PH: -1.916825938304266, PE: -0.7567617958254296, PHE: -1.3666421324080873\n",
      "New PH best: 0.0013134414330124855, PE best: 0.011869426816701889, PHE best: 0.004190187435597181, BARE best: 0.2484886646270752\n",
      "Epoch: 26000, Losses: BARE: 0.28730593104544855, PH: -1.918455186155106, PE: -0.7578055511679914, PHE: -1.3697813837197088\n",
      "Epoch: 26500, Losses: BARE: 0.2763456252955108, PH: -1.9274562020831638, PE: -0.770890527475211, PHE: -1.3779522579113639\n",
      "New BARE best: 0.24561148881912231, PH best: 0.0013134414330124855, PE best: 0.011869426816701889, PHE best: 0.004190187435597181\n",
      "Epoch: 27000, Losses: BARE: 0.2746552503918521, PH: -1.9294879414770338, PE: -0.7651041230145436, PHE: -1.3825781364176002\n",
      "Epoch: 27500, Losses: BARE: 0.2678909054885102, PH: -1.9338453479342999, PE: -0.7700337074697018, PHE: -1.3862045869694832\n",
      "New BARE best: 0.23344877362251282, PH best: 0.0013134414330124855, PE best: 0.011869426816701889, PHE best: 0.004190187435597181\n",
      "New PH best: 0.0011222026078030467, PE best: 0.011869426816701889, PHE best: 0.004190187435597181, BARE best: 0.23344877362251282\n",
      "Epoch: 28000, Losses: BARE: 0.2652715174488387, PH: -1.9385544183784058, PE: -0.7691407799770437, PHE: -1.386508626951111\n",
      "Epoch: 28500, Losses: BARE: 0.2674514632191116, PH: -1.9403829969300146, PE: -0.7736508119669736, PHE: -1.3978063081502925\n",
      "New BARE best: 0.2287149280309677, PH best: 0.0011222026078030467, PE best: 0.011869426816701889, PHE best: 0.004190187435597181\n",
      "Epoch: 29000, Losses: BARE: 0.2520171109320137, PH: -1.9401559533013233, PE: -0.7719180239240326, PHE: -1.4017861941258112\n",
      "Epoch: 29500, Losses: BARE: 0.24648962331389027, PH: -1.9484900145000879, PE: -0.7768846175339487, PHE: -1.4032681382232244\n",
      "Epoch: 30000, Losses: BARE: 0.24646785035658036, PH: -1.9482086904578728, PE: -0.7800429042180383, PHE: -1.4122972428136402\n",
      "Epoch: 30500, Losses: BARE: 0.23555459930559539, PH: -1.9544522563086613, PE: -0.779908680319786, PHE: -1.4149487558205913\n",
      "New PH best: 0.0010409646201878786, PE best: 0.011869426816701889, PHE best: 0.004190187435597181, BARE best: 0.2287149280309677\n",
      "New PHE best: 0.0038950443267822266, BARE best: 0.2287149280309677, PH best: 0.0010409646201878786, PE best: 0.011869426816701889\n",
      "Epoch: 31000, Losses: BARE: 0.2327600325806529, PH: -1.9555497745407968, PE: -0.7847038409175965, PHE: -1.4180863486263493\n",
      "Epoch: 31500, Losses: BARE: 0.2337799439365412, PH: -1.9547979174455017, PE: -0.7831397779517706, PHE: -1.4181943967607304\n",
      "Epoch: 32000, Losses: BARE: 0.22933433781085233, PH: -1.9596905763149264, PE: -0.7826501882606083, PHE: -1.4311773449314955\n",
      "Epoch: 32500, Losses: BARE: 0.23045243074561408, PH: -1.960401550504897, PE: -0.7854482998351255, PHE: -1.4221340511308767\n",
      "Epoch: 33000, Losses: BARE: 0.21639563084596813, PH: -1.9665347057183586, PE: -0.7897171080509815, PHE: -1.4330848038329018\n",
      "Epoch: 33500, Losses: BARE: 0.2117345771823422, PH: -1.9677558064460754, PE: -0.792447999315129, PHE: -1.4363050010999052\n",
      "New PHE best: 0.003535514697432518, BARE best: 0.2287149280309677, PH best: 0.0010409646201878786, PE best: 0.011869426816701889\n",
      "Epoch: 34000, Losses: BARE: 0.21305048653844585, PH: -1.9663385881582907, PE: -0.795420053602093, PHE: -1.4339339219861547\n",
      "Epoch: 34500, Losses: BARE: 0.21380954118589493, PH: -1.9722522122595045, PE: -0.7976122766385477, PHE: -1.4350168686575355\n",
      "New BARE best: 0.21736527979373932, PH best: 0.0010409646201878786, PE best: 0.011869426816701889, PHE best: 0.003535514697432518\n",
      "Epoch: 35000, Losses: BARE: 0.20559508638662305, PH: -1.9739211989773642, PE: -0.7977677076796283, PHE: -1.4469181147283994\n",
      "Epoch: 35500, Losses: BARE: 0.20303038505060897, PH: -1.972321285963059, PE: -0.7954725734202399, PHE: -1.4468248578177565\n",
      "Epoch: 36000, Losses: BARE: 0.21785200363493334, PH: -1.9780841511885345, PE: -0.8027851660284737, PHE: -1.459243790719244\n",
      "Epoch: 36500, Losses: BARE: 0.19211279766022285, PH: -1.9794485947026141, PE: -0.8014838612907464, PHE: -1.4588232658704121\n",
      "Epoch: 37000, Losses: BARE: 0.1900983726641285, PH: -1.9836452673276268, PE: -0.8033153348333306, PHE: -1.4590403268337233\n",
      "Epoch: 37500, Losses: BARE: 0.18867710850027253, PH: -1.9830617644786843, PE: -0.8002301657448212, PHE: -1.4627780430714297\n",
      "New BARE best: 0.20571617782115936, PH best: 0.0010409646201878786, PE best: 0.011869426816701889, PHE best: 0.003535514697432518\n",
      "Epoch: 38000, Losses: BARE: 0.1851470517912529, PH: -1.985900268448723, PE: -0.8051622966594172, PHE: -1.4636335340870772\n",
      "Epoch: 38500, Losses: BARE: 0.17851986893474944, PH: -1.9869364524682374, PE: -0.8097208050025839, PHE: -1.4628944154712884\n",
      "Epoch: 39000, Losses: BARE: 0.18223069727255153, PH: -1.9922184098561604, PE: -0.8094981430139806, PHE: -1.4701704590055682\n",
      "Epoch: 39500, Losses: BARE: 0.17027777056777352, PH: -1.994344703329935, PE: -0.8140596703125378, PHE: -1.4823964735931818\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "timestamp = int(time.time())\n",
    "root_folder = f'../outputs/comparison/mind_change/bare_ph_pe/{str(timestamp)}/'\n",
    "\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "\n",
    "if not os.path.exists(f'{root_folder}saved_models/'):\n",
    "    os.makedirs(f'{root_folder}saved_models/')\n",
    "\n",
    "img_folder = f'{root_folder}img/'\n",
    "if not os.path.exists(img_folder):\n",
    "    os.makedirs(img_folder)\n",
    "\n",
    "torch.save(y_train, f'{root_folder}y.pt')\n",
    "\n",
    "\n",
    "epochs = 500_000\n",
    "epoch_iter = num_demos // batch_size\n",
    "test_epoch_iter = num_test//batch_size\n",
    "avg_loss0, avg_loss1, avg_loss2, avg_loss3 = 0, 0, 0, 0\n",
    "loss_report_interval = 500\n",
    "test_per_epoch = 1000\n",
    "min_test_loss0, min_test_loss1, min_test_loss2, min_test_loss3 = 1000000, 1000000, 1000000, 1000000\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "plot_test = True\n",
    "\n",
    "l0, l1, l2, l3 = [], [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss0, epoch_loss1, epoch_loss2, epoch_loss3 = 0, 0, 0, 0\n",
    "\n",
    "    traj_ids = torch.randperm(num_demos)[:batch_size * epoch_iter].chunk(epoch_iter)\n",
    "\n",
    "    for i in range(epoch_iter):\n",
    "        prepare_masked_batch(y_train, traj_ids[i])\n",
    "\n",
    "        opt0.zero_grad()\n",
    "        pred0 = m0(obs0, tar_x0, obs_mask)\n",
    "        loss0 = m0.loss(pred0, tar_y, tar_mask)\n",
    "        loss0.backward()\n",
    "        opt0.step()\n",
    "\n",
    "        epoch_loss0 += loss0.item()\n",
    "\n",
    "\n",
    "        opt1.zero_grad()\n",
    "        pred1 = m1(obs1, tar_x1, obs_mask)\n",
    "        loss1 = m1.loss(pred1, tar_y, tar_mask)\n",
    "        loss1.backward()\n",
    "        opt1.step()\n",
    "\n",
    "        epoch_loss1 += loss1.item()\n",
    "\n",
    "\n",
    "        opt2.zero_grad()\n",
    "        pred2 = m2(obs2, tar_x2, obs_mask)\n",
    "        loss2 = m2.loss(pred2, tar_y, tar_mask)\n",
    "        loss2.backward()\n",
    "        opt2.step()\n",
    "\n",
    "        epoch_loss2 += loss2.item()\n",
    "\n",
    "\n",
    "        opt3.zero_grad()\n",
    "        pred3 = m3(obs3, tar_x3, obs_mask)\n",
    "        loss3 = m3.loss(pred3, tar_y, tar_mask)\n",
    "        loss3.backward()\n",
    "        opt3.step()\n",
    "\n",
    "        epoch_loss3 += loss3.item()\n",
    "\n",
    "\n",
    "    if epoch % test_per_epoch == 0:# and epoch > 0:\n",
    "        test_traj_ids = torch.randperm(num_test)[:batch_size*test_epoch_iter].chunk(test_epoch_iter)\n",
    "        test_loss0, test_loss1, test_loss2, test_loss3 = 0, 0, 0, 0\n",
    "\n",
    "        for j in range(test_epoch_iter):\n",
    "            prepare_masked_test_batch(y_test, test_traj_ids[j])\n",
    "\n",
    "            pred0 = m0.val(test_obs0, test_tar_x0, test_obs_mask)\n",
    "            pred1 = m1.val(test_obs1, test_tar_x1, test_obs_mask)\n",
    "            pred2 = m2.val(test_obs2, test_tar_x2, test_obs_mask)\n",
    "            pred3 = m3.val(test_obs3, test_tar_x3, test_obs_mask)\n",
    "            \n",
    "            if plot_test:\n",
    "                for k in range(batch_size):\n",
    "                    current_n = test_obs_mask[k].sum().item()\n",
    "                    plt.scatter(last_obs_vals[k, :current_n, :dx].cpu().numpy(), test_obs0[k, :current_n, dx:].cpu().numpy(), label='Condition')\n",
    "                    plt.plot(test_tar_y[k, :, 0].cpu().numpy(), label=f\"Groundtruth\")\n",
    "                    plt.plot(pred0[k, :, 0].cpu().numpy(), label=f\"Prediction\")\n",
    "                    \n",
    "                    plt.legend(loc='upper left')\n",
    "                    plt.title(f'Epoch: {epoch}', fontsize=20)\n",
    "                    plt.savefig(f'{img_folder}{epoch}_{test_traj_ids[j][k]}_bare.png')\n",
    "                    plt.clf()\n",
    "\n",
    "                    plt.scatter(last_obs_vals[k, :current_n, :dx].cpu().numpy(), test_obs1[k, :current_n, dph:].cpu().numpy(), label='Condition')\n",
    "                    plt.plot(test_tar_y[k, :, 0].cpu().numpy(), label=f\"Groundtruth\")\n",
    "                    plt.plot(pred1[k, :, 0].cpu().numpy(), label=f\"Prediction\")\n",
    "                    \n",
    "                    plt.legend(loc='upper left')\n",
    "                    plt.title(f'Epoch: {epoch}', fontsize=20)\n",
    "                    plt.savefig(f'{img_folder}{epoch}_{test_traj_ids[j][k]}_ph.png')\n",
    "                    plt.clf()\n",
    "\n",
    "                    plt.scatter(last_obs_vals[k, :current_n, :dx].cpu().numpy(), test_obs2[k, :current_n, dpe:].cpu().numpy(), label='Condition')\n",
    "                    plt.plot(test_tar_y[k, :, 0].cpu().numpy(), label=f\"Groundtruth\")\n",
    "                    plt.plot(pred2[k, :, 0].cpu().numpy(), label=f\"Prediction\")\n",
    "                    \n",
    "                    plt.legend(loc='upper left')\n",
    "                    plt.title(f'Epoch: {epoch}', fontsize=20)\n",
    "                    plt.savefig(f'{img_folder}{epoch}_{test_traj_ids[j][k]}_pe.png')\n",
    "                    plt.clf()\n",
    "\n",
    "                    plt.scatter(last_obs_vals[k, :current_n, :dx].cpu().numpy(), test_obs3[k, :current_n, dpe:].cpu().numpy(), label='Condition')\n",
    "                    plt.plot(test_tar_y[k, :, 0].cpu().numpy(), label=f\"Groundtruth\")\n",
    "                    plt.plot(pred3[k, :, 0].cpu().numpy(), label=f\"Prediction\")\n",
    "                    \n",
    "                    plt.legend(loc='upper left')\n",
    "                    plt.title(f'Epoch: {epoch}', fontsize=20)\n",
    "                    plt.savefig(f'{img_folder}{epoch}_{test_traj_ids[j][k]}_phe.png')\n",
    "                    plt.clf()\n",
    "\n",
    "            test_loss0 += mse_loss(pred0[:, :, :m0.output_dim], test_tar_y).item()\n",
    "            test_loss1 += mse_loss(pred1[:, :, :m1.output_dim], test_tar_y).item()\n",
    "            test_loss2 += mse_loss(pred2[:, :, :m2.output_dim], test_tar_y).item()\n",
    "            test_loss3 += mse_loss(pred3[:, :, :m3.output_dim], test_tar_y).item()\n",
    "        \n",
    "        test_loss0 /= test_epoch_iter\n",
    "        test_loss1 /= test_epoch_iter\n",
    "        test_loss2 /= test_epoch_iter\n",
    "        test_loss3 /= test_epoch_iter\n",
    "            \n",
    "        if test_loss0 < min_test_loss0:\n",
    "            min_test_loss0 = test_loss0\n",
    "            print(f'New BARE best: {min_test_loss0}, PH best: {min_test_loss1}, PE best: {min_test_loss2}, PHE best: {min_test_loss3}')\n",
    "            torch.save(m0_.state_dict(), f'{root_folder}saved_models/bare.pt')\n",
    "\n",
    "        if test_loss1 < min_test_loss1:\n",
    "            min_test_loss1 = test_loss1\n",
    "            print(f'New PH best: {min_test_loss1}, PE best: {min_test_loss2}, PHE best: {min_test_loss3}, BARE best: {min_test_loss0}')\n",
    "            torch.save(m1_.state_dict(), f'{root_folder}saved_models/ph.pt')\n",
    "\n",
    "        if test_loss2 < min_test_loss2:\n",
    "            min_test_loss2 = test_loss2\n",
    "            print(f'New PE best: {min_test_loss2}, PHE best: {min_test_loss3}, BARE best: {min_test_loss0}, PH best: {min_test_loss1}')\n",
    "            torch.save(m2_.state_dict(), f'{root_folder}saved_models/pe.pt')\n",
    "\n",
    "        if test_loss3 < min_test_loss3:\n",
    "            min_test_loss3 = test_loss3\n",
    "            print(f'New PHE best: {min_test_loss3}, BARE best: {min_test_loss0}, PH best: {min_test_loss1}, PE best: {min_test_loss2}')\n",
    "            torch.save(m2_.state_dict(), f'{root_folder}saved_models/phe.pt')\n",
    "\n",
    "    epoch_loss0 /= epoch_iter\n",
    "    epoch_loss1 /= epoch_iter\n",
    "    epoch_loss2 /= epoch_iter\n",
    "    epoch_loss3 /= epoch_iter\n",
    "\n",
    "    avg_loss0 += epoch_loss0\n",
    "    avg_loss1 += epoch_loss1\n",
    "    avg_loss2 += epoch_loss2\n",
    "    avg_loss3 += epoch_loss3\n",
    "\n",
    "    l0.append(epoch_loss0)\n",
    "    l1.append(epoch_loss1)\n",
    "    l2.append(epoch_loss2)\n",
    "    l3.append(epoch_loss3)\n",
    "\n",
    "    if epoch % loss_report_interval == 0:\n",
    "        print(\"Epoch: {}, Losses: BARE: {}, PH: {}, PE: {}, PHE: {}\".format(epoch, avg_loss0/loss_report_interval, avg_loss1/loss_report_interval, avg_loss2/loss_report_interval, avg_loss3/loss_report_interval))\n",
    "        avg_loss0, avg_loss1, avg_loss2, avg_loss3 = 0, 0, 0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last_obs_vals.shape\n",
    "test_obs0[k, current_n, dx:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(l0, f'{root_folder}losses_bare.pt')\n",
    "torch.save(l1, f'{root_folder}losses_ph.pt')\n",
    "torch.save(l2, f'{root_folder}losses_pe.pt')\n",
    "torch.save(l3, f'{root_folder}losses_phe.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
