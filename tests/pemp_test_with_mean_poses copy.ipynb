{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import time, os\n",
    "import numpy as np\n",
    "\n",
    "folder_path = '../models/'\n",
    "if folder_path not in sys.path:\n",
    "    sys.path.append(folder_path)\n",
    "\n",
    "from pemp import PEMP\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "def get_free_gpu():\n",
    "    gpu_util = []\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_device(i)  # Switch GPU\n",
    "#        gpu_util.append((i, torch.cuda.memory_stats()['reserved_bytes.all.current'] / (1024 ** 2)))\n",
    "        gpu_util.append((i, torch.cuda.utilization()))\n",
    "    gpu_util.sort(key=lambda x: x[1])\n",
    "    return gpu_util[0][0]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    available_gpu = get_free_gpu()\n",
    "    if available_gpu == 0:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{available_gpu}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device :\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1) must match the existing size (200) at non-singleton dimension 1.  Target sizes: [200, 1].  Tensor sizes: [200]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 56\u001b[0m\n\u001b[1;32m     45\u001b[0m val_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# for i in range(num_demos):\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#     num_peaks = dg #np.random.choice([1, 2, 4])  # Randomly choose 1, 2, or 4 dg\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#     y, peak_positions = with_peaks(x, num_peaks)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#     y, peak_positions = generate_trajectory_with_peaks(x, dg) # Always generate 3 peaks for validation\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#     val_data.append((y, peak_positions))\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m y_train, y_val \u001b[38;5;241m=\u001b[39m \u001b[43mwith_peaks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_demos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_peaks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# y_train = torch.stack([y for y, _ in train_data], dim=0)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# y_val = torch.stack([y for y, _ in val_data], dim=0)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m#     # for ppid, pp in enumerate(peak_positions):\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m#     #     gamma_val[i, ppid] = pp\u001b[39;00m\n\u001b[1;32m     78\u001b[0m x_train \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(num_demos, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 36\u001b[0m, in \u001b[0;36mwith_peaks\u001b[0;34m(num_demos, num_val, num_peaks, t_steps, interp)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, peak \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_peaks):\n\u001b[1;32m     35\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, t_steps)\n\u001b[0;32m---> 36\u001b[0m     \u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m generate_trajectory(x, peak)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, peak \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(val_peaks):\n\u001b[1;32m     39\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, t_steps)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1) must match the existing size (200) at non-singleton dimension 1.  Target sizes: [200, 1].  Tensor sizes: [200]"
     ]
    }
   ],
   "source": [
    "dx, dy, dg, dpe = 1, 1, 1, 10\n",
    "num_demos, num_val = 3, 1\n",
    "t_steps = 200\n",
    "n_max, m_max = 10, 10\n",
    "num_peaks_max = dg\n",
    "\n",
    "x = torch.linspace(0, 1, t_steps).view(-1, 1)\n",
    "\n",
    "# def generate_trajectory(x, peak_x, noise_level=0.0005):\n",
    "#     y = torch.zeros_like(x)\n",
    "#     peak_width = 0.1\n",
    "#     peak_height = 0.2\n",
    "\n",
    "#     y += peak_height * torch.exp(-((x - peak_x)**2) / (2 * peak_width**2))\n",
    "#     y += noise_level * torch.randn_like(x)\n",
    "#     return y\n",
    "\n",
    "\n",
    "# def with_peaks(num_demos, num_val, num_peaks=1, t_steps=200, interp=True):\n",
    "#     peak_positions = np.random.rand(num_demos + num_val, num_peaks)\n",
    "\n",
    "#     y_train = torch.zeros(num_demos, t_steps, 1)\n",
    "#     y_val = torch.zeros(num_val, t_steps, 1)\n",
    "\n",
    "#     if interp:\n",
    "#         # interp_pp = np.sort(peak_positions)[1:-1]\n",
    "#         val_peaks = np.random.choice(np.squeeze(np.sort(peak_positions)[1:-1], axis=-1), num_val, replace=False)\n",
    "#         train_peaks = np.array([x for x in np.sort(peak_positions) if x not in val_peaks])\n",
    "\n",
    "#     else:\n",
    "#         train_peaks = np.sort(peak_positions)[:num_demos]\n",
    "#         val_peaks = np.sort(peak_positions)[num_demos:]\n",
    "\n",
    "#     for i, peak in enumerate(train_peaks):\n",
    "#         x = torch.linspace(0, 1, t_steps)\n",
    "#         y_train[i] = generate_trajectory(x, peak)\n",
    "    \n",
    "#     for i, peak in enumerate(val_peaks):\n",
    "#         x = torch.linspace(0, 1, t_steps)\n",
    "#         y_val[i] = generate_trajectory(x, peak)\n",
    "    \n",
    "#     return y_train, y_val\n",
    "\n",
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "# for i in range(num_demos):\n",
    "#     num_peaks = dg #np.random.choice([1, 2, 4])  # Randomly choose 1, 2, or 4 dg\n",
    "#     y, peak_positions = with_peaks(x, num_peaks)\n",
    "#     train_data.append((y, peak_positions))\n",
    "\n",
    "# for i in range(num_val):\n",
    "#     y, peak_positions = generate_trajectory_with_peaks(x, dg) # Always generate 3 peaks for validation\n",
    "#     val_data.append((y, peak_positions))\n",
    "\n",
    "y_train, y_val = with_peaks(num_demos, num_val, num_peaks=dg)\n",
    "\n",
    "# y_train = torch.stack([y for y, _ in train_data], dim=0)\n",
    "# y_val = torch.stack([y for y, _ in val_data], dim=0)\n",
    "\n",
    "# y_train = torch.zeros(num_demos, t_steps, dy)\n",
    "# y_val = torch.zeros(num_val, t_steps, dy)\n",
    "# gamma_train = torch.zeros(num_demos, num_peaks_max)\n",
    "# gamma_val = torch.zeros(num_val, num_peaks_max)\n",
    "\n",
    "# for i, data in enumerate(train_data):\n",
    "#     y, peak_positions = data\n",
    "#     y_train[i, :, :] = y\n",
    "#     # for ppid, pp in enumerate(peak_positions):\n",
    "#     #     gamma_train[i, ppid] = pp\n",
    "\n",
    "# for i, data in enumerate(val_data):\n",
    "#     y, peak_positions = data\n",
    "#     y_val[i, :, :] = y\n",
    "#     # for ppid, pp in enumerate(peak_positions):\n",
    "#     #     gamma_val[i, ppid] = pp\n",
    "\n",
    "x_train = x.unsqueeze(0).repeat(num_demos, 1, 1)\n",
    "x_val = x.unsqueeze(0).repeat(num_val, 1, 1)\n",
    "\n",
    "# y_val = y_train\n",
    "\n",
    "# Print shapes\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"x_val shape: {x_val.shape}, y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_demos):\n\u001b[0;32m----> 5\u001b[0m     axs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(\u001b[43mx_train\u001b[49m[i, :, \u001b[38;5;241m0\u001b[39m], y_train[i, :, \u001b[38;5;241m0\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDemo \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_val):\n\u001b[1;32m      7\u001b[0m     axs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(x_val[i, :, \u001b[38;5;241m0\u001b[39m], y_val[i, :, \u001b[38;5;241m0\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdashed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot train trajectories on the left and validation trajectories on the right\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "for i in range(num_demos):\n",
    "    axs[0].plot(x_train[i, :, 0], y_train[i, :, 0], label=f\"Demo {i}\")\n",
    "for i in range(num_val):\n",
    "    axs[0].plot(x_val[i, :, 0], y_val[i, :, 0], label=f\"Val {i}\", color='black', linestyle='dashed')\n",
    "    axs[1].plot(x_val[i, :, 0], y_val[i, :, 0], label=f\"Val {i}\", color='black', linestyle='dashed')\n",
    "axs[0].set_title(\"Training\")\n",
    "axs[1].set_title(\"Validation\")\n",
    "axs[0].grid(True)\n",
    "axs[1].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69378\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "model = PEMP(input_dim=dpe+dg, output_dim=dy, n_max=n_max, m_max=m_max, encoder_hidden_dims=[128, 128, 128], decoder_hidden_dims=[128, 128, 128], batch_size=batch_size, device=device)\n",
    "optimizer = torch.optim.Adam(lr=3e-4, params=model.parameters())\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(pytorch_total_params)\n",
    "\n",
    "\n",
    "if torch.__version__ >= \"2.0\":\n",
    "    model_ = torch.compile(model)\n",
    "else:\n",
    "    model_ = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 312.32it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0250, 0.0500, 0.0750,  ..., 0.2000, 0.2250, 0.2500],\n",
      "        [0.0500, 0.1000, 0.1500,  ..., 0.4000, 0.4500, 0.5000],\n",
      "        ...,\n",
      "        [0.9250, 0.1500, 0.8250,  ..., 0.6000, 0.6750, 0.7500],\n",
      "        [0.9500, 0.1000, 0.7500,  ..., 0.4000, 0.4500, 0.5000],\n",
      "        [0.9750, 0.0500, 0.6750,  ..., 0.2000, 0.2250, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_positional_encoding(length=t_steps, d_model=4):  # d_model: dimension of encoding space\n",
    "    pos = torch.arange(length).unsqueeze(1)  # (length, 1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))  # (d_model/2,)\n",
    "    pe = torch.zeros(length, d_model)  # (length, d_model)\n",
    "    pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "    pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "    return pe\n",
    "\n",
    "\n",
    "def generate_encoding(len=200, d_model=4):\n",
    "    enc = torch.zeros(d_model, len)\n",
    "    max_num = len\n",
    "\n",
    "    i, j = 0, 1\n",
    "\n",
    "    for i in tqdm(range(d_model)):\n",
    "        increment = (i + 1) * 5\n",
    "        \n",
    "        while True:\n",
    "            while enc[i, j-1] <= max_num-increment and j < len:\n",
    "                enc[i, j] = enc[i, j-1] + increment\n",
    "                j += 1\n",
    "            while enc[i, j-1] >= increment and j < len:\n",
    "                enc[i, j] = enc[i, j-1] - increment\n",
    "                j += 1\n",
    "\n",
    "            if j < len:\n",
    "                continue\n",
    "            else:\n",
    "                j = 1\n",
    "                break\n",
    "    return enc.T  \n",
    "\n",
    "d_model = 10\n",
    "pe = generate_encoding(d_model=d_model) / t_steps\n",
    "\n",
    "# for i in range(t_steps):\n",
    "#     for j in range(d_model):\n",
    "#         print(pe[i, j].item(), end='\\t')\n",
    "#     print()\n",
    "\n",
    "    # pe = torch.zeros((4, 200))\n",
    "    # for i in range(4):\n",
    "    #     pe[i] = torch.linspace(0, 1, t_steps)\n",
    "    # return pe.T\n",
    "\n",
    "\n",
    "# pe = generate_positional_encoding(1000, 256)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(pe.T, cmap=plt.get_cmap('gray'))\n",
    "plt.show()\n",
    "\n",
    "print(pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpe_aug = dpe + dg  # dg for gamma (peak_positions)\n",
    "\n",
    "obs = torch.zeros((batch_size, n_max, dpe_aug+dy), dtype=torch.float32, device=device)\n",
    "tar_x = torch.zeros((batch_size, m_max, dpe_aug), dtype=torch.float32, device=device)\n",
    "tar_y = torch.zeros((batch_size, m_max, dy), dtype=torch.float32, device=device)\n",
    "obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "tar_mask = torch.zeros((batch_size, m_max), dtype=torch.bool, device=device)\n",
    "\n",
    "def prepare_masked_batch(t: list, traj_ids: list):\n",
    "    obs.fill_(0)\n",
    "    tar_x.fill_(0)\n",
    "    tar_y.fill_(0)\n",
    "    obs_mask.fill_(False)\n",
    "    tar_mask.fill_(False)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = t[traj_id]\n",
    "        pe = generate_positional_encoding(traj.shape[0], dpe) / dpe\n",
    "        gamma = torch.zeros(dg)\n",
    "\n",
    "        for ppid, pp in enumerate(gamma_train[traj_id]):\n",
    "            gamma[ppid] = pp  # gamma: peak_positions\n",
    "\n",
    "        n = torch.randint(5, n_max, (1,)).item()\n",
    "        m = torch.randint(1, m_max, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = permuted_ids[n:n+m]\n",
    "        \n",
    "        obs[i, :n, :dpe] = pe[n_ids] # PE(t)\n",
    "        obs[i, :n, dpe:dpe_aug] = gamma\n",
    "        obs[i, :n, dpe_aug:] = traj[n_ids]  # SM(t)\n",
    "        obs_mask[i, :n] = True\n",
    "        \n",
    "        tar_x[i, :m, :dpe] = pe[m_ids]\n",
    "        tar_x[i, :m, dpe:] = gamma\n",
    "        tar_y[i, :m] = traj[m_ids]\n",
    "        tar_mask[i, :m] = True\n",
    "\n",
    "val_obs = torch.zeros((batch_size, n_max, dpe_aug+dy), dtype=torch.float32, device=device)\n",
    "val_tar_x = torch.zeros((batch_size, t_steps, dpe_aug), dtype=torch.float32, device=device)\n",
    "val_tar_y = torch.zeros((batch_size, t_steps, dy), dtype=torch.float32, device=device)\n",
    "val_obs_mask = torch.zeros((batch_size, n_max), dtype=torch.bool, device=device)\n",
    "\n",
    "def prepare_masked_val_batch(t: list, traj_ids: list):\n",
    "    val_obs.fill_(0)\n",
    "    val_tar_x.fill_(0)\n",
    "    val_tar_y.fill_(0)\n",
    "    val_obs_mask.fill_(False)\n",
    "\n",
    "    for i, traj_id in enumerate(traj_ids):\n",
    "        traj = t[traj_id]\n",
    "        pe = generate_positional_encoding(traj.shape[0], dpe) / dpe\n",
    "        gamma = torch.zeros(dg)\n",
    "        \n",
    "        for ppid, pp in enumerate(gamma_val[traj_id]):\n",
    "            gamma[ppid] = pp  # gamma: peak_positions\n",
    "\n",
    "        n = torch.randint(5, n_max, (1,)).item()\n",
    "\n",
    "        permuted_ids = torch.randperm(t_steps)\n",
    "        n_ids = permuted_ids[:n]\n",
    "        m_ids = torch.arange(t_steps)\n",
    "        \n",
    "        val_obs[i, :n, :dpe] = pe[n_ids]\n",
    "        val_obs[i, :n, dpe:dpe_aug] = gamma\n",
    "        val_obs[i, :n, dpe_aug:] = traj[n_ids]\n",
    "        val_obs_mask[i, :n] = True\n",
    "        \n",
    "        val_tar_x[i, :, :dpe] = pe\n",
    "        val_tar_x[i, :, dpe:] = gamma\n",
    "        val_tar_y[i] = traj[m_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.018546029925346375\n",
      "Epoch: 100, Loss: -4.154581399746239\n",
      "Epoch: 200, Loss: -5.866215574741363\n",
      "Epoch: 300, Loss: -6.131628822088242\n",
      "Epoch: 400, Loss: -6.217544428408146\n",
      "Epoch: 500, Loss: -6.586471844911575\n",
      "Epoch: 600, Loss: -6.433518027067184\n",
      "Epoch: 700, Loss: -6.577166272550821\n",
      "Epoch: 800, Loss: -6.8188392400741575\n",
      "Epoch: 900, Loss: -6.837025869488716\n",
      "New best: 0.0005123100127093494\n",
      "Epoch: 1000, Loss: -7.027147132158279\n",
      "Epoch: 1100, Loss: -7.010691081881523\n",
      "Epoch: 1200, Loss: -7.118570015430451\n",
      "Epoch: 1300, Loss: -6.8856641697883605\n",
      "Epoch: 1400, Loss: -6.981454734802246\n",
      "Epoch: 1500, Loss: -7.13138039290905\n",
      "Epoch: 1600, Loss: -7.200977095961571\n",
      "Epoch: 1700, Loss: -7.383761692047119\n",
      "Epoch: 1800, Loss: -7.3997855269908905\n",
      "Epoch: 1900, Loss: -7.641314807534218\n",
      "New best: 0.00047320727026090026\n",
      "Epoch: 2000, Loss: -7.709476565122604\n",
      "Epoch: 2100, Loss: -7.967921464443207\n",
      "Epoch: 2200, Loss: -8.256255739666521\n",
      "Epoch: 2300, Loss: -8.719236137866973\n",
      "Epoch: 2400, Loss: -8.471223548948764\n",
      "Epoch: 2500, Loss: -9.319067804813384\n",
      "Epoch: 2600, Loss: -8.54587916046381\n",
      "Epoch: 2700, Loss: -9.071021709442139\n",
      "Epoch: 2800, Loss: -9.405903176069259\n",
      "Epoch: 2900, Loss: -9.310707760453225\n",
      "New best: 0.00028779671993106604\n",
      "Epoch: 3000, Loss: -8.923851501345634\n",
      "Epoch: 3100, Loss: -8.284134933948517\n",
      "Epoch: 3200, Loss: -9.465334696769714\n",
      "Epoch: 3300, Loss: -9.408638777136803\n",
      "Epoch: 3400, Loss: -9.28518573243171\n",
      "Epoch: 3500, Loss: -8.842061035186052\n",
      "Epoch: 3600, Loss: -9.411910606771707\n",
      "Epoch: 3700, Loss: -9.284437680840492\n",
      "Epoch: 3800, Loss: -9.90487906217575\n",
      "Epoch: 3900, Loss: -9.443328915834426\n",
      "New best: 0.0002157381532015279\n",
      "Epoch: 4000, Loss: -8.66543953821063\n",
      "Epoch: 4100, Loss: -9.743472702503205\n",
      "Epoch: 4200, Loss: -9.542971774935722\n",
      "Epoch: 4300, Loss: -9.263718687146902\n",
      "Epoch: 4400, Loss: -9.955567650794983\n",
      "Epoch: 4500, Loss: -9.321493241488934\n",
      "Epoch: 4600, Loss: -9.906877997219562\n",
      "Epoch: 4700, Loss: -8.94117895744741\n",
      "Epoch: 4800, Loss: -10.034711515903473\n",
      "Epoch: 4900, Loss: -9.712184824943542\n",
      "New best: 0.00018586177611723542\n",
      "Epoch: 5000, Loss: -9.532488189935684\n",
      "Epoch: 5100, Loss: -9.763393944501876\n",
      "Epoch: 5200, Loss: -9.680772880911826\n",
      "Epoch: 5300, Loss: -9.486466676592826\n",
      "Epoch: 5400, Loss: -9.53233552902937\n",
      "Epoch: 5500, Loss: -9.157292795479298\n",
      "Epoch: 5600, Loss: -8.77561639636755\n",
      "Epoch: 5700, Loss: -10.048868227005006\n",
      "Epoch: 5800, Loss: -9.694629402160645\n",
      "Epoch: 5900, Loss: -9.350943853855133\n",
      "Epoch: 6000, Loss: -8.718487829044461\n",
      "Epoch: 6100, Loss: -10.167178709506988\n",
      "Epoch: 6200, Loss: -9.628842712640763\n",
      "Epoch: 6300, Loss: -10.181078791618347\n",
      "Epoch: 6400, Loss: -9.959918065816163\n",
      "Epoch: 6500, Loss: -10.10774014905095\n",
      "Epoch: 6600, Loss: -9.845011518299579\n",
      "Epoch: 6700, Loss: -9.875338037610055\n",
      "Epoch: 6800, Loss: -9.888502589464188\n",
      "Epoch: 6900, Loss: -9.797310551926493\n",
      "New best: 0.00018299576186109334\n",
      "Epoch: 7000, Loss: -10.14381833076477\n",
      "Epoch: 7100, Loss: -9.78470850765705\n",
      "Epoch: 7200, Loss: -10.315902961492538\n",
      "Epoch: 7300, Loss: -9.899379324913024\n",
      "Epoch: 7400, Loss: -10.086245540380478\n",
      "Epoch: 7500, Loss: -9.880639591068029\n",
      "Epoch: 7600, Loss: -10.211291039586067\n",
      "Epoch: 7700, Loss: -10.37648423165083\n",
      "Epoch: 7800, Loss: -10.026406831741333\n",
      "Epoch: 7900, Loss: -9.941948444768787\n",
      "Epoch: 8000, Loss: -10.484356921911239\n",
      "Epoch: 8100, Loss: -10.108443343639374\n",
      "Epoch: 8200, Loss: -10.231553854942321\n",
      "Epoch: 8300, Loss: -10.533541968464851\n",
      "Epoch: 8400, Loss: -9.968173934817314\n",
      "Epoch: 8500, Loss: -10.509582670927047\n",
      "Epoch: 8600, Loss: -9.487834295630455\n",
      "Epoch: 8700, Loss: -10.393422706127167\n",
      "Epoch: 8800, Loss: -10.26540166735649\n",
      "Epoch: 8900, Loss: -10.101525647640228\n",
      "New best: 0.0001778349105734378\n",
      "Epoch: 9000, Loss: -10.435773371458053\n",
      "Epoch: 9100, Loss: -10.600041377544404\n",
      "Epoch: 9200, Loss: -9.509654839038848\n",
      "Epoch: 9300, Loss: -10.635343700647354\n",
      "Epoch: 9400, Loss: -10.119847014248371\n",
      "Epoch: 9500, Loss: -10.413372043073178\n",
      "Epoch: 9600, Loss: -10.810894702523946\n",
      "Epoch: 9700, Loss: -9.894400678873062\n",
      "Epoch: 9800, Loss: -10.497188597917557\n",
      "Epoch: 9900, Loss: -10.302427193522453\n",
      "Epoch: 10000, Loss: -10.48071802765131\n",
      "Epoch: 10100, Loss: -10.08737973228097\n",
      "Epoch: 10200, Loss: -10.415867584943772\n",
      "Epoch: 10300, Loss: -9.985731056556105\n",
      "Epoch: 10400, Loss: -10.704696880578995\n",
      "Epoch: 10500, Loss: -10.401274726986886\n",
      "Epoch: 10600, Loss: -9.465442785471678\n",
      "Epoch: 10700, Loss: -10.390541934967041\n",
      "Epoch: 10800, Loss: -10.215869868397712\n",
      "Epoch: 10900, Loss: -10.132708624601364\n",
      "Epoch: 11000, Loss: -10.616717215776443\n",
      "Epoch: 11100, Loss: -10.33918239057064\n",
      "Epoch: 11200, Loss: -10.413842953443528\n",
      "Epoch: 11300, Loss: -10.372099226862192\n",
      "Epoch: 11400, Loss: -10.567850788831711\n",
      "Epoch: 11500, Loss: -11.235286386013032\n",
      "Epoch: 11600, Loss: -10.518658987283708\n",
      "Epoch: 11700, Loss: -10.869673882722855\n",
      "Epoch: 11800, Loss: -10.857507408857346\n",
      "Epoch: 11900, Loss: -10.810011683106422\n",
      "New best: 0.00016120723739732057\n",
      "Epoch: 12000, Loss: -10.821592135429382\n",
      "Epoch: 12100, Loss: -10.486711512804032\n",
      "Epoch: 12200, Loss: -10.675793050527572\n",
      "Epoch: 12300, Loss: -11.12154996395111\n",
      "Epoch: 12400, Loss: -10.552921096682548\n",
      "Epoch: 12500, Loss: -10.708483959436416\n",
      "Epoch: 12600, Loss: -11.158688505887985\n",
      "Epoch: 12700, Loss: -11.0418910074234\n",
      "Epoch: 12800, Loss: -10.325359128713608\n",
      "Epoch: 12900, Loss: -10.598933370113373\n",
      "Epoch: 13000, Loss: -10.59042587518692\n",
      "Epoch: 13100, Loss: -11.386326410770415\n",
      "Epoch: 13200, Loss: -10.996971731185914\n",
      "Epoch: 13300, Loss: -10.219984118044376\n",
      "Epoch: 13400, Loss: -10.42053684413433\n",
      "Epoch: 13500, Loss: -10.787312105894088\n",
      "Epoch: 13600, Loss: -10.886760751605033\n",
      "Epoch: 13700, Loss: -11.062234432697297\n",
      "Epoch: 13800, Loss: -11.074977091550828\n",
      "Epoch: 13900, Loss: -11.1220028591156\n",
      "Epoch: 14000, Loss: -11.042704068422317\n",
      "Epoch: 14100, Loss: -11.166251300573348\n",
      "Epoch: 14200, Loss: -11.159941909313202\n",
      "Epoch: 14300, Loss: -11.000936406627297\n",
      "Epoch: 14400, Loss: -10.910173513889312\n",
      "Epoch: 14500, Loss: -10.998801535367965\n",
      "Epoch: 14600, Loss: -11.078620167970657\n",
      "Epoch: 14700, Loss: -11.222350656986237\n",
      "Epoch: 14800, Loss: -10.683176713436842\n",
      "Epoch: 14900, Loss: -10.699076580405235\n",
      "Epoch: 15000, Loss: -10.935174773931504\n",
      "Epoch: 15100, Loss: -11.138890981674194\n",
      "Epoch: 15200, Loss: -11.122834888696671\n",
      "Epoch: 15300, Loss: -11.049266051054001\n",
      "Epoch: 15400, Loss: -11.298183155059814\n",
      "Epoch: 15500, Loss: -11.069982939958573\n",
      "Epoch: 15600, Loss: -11.176521077156067\n",
      "Epoch: 15700, Loss: -10.730325013399124\n",
      "Epoch: 15800, Loss: -11.22384482383728\n",
      "Epoch: 15900, Loss: -10.886880764365197\n",
      "Epoch: 16000, Loss: -11.247827497720719\n",
      "Epoch: 16100, Loss: -10.756905373334885\n",
      "Epoch: 16200, Loss: -10.747683034539223\n",
      "Epoch: 16300, Loss: -11.002245244979859\n",
      "Epoch: 16400, Loss: -11.148382421731949\n",
      "Epoch: 16500, Loss: -11.24316535949707\n",
      "Epoch: 16600, Loss: -11.273924660682678\n",
      "Epoch: 16700, Loss: -10.945537664890288\n",
      "Epoch: 16800, Loss: -11.253890999555587\n",
      "Epoch: 16900, Loss: -10.92816996216774\n",
      "Epoch: 17000, Loss: -10.914267262220383\n",
      "Epoch: 17100, Loss: -11.305053722858428\n",
      "Epoch: 17200, Loss: -11.16029041647911\n",
      "Epoch: 17300, Loss: -11.369998197555542\n",
      "Epoch: 17400, Loss: -10.936383848786354\n",
      "Epoch: 17500, Loss: -11.113010526895524\n",
      "Epoch: 17600, Loss: -11.600508860349656\n",
      "Epoch: 17700, Loss: -11.309907339811325\n",
      "Epoch: 17800, Loss: -11.030632600784301\n",
      "Epoch: 17900, Loss: -11.206986459493637\n",
      "Epoch: 18000, Loss: -11.232848794460297\n",
      "Epoch: 18100, Loss: -11.344682281017304\n",
      "Epoch: 18200, Loss: -11.172088453769684\n",
      "Epoch: 18300, Loss: -11.2768193089962\n",
      "Epoch: 18400, Loss: -11.204999347925186\n",
      "Epoch: 18500, Loss: -11.476355088949203\n",
      "Epoch: 18600, Loss: -11.523242429494857\n",
      "Epoch: 18700, Loss: -11.229382627010345\n",
      "Epoch: 18800, Loss: -11.5819595515728\n",
      "Epoch: 18900, Loss: -11.18322394490242\n",
      "Epoch: 19000, Loss: -11.443527929782867\n",
      "Epoch: 19100, Loss: -11.199942281246186\n",
      "Epoch: 19200, Loss: -11.050094273090362\n",
      "Epoch: 19300, Loss: -11.00113421946764\n",
      "Epoch: 19400, Loss: -11.383068696260452\n",
      "Epoch: 19500, Loss: -11.319534233808518\n",
      "Epoch: 19600, Loss: -11.498818808794022\n",
      "Epoch: 19700, Loss: -11.226163390874863\n",
      "Epoch: 19800, Loss: -11.142169443964958\n",
      "Epoch: 19900, Loss: -11.125918496847152\n",
      "Epoch: 20000, Loss: -11.346497781276703\n",
      "Epoch: 20100, Loss: -10.67367218831554\n",
      "Epoch: 20200, Loss: -11.228793375492096\n",
      "Epoch: 20300, Loss: -11.46522718667984\n",
      "Epoch: 20400, Loss: -11.570546888113022\n",
      "Epoch: 20500, Loss: -11.668909695148468\n",
      "Epoch: 20600, Loss: -11.261987227201463\n",
      "Epoch: 20700, Loss: -11.255205862522125\n",
      "Epoch: 20800, Loss: -11.399564994573593\n",
      "Epoch: 20900, Loss: -11.074886128902435\n",
      "Epoch: 21000, Loss: -11.355454692840576\n",
      "Epoch: 21100, Loss: -11.531540067195893\n",
      "Epoch: 21200, Loss: -11.275273203849792\n",
      "Epoch: 21300, Loss: -10.961890483051539\n",
      "Epoch: 21400, Loss: -11.287138521671295\n",
      "Epoch: 21500, Loss: -11.555991860628128\n",
      "Epoch: 21600, Loss: -11.178219497203827\n",
      "Epoch: 21700, Loss: -11.260014183521271\n",
      "Epoch: 21800, Loss: -11.232854508161545\n",
      "Epoch: 21900, Loss: -11.303294094800949\n",
      "Epoch: 22000, Loss: -11.207566165924073\n",
      "Epoch: 22100, Loss: -11.085346579551697\n",
      "Epoch: 22200, Loss: -11.597684546709061\n",
      "Epoch: 22300, Loss: -11.605848436355592\n",
      "Epoch: 22400, Loss: -11.131575573682785\n",
      "Epoch: 22500, Loss: -11.726554741859436\n",
      "Epoch: 22600, Loss: -11.423672515153886\n",
      "Epoch: 22700, Loss: -11.03220919430256\n",
      "Epoch: 22800, Loss: -11.319661502838136\n",
      "Epoch: 22900, Loss: -11.746682344675063\n",
      "Epoch: 23000, Loss: -11.463855167627335\n",
      "Epoch: 23100, Loss: -11.640597955845296\n",
      "Epoch: 23200, Loss: -11.646748563051224\n",
      "Epoch: 23300, Loss: -11.612713750600815\n",
      "Epoch: 23400, Loss: -11.50509578704834\n",
      "Epoch: 23500, Loss: -11.488933999538421\n",
      "Epoch: 23600, Loss: -11.638875663280487\n",
      "Epoch: 23700, Loss: -11.560824205875397\n",
      "Epoch: 23800, Loss: -11.286067926883698\n",
      "Epoch: 23900, Loss: -11.551029826402663\n",
      "New best: 0.00015895772958174348\n",
      "Epoch: 24000, Loss: -11.465895231962204\n",
      "Epoch: 24100, Loss: -11.492307794094085\n",
      "Epoch: 24200, Loss: -11.207415500879288\n",
      "Epoch: 24300, Loss: -11.591304116249084\n",
      "Epoch: 24400, Loss: -11.47688591837883\n",
      "Epoch: 24500, Loss: -11.344841798245907\n",
      "Epoch: 24600, Loss: -11.61277274608612\n",
      "Epoch: 24700, Loss: -11.321110479831695\n",
      "Epoch: 24800, Loss: -11.502134588956833\n",
      "Epoch: 24900, Loss: -11.416064766645432\n",
      "Epoch: 25000, Loss: -11.479377456903457\n",
      "Epoch: 25100, Loss: -11.760353484153747\n",
      "Epoch: 25200, Loss: -11.847877655029297\n",
      "Epoch: 25300, Loss: -11.135937459468842\n",
      "Epoch: 25400, Loss: -11.707353081703186\n",
      "Epoch: 25500, Loss: -11.466474427580833\n",
      "Epoch: 25600, Loss: -11.701555169820786\n",
      "Epoch: 25700, Loss: -11.384093145132065\n",
      "Epoch: 25800, Loss: -11.342170742750168\n",
      "Epoch: 25900, Loss: -11.593443549871445\n",
      "Epoch: 26000, Loss: -11.400773739814758\n",
      "Epoch: 26100, Loss: -11.704434642791748\n",
      "Epoch: 26200, Loss: -11.674473065137864\n",
      "Epoch: 26300, Loss: -11.516902482509613\n",
      "Epoch: 26400, Loss: -11.738440536260605\n",
      "Epoch: 26500, Loss: -11.724141218662261\n",
      "Epoch: 26600, Loss: -11.456812517642975\n",
      "Epoch: 26700, Loss: -11.28431285738945\n",
      "Epoch: 26800, Loss: -11.627352669239045\n",
      "Epoch: 26900, Loss: -11.698694641590118\n",
      "Epoch: 27000, Loss: -12.005897660255432\n",
      "Epoch: 27100, Loss: -11.845668122768402\n",
      "Epoch: 27200, Loss: -11.793394787311554\n",
      "Epoch: 27300, Loss: -11.697806954644621\n",
      "Epoch: 27400, Loss: -11.893371820449829\n",
      "Epoch: 27500, Loss: -12.058716182708741\n",
      "Epoch: 27600, Loss: -12.056918046474458\n",
      "Epoch: 27700, Loss: -12.33438514828682\n",
      "Epoch: 27800, Loss: -12.209687575101853\n",
      "Epoch: 27900, Loss: -12.448187603950501\n",
      "New best: 3.275438939454034e-05\n",
      "Epoch: 28000, Loss: -12.337144297361373\n",
      "Epoch: 28100, Loss: -12.289224841594695\n",
      "Epoch: 28200, Loss: -12.338948362469672\n",
      "Epoch: 28300, Loss: -12.491342726349831\n",
      "Epoch: 28400, Loss: -12.18292206287384\n",
      "Epoch: 28500, Loss: -12.438991826772689\n",
      "Epoch: 28600, Loss: -12.453733316659928\n",
      "Epoch: 28700, Loss: -12.728008879423141\n",
      "Epoch: 28800, Loss: -12.606847912669181\n",
      "Epoch: 28900, Loss: -12.67409219264984\n",
      "New best: 2.3041960957925767e-05\n",
      "Epoch: 29000, Loss: -12.629655369520187\n",
      "Epoch: 29100, Loss: -12.415945645570755\n",
      "Epoch: 29200, Loss: -13.11656410753727\n",
      "Epoch: 29300, Loss: -12.730360768437386\n",
      "Epoch: 29400, Loss: -12.949644985198974\n",
      "Epoch: 29500, Loss: -12.559117837250232\n",
      "Epoch: 29600, Loss: -13.02548618376255\n",
      "Epoch: 29700, Loss: -12.759260777831077\n",
      "Epoch: 29800, Loss: -13.326451237797738\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m prepare_masked_batch(y_train, traj_ids[i])\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 40\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtar_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(pred, tar_y, tar_mask)\n\u001b[1;32m     42\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/pemp/tests/../models/pemp.py:49\u001b[0m, in \u001b[0;36mPEMP.forward\u001b[0;34m(self, obs, tar, obs_mask, latent)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs, tar, obs_mask, latent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# obs: (batch_size, n_max, input_dim+output_dim)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# tar: (batch_size, m_max, input_dim)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# obs_mask: (batch_size, n_max)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# encoding\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     encoded_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, n_max, encoder_hidden_dims[-1])\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     obs_mask_exp \u001b[38;5;241m=\u001b[39m obs_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype_as(encoded_obs)  \u001b[38;5;66;03m# (batch_size, n_max, 1)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     masked_encoded_obs \u001b[38;5;241m=\u001b[39m encoded_obs \u001b[38;5;241m*\u001b[39m obs_mask_exp  \u001b[38;5;66;03m# (batch_size, n_max, encoder_hidden_dims[-1])\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "timestamp = int(time.time())\n",
    "root_folder = f'../outputs/tests/simple/{str(timestamp)}/'\n",
    "\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "\n",
    "if not os.path.exists(f'{root_folder}saved_models/'):\n",
    "    os.makedirs(f'{root_folder}saved_models/')\n",
    "\n",
    "img_folder = f'{root_folder}img/'\n",
    "if not os.path.exists(img_folder):\n",
    "    os.makedirs(img_folder)\n",
    "\n",
    "torch.save(y_train, f'{root_folder}y.pt')\n",
    "\n",
    "epochs = 2000000\n",
    "epoch_iter = num_demos // batch_size\n",
    "v_epoch_iter = num_val//batch_size\n",
    "avg_loss = 0\n",
    "val_per_epoch = 1000\n",
    "min_val_loss = 1000000\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "plot_validation = True\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    traj_ids = torch.randperm(num_demos)[:batch_size * epoch_iter].chunk(epoch_iter)\n",
    "\n",
    "    for i in range(epoch_iter):\n",
    "        prepare_masked_batch(y_train, traj_ids[i])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model(obs, tar_x, obs_mask)\n",
    "        loss = model.loss(pred, tar_y, tar_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    if epoch % val_per_epoch == 0 and epoch > 0:\n",
    "        with torch.no_grad():\n",
    "            v_traj_ids = torch.randperm(num_val)[:batch_size*v_epoch_iter].chunk(v_epoch_iter)\n",
    "            val_loss = 0\n",
    "\n",
    "            for j in range(v_epoch_iter):\n",
    "                prepare_masked_val_batch(y_val, v_traj_ids[j])\n",
    "                pred = model.val(val_obs, val_tar_x, val_obs_mask)\n",
    "                if plot_validation:\n",
    "                    for k in range(batch_size):\n",
    "                        plt.plot(val_tar_y[k, :, 0].cpu().numpy(), label=f\"True {k}\")\n",
    "                        plt.plot(pred[k, :, 0].cpu().numpy(), label=f\"Pred {k}\")\n",
    "                        \n",
    "                        plt.legend()\n",
    "                        plt.savefig(f'{img_folder}val_{epoch}_{j}_{k}.png')\n",
    "                        plt.clf()\n",
    "                val_loss += mse_loss(pred[:, :, :model.output_dim], val_tar_y).item()\n",
    "                \n",
    "            if val_loss < min_val_loss:\n",
    "                min_val_loss = val_loss\n",
    "                print(f'New best: {min_val_loss}')\n",
    "                torch.save(model_.state_dict(), f'{root_folder}saved_models/pemp.pt')\n",
    "\n",
    "    avg_loss += epoch_loss\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch: {}, Loss: {}\".format(epoch, avg_loss / 100))\n",
    "        avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
